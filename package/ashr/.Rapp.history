M
M=C(100*(1:9),1000*(1:10))
M=c(100*(1:9),1000*(1:10))
M
M1=c(20,100,200,400,600,800,1000,1500,2000,3000,4000,5000)
length(M1)
M1=c(50,100,200,400,600,800,1000,1500,2000,3000,4000,5000)#
#
llrm1=matrix(NA,nrow=length(M1),ncol=1000)
dim(llrm1)
ash
' @useDynLib ashr#
#todo#
##
#' @title Main Adaptive SHrinkage function#
#'#
#' @description Takes vectors of estimates (betahat) and their standard errors (sebetahat), and applies#
#' shrinkage to them, using Empirical Bayes methods, to compute shrunk estimates for beta.#
#'#
#' @details See readme for more details#
#' #
#' @param betahat, a p vector of estimates #
#' @param sebetahat, a p vector of corresponding standard errors#
#' @param method: specifies how ash is to be run. Can be "shrinkage" (if main aim is shrinkage) or "fdr" (if main aim is to assess fdr or fsr)#
#' This is simply a convenient way to specify certain combinations of parameters: "shrinkage" sets pointmass=FALSE and prior="uniform";#
#' "fdr" sets pointmass=TRUE and prior="nullbiased".#
#' @param mixcompdist: distribution of components in mixture ("normal", "uniform" or "halfuniform")#
#'#
#' @param lambda1: multiplicative "inflation factor" for standard errors (like Genomic Control)#
#' @param lambda2: additive "inflation factor" for standard errors (like Genomic Control)#
#' @param nullcheck: whether to check that any fitted model exceeds the "null" likelihood#
#' in which all weight is on the first component#
#' @param df: appropriate degrees of freedom for (t) distribution of betahat/sebetahat#
#' @param randomstart: bool, indicating whether to initialize EM randomly. If FALSE, then initializes to prior mean (for EM algorithm) or prior (for VBEM)#
#' @param pointmass: bool, indicating whether to use a point mass at zero as one of components for a mixture distribution#
#' @param nullweight: scalar, the weight put on the prior of null under "fdr" method#
#' @param onlylogLR: bool, indicating whether to use this function to get logLR. Skip posterior prob, posterior mean, lfdr...#
#' @param prior: string, or numeric vector indicating Dirichlet prior on mixture proportions (defaults to "uniform", or 1,1...,1; also can be "nullbiased" 1,1/k-1,...,1/k-1 to put more weight on first component)#
#' @param mixsd: vector of sds for underlying mixture components #
#' @param VB: whether to use Variational Bayes to estimate mixture proportions (instead of EM to find MAP estimate)#
#' @param gridmult: the multiplier by which the default grid values for mixsd differ by one another. (Smaller values produce finer grids)#
#' @param minimal_output: if TRUE, just outputs the fitted g and the lfsr (useful for very big data sets where memory is an issue) #
#' @param g: the prior distribution for beta (usually estimated from the data; this is used primarily in simulated data to do computations with the "true" g)#
#' @param maxiter: maximum number of iterations of the EM algorithm#
#' @param cxx: flag to indicate whether to use the c++ (Rcpp) version#
#' #
#'#
#' @return a list with elements fitted.g is fitted mixture#
#' logLR : logP(D|mle(pi)) - logP(D|null)#
#' #
#' @export#
#' #
#' @examples #
#' beta = c(rep(0,100),rnorm(100))#
#' sebetahat = abs(rnorm(200,0,1))#
#' betahat = rnorm(200,beta,sebetahat)#
#' beta.ash = ash(betahat, sebetahat)#
#' summary(beta.ash)#
#' plot(betahat,beta.ash$PosteriorMean,xlim=c(-4,4),ylim=c(-4,4))#
#' #
#' #
#Things to do:#
# check sampling routine#
# check number of iterations#
ash = function(betahat,sebetahat,method = c("shrink","fdr"), #
               mixcompdist = c("normal","uniform","halfuniform"),#
               lambda1=1,lambda2=0,nullcheck=TRUE,df=NULL,randomstart=FALSE, #
               pointmass = FALSE, #
               nullweight=10,               #
               onlylogLR = FALSE, #
               prior=c("uniform","nullbiased"), #
               mixsd=NULL, VB=FALSE,gridmult=sqrt(2),#
               minimaloutput=FALSE,#
               multiseqoutput=FALSE,#
               g=NULL,#
               maxiter = 5000,#
               cxx=FALSE){#
  #method provides a convenient interface to set a particular combinations of parameters for prior an#
  #If method is supplied, use it to set up specific values for these parameters; provide warning if values#
  #are also specified by user#
  #If method is not supplied use the user-supplied values (or defaults if user does not specify them)#
  if(!missing(method)){#
    method = match.arg(method) #
    if(method=="shrink"){#
      if(missing(prior)){#
        prior = "uniform"#
      } else {#
        warning("Specification of prior overrides default for method shrink")#
      }#
      if(missing(pointmass)){#
        pointmass=FALSE#
      } else {#
        warning("Specification of pointmass overrides default for method shrink")#
      }#
    }#
    if(method=="fdr"){#
      if(missing(prior)){#
        prior = "nullbiased"#
      } else {#
        warning("Specification of prior overrides default for method fdr")#
      }#
      if(missing(pointmass)){#
        pointmass=TRUE#
      } else {#
        warning("Specification of pointmass overrides default for method fdr")#
      }#
    }  #
  }#
  if(gridmult<=1&multiseqoutput!=TRUE)#
    stop("gridmult must be > 1")#
  mixcompdist = match.arg(mixcompdist)#
  # if(mixcompdist=="uniform" & pointmass==TRUE){#
  #    stop("point mass not yet implemented for uniform or half-uniform")#
  #  }#
  #  if(mixcompdist=="halfuniform" & pointmass==TRUE){#
  #    stop("point mass not yet implemented for uniform or half-uniform")#
  #  }#
  if(!is.numeric(prior)){#
    prior = match.arg(prior)#
  }  #
  if(length(sebetahat)==1){#
    sebetahat = rep(sebetahat,length(betahat))#
  }#
  if(length(sebetahat) != length(betahat)){#
    stop("Error: sebetahat must have length 1, or same length as betahat")#
  }#
  completeobs = (!is.na(betahat) & !is.na(sebetahat))#
  n=sum(completeobs)#
  if(n==0){#
    if(onlylogLR){#
      return(list(pi=NULL, logLR = 0))#
    }#
    else{#
      stop("Error: all input values are missing")#
    }#
  }  #
  if(!is.null(g)){#
    maxiter = 1 # if g is specified, don't iterate the EM#
    prior = rep(1,ncomp(g)) #prior is not actually used if g specified, but required to make sure EM doesn't produce warning#
    null.comp=1 #null.comp also not used, but required #
  } else {#
    if(is.null(mixsd)){#
      mixsd = autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)#
    }#
    if(pointmass){#
      mixsd = c(0,mixsd)#
    }#
    null.comp = which.min(mixsd) #which component is the "null"#
    print(null.comp)#
    k = length(mixsd)#
    if(!is.numeric(prior)){#
      if(prior=="nullbiased"){ # set up prior to favour "null"#
        prior = rep(1,k)#
        prior[null.comp] = nullweight #prior nullweight-1 in favour of null#
      }else if(prior=="uniform"){#
        prior = rep(1,k)#
      }#
    }#
    if(length(prior)!=k | !is.numeric(prior)){#
      stop("invalid prior specification")#
    }#
    if(randomstart){#
      pi = rgamma(k,1,1)#
    } else {#
      if(k<n){#
        pi=rep(1,k)/n #default initialization strongly favours null; puts weight 1/n on everything except null#
        pi[null.comp] = (n-k+1)/n #the motivation is data can quickly drive away from null, but tend to drive only slowly toward null.#
      } else {#
        pi=rep(1,k)/k#
      }#
    }#
    pi=normalize(pi)#
    if(!is.element(mixcompdist,c("normal","uniform","halfuniform"))) stop("Error: invalid type of mixcompdist")#
    if(mixcompdist=="normal") g=normalmix(pi,rep(0,k),mixsd)#
    if(mixcompdist=="uniform") g=unimix(pi,-mixsd,mixsd)#
    if(mixcompdist=="halfuniform"){#
      g = unimix(c(pi,pi)/2,c(-mixsd,rep(0,k)),c(rep(0,k),mixsd))#
      prior = rep(prior, 2)#
      pi = rep(pi, 2)#
    }#
  }#
  pi.fit=EMest(betahat[completeobs],lambda1*sebetahat[completeobs]+lambda2,g,prior,null.comp=null.comp,nullcheck=nullcheck,VB=VB,maxiter = maxiter, cxx=cxx, df=df)  #
  if (!onlylogLR){#
      n=length(betahat)#
      if (!multiseqoutput){#
          ZeroProb = rep(0,length=n)#
          NegativeProb = rep(0,length=n)#
      }#
      if (!minimaloutput){#
          PosteriorMean = rep(0,length=n)#
          PosteriorSD = rep(0,length=n)#
      }#
      if(is.null(df)){#
                                        #print("normal likelihood")#
          if (!multiseqoutput){  #
              ZeroProb[completeobs] = colSums(comppostprob(pi.fit$g,betahat[completeobs],sebetahat[completeobs])[comp_sd(pi.fit$g)==0,,drop=FALSE])     #
              NegativeProb[completeobs] = cdf_post(pi.fit$g, 0, betahat[completeobs],sebetahat[completeobs]) - ZeroProb[completeobs]#
          }#
          if (!minimaloutput){#
              PosteriorMean[completeobs] = postmean(pi.fit$g,betahat[completeobs],sebetahat[completeobs])#
              PosteriorSD[completeobs] = postsd(pi.fit$g,betahat[completeobs],sebetahat[completeobs])#
          }#
      }#
      else{#
                                        #print("student-t likelihood")#
          if (!multiseqoutput){#
              ZeroProb[completeobs] = colSums(comppostprob_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)[comp_sd(pi.fit$g)==0,,drop=FALSE])     #
              NegativeProb[completeobs] = cdf_post_t(pi.fit$g, 0, betahat[completeobs],sebetahat[completeobs],df) - ZeroProb[completeobs]#
          }#
          if (!minimaloutput){#
              PosteriorMean[completeobs] = postmean_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
              PosteriorSD[completeobs] = postsd_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
          }#
      }#
                                        #FOR MISSING OBSERVATIONS, USE THE PRIOR INSTEAD OF THE POSTERIOR#
      if (!multiseqoutput){#
          ZeroProb[!completeobs] = sum(mixprop(pi.fit$g)[comp_sd(pi.fit$g)==0])#
          NegativeProb[!completeobs] = mixcdf(pi.fit$g,0)#
          lfsr = compute_lfsr(NegativeProb,ZeroProb)#
      }#
      if (!minimaloutput){#
          PosteriorMean[!completeobs] = mixmean(pi.fit$g)#
          PosteriorSD[!completeobs] = mixsd(pi.fit$g)#
      }#
      if (!minimaloutput & !multiseqoutput){#
          PositiveProb = 1- NegativeProb-ZeroProb#
          lfsra = compute_lfsra(PositiveProb,NegativeProb,ZeroProb) #
          lfdr = ZeroProb#
          qvalue = qval.from.lfdr(lfdr)#
      }#
  }#
  if (!minimaloutput)#
      logLR = tail(pi.fit$loglik,1) - pi.fit$null.loglik#
  if (onlylogLR)#
      return(list(fitted.g=pi.fit$g, logLR = logLR))#
  else if (minimaloutput)#
      return(list(fitted.g = pi.fit$g, lfsr = lfsr, fit = pi.fit))#
  else if (multiseqoutput)#
      return(list(fitted.g = pi.fit$g, logLR = logLR, PosteriorMean = PosteriorMean, PosteriorSD = PosteriorSD, call= match.call()))#
  else{#
      result = list(fitted.g = pi.fit$g, logLR = logLR, PosteriorMean = PosteriorMean, PosteriorSD = PosteriorSD, PositiveProb = PositiveProb, NegativeProb = NegativeProb, ZeroProb = ZeroProb, lfsr = lfsr,lfsra = lfsra, lfdr = lfdr, qvalue = qvalue, fit = pi.fit, lambda1 = lambda1, lambda2 = lambda2, call = match.call(), data = list(betahat = betahat, sebetahat=sebetahat))#
      class(result) = "ash"#
      return(result)#
  }#
}#
  #if(nsamp>0){#
  #  sample = posterior_sample(post,nsamp)#
  #}#
#
# #' @title Faster version of function ash#
# #'#
# #' @description This function has similar functionality as ash, but only returns some of the outputs.#
# #'#
# #' @param betahat, a p vector of estimates#
# #' @param sebetahat, a p vector of corresponding standard errors#
# #' @param nullcheck: whether to check that any fitted model exceeds the "null" likelihood in which all weight is on the first component#
# #' @param randomstart: bool, indicating whether to initialize EM randomly. If FALSE, then initializes to prior mean (for EM algorithm) or prior (for VBEM)#
# #' @param pointmass: bool, indicating whether to use a point mass at zero as one of components for a mixture distribution#
# #' @param onlylogLR: bool, indicating whether to use this function to get logLR. Skip posterior prob, posterior mean, lfdr...#
# #' @param prior: string, or numeric vector indicating Dirichlet prior on mixture proportions (defaults to "uniform", or 1,1...,1; also can be "nullbiased" 1,1/k-1,...,1/k-1 to put more weight on first component)#
# #' @param mixsd: vector of sds for underlying mixture components#
# #' @param VB: whether to use Variational Bayes to estimate mixture proportions (instead of EM to find MAP estimate)#
# #' @param gridmult: the multiplier by which the default grid values for mixsd differ by one another. (Smaller values produce finer grids)#
# #' @param g: the prior distribution for beta (usually estimated from the data; this is used primarily in simulated data to do computations with the "true" g)#
# #' @param cxx: flag to indicate whether to use the c++ (Rcpp) version#
# #'#
# #' @return a list with elements fitted.g is fitted mixture#
# #' logLR : logP(D|mle(pi)) - logP(D|null)#
# #'#
# #' @export#
# fast.ash = function(betahat,sebetahat, #
#                     nullcheck=TRUE,randomstart=FALSE, #
#                     pointmass = TRUE,    #
#                     prior=c("nullbiased","uniform"), #
#                     mixsd=NULL, VB=FALSE,gridmult=4,#
#                     g=NULL, cxx=TRUE,#
#                     onlylogLR = FALSE,df=NULL){#
#   #
#   if(onlylogLR){#
#     pointmass <- TRUE  #
#   }#
#   #
#   #If method is supplied, use it to set up defaults; provide warning if these default values#
#   #are also specified by user#
#   if(!is.numeric(prior)){#
#     prior = match.arg(prior)#
#   }#
#   #
#   if(length(sebetahat)==1){#
#     sebetahat = rep(sebetahat,length(betahat))#
#   }#
#   if(length(sebetahat) != length(betahat)){#
#     stop("Error: sebetahat must have length 1, or same length as betahat")#
#   }#
#   #
#   completeobs = (!is.na(betahat) & !is.na(sebetahat))#
#   if(sum(completeobs)==0){#
#     if(onlylogLR){#
#       return(list(pi=NULL, logLR = 0))#
#     }else{#
#       stop("Error: all input values are missing")#
#     }#
#   }  #
#   #
#   if(is.null(mixsd)){#
#     mixsd= autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)#
#   }#
#   if(pointmass){#
#     mixsd = c(0,mixsd)#
#   }#
#   #
#   k=length(mixsd)  #
#   null.comp = which.min(mixsd) #which component is the "null"#
#   #
#   if(!is.numeric(prior)){#
#     if(prior=="nullbiased"){ # set up prior to favour "null"#
#       prior = rep(1,k)#
#       prior[null.comp] = 10 #prior 10-1 in favour of null#
#     }else if(prior=="uniform"){#
#       prior = rep(1,k)#
#     }#
#   }#
#   #
#   if(length(prior)!=k | !is.numeric(prior)){#
#     stop("invalid prior specification")#
#   }#
#   #
#   if(missing(g)){#
#     pi = prior^2 #default is to initialize pi at prior (mean)#
#     if(randomstart){pi=rgamma(k,1,1)}#
#     pi=normalize(pi)#
#     g=normalmix(pi,rep(0,k),mixsd)#
#     maxiter = 5000#
#   } else {#
#     maxiter = 1; # if g is specified, don't iterate the EM #
#   }#
#   #
#   pi.fit=EMest(betahat[completeobs],sebetahat[completeobs],g,prior,null.comp=null.comp,nullcheck=nullcheck,VB=VB,maxiter = maxiter, cxx=cxx, df=df)  #
#   #
#   if(onlylogLR){#
#     logLR = tail(pi.fit$loglik,1) - pi.fit$null.loglik#
#     return(list(pi=pi.fit$pi, logLR = logLR))#
#   }else{#
#     #
#     n=length(betahat)#
#     PosteriorMean = rep(0,length=n)#
#     PosteriorSD=rep(0,length=n)#
#     #
#     if(is.null(df)){#
#       PosteriorMean[completeobs] = postmean(pi.fit$g,betahat[completeobs],sebetahat[completeobs])#
#       PosteriorSD[completeobs] =postsd(pi.fit$g,betahat[completeobs],sebetahat[completeobs]) #
#     }#
#     else{#
#       PosteriorMean[completeobs] = postmean_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
#       PosteriorSD[completeobs] =postsd_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
#     }#
#     #FOR MISSING OBSERVATIONS, USE THE PRIOR INSTEAD OF THE POSTERIOR#
#     PosteriorMean[!completeobs] = mixmean(pi.fit$g)#
#     PosteriorSD[!completeobs] =mixsd(pi.fit$g)  #
#     #
#     result = list(fitted.g=pi.fit$g,PosteriorMean = PosteriorMean,PosteriorSD=PosteriorSD,call=match.call(),data=list(betahat = betahat, sebetahat=sebetahat))#
#     return(result)#
#   }#
#   #if(nsamp>0){#
#   #  sample = posterior_sample(post,nsamp)#
#   #}#
# }#
compute_lfsr = function(NegativeProb,ZeroProb){#
  ifelse(NegativeProb> 0.5*(1-ZeroProb),1-NegativeProb,NegativeProb+ZeroProb)#
}#
#
compute_lfsra = function(PositiveProb, NegativeProb,ZeroProb){#
  ifelse(PositiveProb<NegativeProb,2*PositiveProb+ZeroProb,2*NegativeProb+ZeroProb)  #
}  #
#' @title Estimate posterior distribution on mixture proportions of a mixture model by a Variational Bayes EM algorithm#
#'#
#' @description Given the individual component likelihoods for a mixture model, estimates the posterior on #
#' the mixture proportions by an VBEM algorithm. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates posterior on mixture proportions \eqn{\pi} by Variational Bayes, #
#' with a Dirichlet prior on \eqn{\pi}. #
#' Algorithm adapted from Bishop (2009), Pattern Recognition and Machine Learning, Chapter 10.#
#' #
#' @param matrix_lik: a n by k matrix with (j,k)th element equal to \eqn{f_k(x_j)}.#
#' @param prior: a k vector of the parameters of the Dirichlet prior on \eqn{\pi}. Recommended to be rep(1,k)#
#' @param post.init: the initial value of the posterior parameters. If not specified defaults to the prior parameters.#
#' @param tol: the tolerance for convergence of log-likelihood bound.#
#' @param maxiter: the maximum number of iterations performed#
#' #
#' @return A list, whose components include point estimates (pihat), #
#' the parameters of the fitted posterior on \eqn{\pi} (pipost),#
#' the bound on the log likelihood for each iteration (B)#
#' and a flag to indicate convergence (converged).#
#'  #
#' @export#
#' #
mixVBEM = function(matrix_lik, prior, pi.init = NULL,tol=1e-7, maxiter=5000){#
  k=ncol(matrix_lik)#
  if(is.null(pi.init)){#
    pi.init = rep(1,k)# Use as starting point for pi#
  } #
  res = squarem(par=pi.init,fixptfn=VBfixpoint, objfn=VBnegpenloglik,matrix_lik=matrix_lik, prior=prior, control=list(maxiter=maxiter,tol=tol))#
  return(list(pihat = res$par/sum(res$par), B=res$value.objfn, niter = res$iter, converged=res$convergence,post=res$par))#
}#
VBfixpoint = function(pipost, matrix_lik, prior){  #
  n=nrow(matrix_lik)#
  k=ncol(matrix_lik)#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost*matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  pipostnew = colSums(classprob) + prior#
  return(pipostnew)#
}#
#
VBnegpenloglik=function(pipost,matrix_lik,prior){#
  return(-VBpenloglik(pipost,matrix_lik,prior))#
}#
#
VBpenloglik = function(pipost, matrix_lik, prior){#
  n=nrow(matrix_lik)#
  k=ncol(matrix_lik)#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost*matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  B= sum(classprob*log(avgpipost*matrix_lik),na.rm=TRUE) - diriKL(prior,pipost) - sum(classprob*log(classprob)) #
  return(B)#
}#
#' @title Estimate mixture proportions of a mixture model by EM algorithm#
#'#
#' @description Given the individual component likelihoods for a mixture model, estimates the mixture proportions by an EM algorithm.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates mixture proportions \eqn{\pi} by maximum likelihood, or by maximum a posteriori (MAP) estimation for a Dirichlet prior on $\pi$ #
#' (if a prior is specified).  Uses the SQUAREM package to accelerate convergence of EM. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' #
#' @param matrix_lik, a n by k matrix with (j,k)th element equal to \eqn{f_k(x_j)}.#
#' @param prior, a k vector of the parameters of the Dirichlet prior on \eqn{\pi}. Recommended to be rep(1,k)#
#' @param pi.init, the initial value of \eqn{\pi} to use. If not specified defaults to (1/k,...,1/k).#
#' @param tol, the tolerance for convergence of log-likelihood.#
#' @param maxiter the maximum number of iterations performed#
#' #
#' @return A list, including the estimates (pihat), the log likelihood for each interation (B)#
#' and a flag to indicate convergence#
#'  #
#' @export#
#' #
#' #
mixEM = function(matrix_lik, prior, pi.init = NULL,tol=1e-7, maxiter=5000){#
  if(is.null(pi.init)){#
    pi.init = rep(1/k,k)# Use as starting point for pi#
  } #
  res = squarem(par=pi.init,fixptfn=fixpoint, objfn=negpenloglik,matrix_lik=matrix_lik, prior=prior, control=list(maxiter=maxiter,tol=tol))#
  return(list(pihat = normalize(pmax(0,res$par)), B=res$value.objfn, #
              niter = res$iter, converged=res$convergence))#
}#
#
# helper functions used by mixEM#
normalize = function(x){return(x/sum(x))}#
#
fixpoint = function(pi, matrix_lik, prior){  #
  pi = normalize(pmax(0,pi)) #avoid occasional problems with negative pis due to rounding#
  m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
  m.rowsum = rowSums(m)#
  classprob = m/m.rowsum #an n by k matrix#
  pinew = normalize(colSums(classprob) + prior - 1)#
  return(pinew)#
}#
#
negpenloglik = function(pi,matrix_lik,prior){return(-penloglik(pi,matrix_lik,prior))}#
#
penloglik = function(pi, matrix_lik, prior){#
  pi = normalize(pmax(0,pi))#
  m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
  m.rowsum = rowSums(m)#
  loglik = sum(log(m.rowsum))#
  subset = (prior != 1.0)#
  priordens = sum((prior-1)[subset]*log(pi[subset]))#
  return(loglik+priordens)#
}#
#
#The kth element of this vector is the derivative #
#of the loglik for $\pi=(\pi_0,...,1-\pi_0,...)$ with respect to $\pi_0$ at $\pi_0=1$.#
gradient = function(matrix_lik){#
  n = nrow(matrix_lik)#
  grad = n - colSums(matrix_lik/matrix_lik[,1]) #
  return(grad)#
}#
#
# mixEM = function(matrix_lik, prior, pi.init = NULL,tol=0.0001, maxiter=5000){#
#   n=nrow(matrix_lik)#
#   k=ncol(matrix_lik)#
#   B = rep(0,maxiter)#
#   pi = pi.init#
#   if(is.null(pi.init)){#
#     pi = rep(1/k,k)# Use as starting point for pi#
#   } #
#   pi = ifelse(pi<1e-5,1e-5,pi) #set any estimates that are too small to be just very small#
#   pi = normalize(pi)#
#   #
#   loglik = rep(0,maxiter)#
#   priordens= rep(0,maxiter)#
#   m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
#   m.rowsum = rowSums(m)#
#   loglik[1] = sum(log(m.rowsum))#
#   priordens[1] = sum((prior-1)*log(pi)) #
#   classprob = m/m.rowsum #an n by k matrix#
#   i=1#
#   if(maxiter >= 2){#
#     for(i in 2:maxiter){  #
#       pi = colSums(classprob) + prior-1#
#       pi = ifelse(pi<1e-5,1e-5,pi) #set any estimates that are less than zero, which can happen with prior<1, to 0#
#       pi = normalize(pi)#
#         #
#       #Now re-estimate pi#
#       m  = t(pi * t(matrix_lik)) #
#       m.rowsum = rowSums(m)#
#       loglik[i] = sum(log(m.rowsum))#
#       priordens[i] = sum((prior-1)*log(pi)) #
#       classprob = m/m.rowsum#
#     #
#     #
#       if(abs(loglik[i]+priordens[i]-loglik[i-1]-priordens[i-1])<tol) break;#
#     }#
#   }#
#   converged=(abs(loglik[i]+priordens[i]-loglik[i-1]-priordens[i-1])<tol)#
#   if(!converged){#
#       warning("EM algorithm in function mixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
#   }#
#   return(list(pihat = pi, B=loglik[1:i], #
#               niter = i, converged=converged))#
# }#
#estimate mixture proportions of sigmaa by EM algorithm#
#prior gives the parameter of a Dirichlet prior on pi#
#(prior is used to encourage results towards smallest value of sigma when#
#likelihood is flat)#
#nullcheck indicates whether to check whether the loglike exceeds the null#
#(may not want to use if prior is used)#
#VB provides an approach to estimate the approximate posterior distribution#
#of mixture proportions of sigmaa by variational Bayes method#
#(use Dirichlet prior and approximate Dirichlet posterior)#
#if cxx TRUE use cpp version of R function mixEM#
EMest = function(betahat,sebetahat,g,prior,null.comp=1,nullcheck=TRUE,VB=FALSE, maxiter=5000, cxx=TRUE, df=NULL){ #
  pi.init = g$pi#
  k=ncomp(g)#
  n = length(betahat)#
  tol = min(0.1/n,1e-5) # set convergence criteria to be more stringent for larger samples#
  if(is.null(df)){#
    matrix_lik = t(compdens_conv(g,betahat,sebetahat))#
  }#
  else{#
    matrix_lik = t(compdens_conv_t(g,betahat,sebetahat,df))#
  }#
  #checks whether the gradient at pi0=1 is positive (suggesting that this is a fixed point)#
  #if(nullcheck){#
  #  if(all(gradient(matrix_lik)>=0)){#
  #    pi.init=rep(0,k)#
  #    pi.init[null.comp]=1 #this will make pi.init=(1,0,0...,0) which is a fixed point of the EM#
  #  }#
  #}#
  if(VB==TRUE){#
    EMfit=mixVBEM(matrix_lik,prior,maxiter=maxiter)}#
  else{#
    if (cxx==TRUE){#
      EMfit = cxxMixEM(matrix_lik,prior,pi.init,1e-5, maxiter) #currently use different convergence criteria for cxx version #
      if(!EMfit$converged){#
        warning("EM algorithm in function cxxMixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
      }#
    }#
    else{#
      EMfit = mixEM(matrix_lik,prior,pi.init,tol, maxiter)#
      if(!EMfit$converged & !(maxiter==1)){#
        warning("EM algorithm in function mixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
      }#
    }#
  }#
  pi = EMfit$pihat     #
  penloglik = EMfit$B #
  converged = EMfit$converged#
  niter = EMfit$niter#
  loglik.final =  penloglik(pi,matrix_lik,1) #compute penloglik without penalty#
  null.loglik = sum(log(matrix_lik[,null.comp]))  #
  if(nullcheck==TRUE & VB==FALSE){ #null check doesn't work with VB yet#
    pinull = rep(0,k)#
    pinull[null.comp]=1#
    null.penloglik = penloglik(pinull,matrix_lik,prior)#
    final.penloglik = penloglik(pi,matrix_lik,prior)#
    if(null.penloglik > final.penloglik){ #check whether exceeded "null" likelihood where everything is null#
      pi=pinull#
      loglik.final=penloglik(pi,matrix_lik,1)#
    }#
  }#
  g$pi=pi#
  return(list(loglik=loglik.final,null.loglik=null.loglik,#
              matrix_lik=matrix_lik,converged=converged,g=g))#
}#
#' @title Compute Posterior#
#'#
#' @description Return the posterior on beta given a prior (g) that is a mixture of normals (class normalmix) #
#' and observation betahat \sim N(beta,sebetahat)#
#'#
#' @details This can be used to obt#
#'#
#' @param g: a normalmix with components indicating the prior; works only if g has means 0#
#' @param betahat (n vector of observations) #
#' @param sebetahat (n vector of standard errors/deviations of observations)#
#' #
#' @return A list, (pi1,mu1,sigma1) whose components are each k by n matrices#
#' where k is number of mixture components in g, n is number of observations in betahat#
#' #
#' @export#
#' #
#' #
posterior_dist = function(g,betahat,sebetahat){#
  if(class(g)!="normalmix"){#
    stop("Error: posterior_dist implemented only for g of class normalmix")#
  }#
  pi0 = g$pi#
  mu0 = g$mean#
  sigma0 = g$sd  #
  k= length(pi0)#
  n= length(betahat)#
  if(!all.equal(g$mean,rep(0,k))) stop("Error: posterior_dist currently only implemented for zero-centered priors")#
  pi1 = pi0 * t(matrix_dens(betahat,sebetahat,sigma0))#
  pi1 = apply(pi1, 2, normalize) #pi1 is now an k by n matrix#
  #make k by n matrix versions of sigma0^2 and sebetahat^2#
  # and mu0 and betahat#
  s0m2 = matrix(sigma0^2,nrow=k,ncol=n,byrow=FALSE)#
  sebm2 = matrix(sebetahat^2,nrow=k,ncol=n, byrow=TRUE)#
  mu0m = matrix(mu0,nrow=k,ncol=n,byrow=FALSE)#
  bhatm = matrix(betahat,nrow=k,ncol=n,byrow=TRUE)#
  sigma1 = (s0m2*sebm2/(s0m2 + sebm2))^(0.5)  #
  w = sebm2/(s0m2 + sebm2)#
  mu1 = w*mu0m + (1-w)*bhatm#
  #WHERE DATA ARE MISSING, SET POSTERIOR = PRIOR#
  ismiss = (is.na(betahat) | is.na(sebetahat)) #
  pi1[,ismiss] = pi0#
  mu1[,ismiss] = mu0#
  sigma1[,ismiss] = sigma0#
  return(list(pi=pi1,mu=mu1,sigma=sigma1))#
}#
#
#return matrix of densities of observations (betahat) #
# assuming betahat_j \sim N(0, sebetahat_j^2 + sigmaavec_k^2)#
#normalized by maximum of each column#
#INPUT#
#betahat is n vector, #
#sebetahat is n vector, #
#sigmaavec is k vector#
#return is n by k matrix of the normal likelihoods, #
# with (j,k)th element the density of N(betahat_j; mean=0, var = sebetahat_j^2 + sigmaavec_k^2)#
#normalized to have maximum 1 in each column#
matrix_dens = function(betahat, sebetahat, sigmaavec){#
  k = length(sigmaavec)#
  n = length(betahat)#
  ldens = dnorm(betahat,0,sqrt(outer(sebetahat^2,sigmaavec^2,FUN="+")),log=TRUE)#
  maxldens = apply(ldens, 1, max)#
  ldens = ldens - maxldens#
  return(exp(ldens))#
}#
#
#return the "effective" estimate#
#that is the effect size betanew whose z score betanew/se#
#would give the same p value as betahat/se compared to a t with df#
effective.effect=function(betahat,se,df){#
  p = pt(betahat/se,df)#
  qnorm(p,sd=se)#
}#
#' @title Function to compute q values from local false discovery rates#
#'#
#' @description Computes q values from a vector of local fdr estimates#
#'#
#' @details The q value for a given lfdr is an estimate of the (tail) False Discovery Rate #
#' for all findings with a smaller lfdr, and is found by the average of the lfdr for#
#' all more significant findings. See Storey (2003), Annals of Statistics, for definition of q value.  #
#' #
#' #
#' @param lfdr, a vector of local fdr estimates#
#'#
#' @return vector of q values#
#' #
#' @export#
qval.from.lfdr = function(lfdr){#
  o = order(lfdr)#
  qvalue=rep(NA,length(lfdr))#
  qvalue[o] = (cumsum(sort(lfdr))/(1:sum(!is.na(lfdr))))#
  return(qvalue)#
}#
#
# try to select a default range for the sigmaa values#
# that should be used, based on the values of betahat and sebetahat#
# mult is the multiplier by which the sds differ across the grid#
autoselect.mixsd = function(betahat,sebetahat,mult){#
  sigmaamin = min(sebetahat)/10 #so that the minimum is small compared with measurement precision#
  if(all(betahat^2<sebetahat^2)){#
    sigmaamax = 8*sigmaamin #to deal with the occassional odd case where this could happen; 8 is arbitrary#
  } else {#
    sigmaamax = 2*sqrt(max(betahat^2-sebetahat^2)) #this computes a rough largest value you'd want to use, based on idea that sigmaamax^2 + sebetahat^2 should be at least betahat^2   #
  }#
  if(mult==0){#
    return(c(0,sigmaamax/2))#
  }else{#
    npoint = ceiling(log2(sigmaamax/sigmaamin)/log2(mult))#
    return(mult^((-npoint):0) * sigmaamax)#
  }#
}#
#' @title Summary method for ash object#
#'#
#' @description Print summary of fitted ash object#
#'#
#' @details See readme for more details#
#' #
#' @export#
#' #
summary.ash=function(a){#
  print(a$fitted.g)#
  print(tail(a$fit$loglik,1),digits=10)#
  print(a$fit$converged)#
}#
#
#' @title Print method for ash object#
#'#
#' @description Print the fitted distribution of beta values in the EB hierarchical model#
#'#
#' @details None#
#' #
#' @export#
#' #
print.ash =function(a){#
  print(a$fitted.g)#
}#
#
#' @title Plot method for ash object#
#'#
#' @description Plot the density of the underlying fitted distribution#
#'#
#' @details None#
#' #
#' @export#
#' #
plot.ash = function(a,xmin,xmax,...){#
  x = seq(xmin,xmax,length=1000)#
  y = density(a,x)#
  plot(y,type="l",...)#
}#
#
#compute the predictive density of an observation#
#given the fitted ash object a and the vector se of standard errors#
#not implemented yet#
predictive=function(a,se){#
}#
#' @title Get fitted loglikelihood for ash object#
#'#
#' @description Return the log-likelihood of the data under the fitted distribution#
#'#
#' @param a the fitted ash object#
#'#
#' @details None#
#' #
#' @export#
#' #
#'#
get_loglik = function(a){#
  return(tail(a$fit$loglik,1))#
}#
#
#' @title Get pi0 estimate for ash object#
#'#
#' @description Return estimate of the null proportion, pi0#
#'#
#' @param a the fitted ash object#
#'#
#' @details Extracts the estimate of the null proportion, pi0, from the object a#
#' #
#' @export#
#' #
get_pi0 = function(a){#
  null.comp = comp_sd(a$fitted.g)==0#
  return(sum(a$fitted.g$pi[null.comp]))#
}#
#
#' @title Compute loglikelihood for data from ash fit#
#'#
#' @description Return the log-likelihood of the data betahat, with standard errors betahatsd, #
#' under the fitted distribution in the ash object. #
#' #
#'#
#' @param a the fitted ash object#
#' @param betahat the data#
#' @param betahatsd the observed standard errors#
#' @param zscores indicates whether ash object was originally fit to z scores #
#' @details None#
#' #
#' @export#
#' #
#'#
loglik.ash = function(a,betahat,betahatsd,zscores=FALSE){#
  g=a$fitted.g#
  FUN="+"#
  if(zscores==TRUE){#
    g$sd = sqrt(g$sd^2+1) #
    FUN="*"#
  }#
  return(loglik_conv(g,betahat, betahatsd,FUN))#
}#
#
#' @title Density method for ash object#
#'#
#' @description Return the density of the underlying fitted distribution#
#'#
#' @param a the fitted ash object#
#' @param x the vector of locations at which density is to be computed#
#'#
#' @details None#
#' #
#' @export#
#' #
#'#
density.ash=function(a,x){list(x=x,y=dens(a$fitted.g,x))}#
#
#' @title cdf method for ash object#
#'#
#' @description Computed the cdf of the underlying fitted distribution#
#'#
#' @param a the fitted ash object#
#' @param x the vector of locations at which cdf is to be computed#
#' @param lower.tail (default=TRUE) whether to compute the lower or upper tail#
#'#
#' @details None#
#' #
#' @export#
#' #
#'#
cdf.ash=function(a,x,lower.tail=TRUE){#
  return(list(x=x,y=mixcdf(a$fitted.g,x,lower.tail)))#
}#
#return the KL-divergence between 2 dirichlet distributions#
#p,q are the vectors of dirichlet parameters of same lengths#
diriKL = function(p,q){#
  p.sum = sum(p)#
  q.sum = sum(q)#
  k = length(q)#
  KL = lgamma(q.sum)-lgamma(p.sum)+sum((q-p)*(digamma(q)-digamma(rep(q.sum,k))))+sum(lgamma(p)-lgamma(q))#
  return(KL)#
}#
#
#helper function for VBEM#
VB.update = function(matrix_lik, pipost){#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost * matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  B = sum(classprob*log(avgpipost*matrix_lik),na.rm=TRUE) - diriKL(prior,pipost) #negative free energy#
  return(list(classprob=classprob,B=B))#
}#
################################## GENERIC FUNCTIONS #############################
# find matrix of densities at y, for each component of the mixture#
# INPUT y is an n-vector#
# OUTPUT k by n matrix of densities#
compdens = function(x,y,log=FALSE){#
  UseMethod("compdens")#
}#
compdens.default = function(x,y,log=FALSE){#
  stop(paste("Invalid class", class(m), "for first argument in",  match.call()))  #
}#
#
#standard deviations#
comp_sd = function(m){#
  UseMethod("comp_sd")#
}#
comp_sd.default = function(m){#
  stop("method comp_sd not written for this class")#
}#
#
#second moments#
comp_mean2 = function(m){#
  UseMethod("comp_mean2")#
}#
comp_mean2.default = function(m){#
  comp_sd(m)^2 + comp_mean(m)^2#
}#
#return the overall mean of the mixture#
mixmean = function(m){#
  UseMethod("mixmean")#
}#
mixmean.default = function(m){#
  sum(m$pi * comp_mean(m))#
}#
#
#return the overall second moment of the mixture#
mixmean2 = function(m){#
  UseMethod("mixmean2")#
}#
mixmean2.default = function(m){#
  sum(m$pi * comp_mean2(m))#
}#
#
#return the overall sd of the mixture#
mixsd = function(m){#
  UseMethod("mixsd")#
}#
mixsd.default = function(m){#
  sqrt(mixmean2(m)-mixmean(m)^2)#
}#
#
#means#
comp_mean = function(m){#
  UseMethod("comp_mean")#
}#
comp_mean.default = function(m){#
  stop("method comp_mean not written for this class")#
}#
#
#number of components#
ncomp = function(m){#
  UseMethod("ncomp")#
}#
ncomp.default = function(m){#
  return(length(m$pi))#
}#
#
#return mixture proportions, a generic function#
mixprop = function(m){#
  UseMethod("mixprop")#
}#
mixprop.default = function(m){#
  m$pi#
}#
#
#' @title mixcdf#
#'#
#' @description Returns cdf for a mixture (generic function)#
#' #
#' @details None#
#' #
#' @param x a mixture (eg of type normalmix or unimix)#
#' @param y locations at which cdf to be computed#
#' @param lower.tail: boolean indicating whether to report lower tail#
#' #
#' @return an object of class normalmix#
#' #
#' @export#
#' #
#' @examples mixcdf(normalmix(c(0.5,0.5),c(0,0),c(1,2)),seq(-4,4,length=100))#
#' #
mixcdf = function(x,y,lower.tail=TRUE){#
  UseMethod("mixcdf")#
}#
#' @title mixcdf.default#
#' @export#
#' #
mixcdf.default = function(x,y,lower.tail=TRUE){#
  x$pi %*% comp_cdf(x,y,lower.tail)#
}#
#
#find cdf for each component, a generic function#
comp_cdf = function(x,y,lower.tail=TRUE){#
  UseMethod("comp_cdf")#
}#
comp_cdf.default = function(x,y,lower.tail=TRUE){#
  stop("comp_cdf not implemented for this class")#
}#
#find density at y, a generic function#
dens = function(x,y){#
  UseMethod("dens")#
}#
dens.default = function(x,y){#
  return (x$pi %*% compdens(x, y))#
}#
#
#find log likelihood of data in x (a vector) for mixture in m#
loglik = function(m,x){#
  UseMethod("loglik")#
}#
loglik.default = function(m,x){#
  sum(log(dens(m,x)))#
}#
#
#find log likelihood of data in betahat, when #
#the mixture m is convolved with a normal with sd betahatsd#
#betahatsd is an n vector#
#betahat is an n vector#
#' @title loglik_conv#
#' #
#' @export#
#' #
loglik_conv = function(m,betahat,betahatsd,FUN="+"){#
  UseMethod("loglik_conv")#
}#
#' @title loglik_conv.default#
#' #
#' @export#
#' #
loglik_conv.default = function(m,betahat,betahatsd,FUN="+"){#
  sum(log(dens_conv(m,betahat,betahatsd,FUN)))#
}#
#
#compute the density of the components of the mixture m#
#when convoluted with a normal with standard deviation s#
#the density is evaluated at x#
#x and s are n-vectors#
#m is a mixture with k components#
#output is a k by n matrix of densities#
compdens_conv = function(m, x, s, FUN="+"){#
  UseMethod("compdens_conv")#
}#
compdens_conv.default = function(m,x, s,FUN="+"){#
  stop(paste("Invalid class", class(m), "for first argument in",  match.call()))  #
}#
#
#compute density of mixture m convoluted with normal of sd (s)#
#at locations x#
#m is a mixture#
#x is an n vector#
#s is an n vector or integer#
dens_conv = function(m,x,s,FUN="+"){#
  UseMethod("dens_conv")#
}#
dens_conv.default = function(m,x,s,FUN="+"){#
  colSums(m$pi * compdens_conv(m,x,s,FUN))#
}#
#
#compute the posterior prob that each observation#
#came from each component of the mixture m#
#output a k by n vector of probabilities#
#computed by weighting the component densities by pi#
#and then normalizing#
comppostprob=function(m,x,s){#
 UseMethod("comppostprob") #
}#
comppostprob.default = function(m,x,s){#
  tmp= (t(m$pi * compdens_conv(m,x,s))/dens_conv(m,x,s))#
  ismissing = (is.na(x) | is.na(s))#
  tmp[ismissing,]=m$pi#
  t(tmp)#
}#
# evaluate cdf of posterior distribution of beta at c#
# m is the prior on beta, a mixture#
# c is location of evaluation#
# assumption is betahat | beta \sim N(beta,sebetahat)#
# m is a mixture with k components#
# c a scalar#
# betahat, sebetahat are n vectors #
# output is a k by n matrix#
compcdf_post=function(m,c,betahat,sebetahat){#
  UseMethod("compcdf_post")#
}#
compcdf_post.default=function(m,c,betahat,sebetahat){#
  stop("method compcdf_post not written for this class")#
}#
cdf_post = function(m,c,betahat,sebetahat){#
  UseMethod("cdf_post")#
}#
cdf_post.default=function(m,c,betahat,sebetahat){#
  colSums(comppostprob(m,betahat,sebetahat)*compcdf_post(m,c,betahat,sebetahat))#
}#
#
#output posterior mean for beta for prior mixture m,#
#given observations betahat, sebetahat#
postmean = function(m, betahat,sebetahat){#
  UseMethod("postmean")#
}#
postmean.default = function(m,betahat,sebetahat){#
  colSums(comppostprob(m,betahat,sebetahat) * comp_postmean(m,betahat,sebetahat))#
}#
#
#output posterior mean-squared value for beta for prior mixture m,#
#given observations betahat, sebetahat#
postmean2 = function(m, betahat,sebetahat){#
  UseMethod("postmean2")#
}#
postmean2.default = function(m,betahat,sebetahat){#
  colSums(comppostprob(m,betahat,sebetahat) * comp_postmean2(m,betahat,sebetahat))#
}#
#
#output posterior sd for beta for prior mixture m,#
#given observations betahat, sebetahat#
postsd = function(m, betahat,sebetahat){#
  UseMethod("postsd")#
}#
postsd.default = function(m,betahat,sebetahat){#
  sqrt(postmean2(m,betahat,sebetahat)-postmean(m,betahat,sebetahat)^2)#
}#
#
#output posterior mean-squared value for beta for prior mixture m,#
#given observations betahat, sebetahat#
comp_postmean2 = function(m, betahat,sebetahat){#
  UseMethod("comp_postmean2")#
}#
comp_postmean2.default = function(m,betahat,sebetahat){#
  comp_postsd(m,betahat,sebetahat)^2 + comp_postmean(m,betahat,sebetahat)^2#
}#
#output posterior mean for beta for each component of prior mixture m,#
#given observations betahat, sebetahat#
comp_postmean = function(m, betahat,sebetahat){#
  UseMethod("comp_postmean")#
}#
comp_postmean.default = function(m,betahat,sebetahat){#
  stop("method comp_postmean not written for this class")#
}#
#
#output posterior sd for beta for each component of prior mixture m,#
#given observations betahat, sebetahat#
comp_postsd = function(m, betahat,sebetahat){#
  UseMethod("comp_postsd")#
}#
comp_postsd.default = function(m,betahat,sebetahat){#
  stop("method comp_postsd not written for this class")#
}#
#
#find nice limits of mixture m for plotting#
min_lim = function(m){#
  UseMethod("min_lim")#
}#
min_lim.default=function(m){#
  -5#
}#
#
max_lim = function(m){#
  UseMethod("max_lim")#
}#
max_lim.default=function(m){#
  5#
}#
#plot density of mixture#
plot_dens = function(m,npoints=100,...){#
  UseMethod("plot_dens")#
}#
plot_dens.default = function(m,npoints=100,...){#
  x = seq(min_lim(m),max_lim(m),length=npoints)#
  plot(x,dens(m,x),type="l",xlab="density",ylab="x",...)#
}#
#
plot_post_cdf = function(m,betahat,sebetahat,npoints=100,...){#
  UseMethod("plot_post_cdf")#
}#
plot_post_cdf.default = function(m,betahat,sebetahat,npoints=100,...){#
  x = seq(min_lim(m),max_lim(m),length=npoints)#
  x_cdf = vapply(x,cdf_post,FUN.VALUE=betahat,m=m,betahat=betahat,sebetahat=sebetahat)#
  plot(x,x_cdf,type="l",xlab="x",ylab="cdf",...)#
 # for(i in 2:nrow(x_cdf)){#
 #   lines(x,x_cdf[i,],col=i)#
 # }#
}#
#
############################### METHODS FOR normalmix class ############################
#
#' @title Constructor for normalmix class#
#'#
#' @description Creates an object of class normalmix (finite mixture of univariate normals)#
#' #
#' @details None#
#' #
#' @param pi vector of mixture proportions#
#' @param mean vector of means#
#' @param sd: vector of standard deviations#
#' #
#' @return an object of class normalmix#
#' #
#' @export#
#' #
#' @examples normalmix(c(0.5,0.5),c(0,0),c(1,2))#
#' #
normalmix = function(pi,mean,sd){#
  structure(data.frame(pi,mean,sd),class="normalmix")#
}#
#
comp_sd.normalmix = function(m){#
  m$sd#
}#
#
comp_mean.normalmix = function(m){#
  m$mean#
}#
#
compdens.normalmix = function(x,y,log=FALSE){#
  k=ncomp(x)#
  n=length(y)#
  d = matrix(rep(y,rep(k,n)),nrow=k)#
  return(matrix(dnorm(d, x$mean, x$sd, log),nrow=k))  #
}#
#
#density of convolution of each component of a normal mixture with N(0,s^2) at x#
# x an n-vector at which density is to be evaluated#
#return a k by n matrix#
#Note that convolution of two normals is normal, so it works that way#
compdens_conv.normalmix = function(m, x, s,FUN="+"){#
  if(length(s)==1){s=rep(s,length(x))}#
  sdmat = sqrt(outer(s^2,m$sd^2,FUN)) #n by k matrix of standard deviations of convolutions#
  return(t(dnorm(outer(x,m$mean,FUN="-")/sdmat)/sdmat))#
}#
comp_cdf.normalmix = function(x,y,lower.tail=TRUE){#
  vapply(y,pnorm,x$mean,x$mean,x$sd,lower.tail)#
}#
#
#c is a scalar#
#m a mixture with k components#
#betahat a vector of n observations#
#sebetahat an n vector of standard errors#
#return a k by n matrix of the posterior cdf#
compcdf_post.normalmix=function(m,c,betahat,sebetahat){#
  k = length(m$pi)#
  n=length(betahat)#
  #compute posterior standard deviation (s1) and posterior mean (m1)#
  s1 = sqrt(outer(sebetahat^2,m$sd^2,FUN="*")/outer(sebetahat^2,m$sd^2,FUN="+"))#
  ismissing = (is.na(betahat) | is.na(sebetahat))#
  s1[ismissing,]=m$sd#
  m1 = t(comp_postmean(m,betahat,sebetahat))#
  t(pnorm(c,mean=m1,sd=s1))#
}#
#
#return posterior mean for each component of prior m, given observations betahat and sebetahat#
#input, m is a mixture with k components#
#betahat, sebetahat are n vectors#
#output is a k by n matrix#
comp_postmean.normalmix = function(m,betahat,sebetahat){#
  tmp=(outer(sebetahat^2,m$mean, FUN="*") + outer(betahat,m$sd^2, FUN="*"))/outer(sebetahat^2,m$sd^2,FUN="+")#
  ismissing = (is.na(betahat) | is.na(sebetahat))#
  tmp[ismissing,]=m$mean #return prior mean when missing data#
  t(tmp)#
}#
#
#return posterior mean for each component of prior m, given observations betahat and sebetahat#
#input, m is a mixture with k components#
#betahat, sebetahat are n vectors#
#output is a k by n matrix#
comp_postsd.normalmix = function(m,betahat,sebetahat){#
  t(sqrt(outer(sebetahat^2,m$sd^2,FUN="*")/outer(sebetahat^2,m$sd^2,FUN="+")))#
}#
############################### METHODS FOR unimix class ############################
#
#constructor; pi, a and b are vectors; kth component is Uniform(a[k],b[k])#
unimix = function(pi,a,b){#
  structure(data.frame(pi,a,b),class="unimix")#
}#
#
comp_cdf.unimix = function(m,y,lower.tail=TRUE){#
  vapply(y,punif,m$a,min=m$a,max=m$b,lower.tail)#
}#
#
comp_sd.unimix = function(m){#
  (m$b-m$a)/sqrt(12)#
}#
#
comp_mean.unimix = function(m){#
  (m$a+m$b)/2#
}#
compdens.unimix = function(x,y,log=FALSE){#
  k=ncomp(x)#
  n=length(y)#
  d = matrix(rep(y,rep(k,n)),nrow=k)#
  return(matrix(dunif(d, x$a, x$b, log),nrow=k))  #
}#
#
#density of convolution of each component of a unif mixture with N(0,s) at x#
# x an n-vector#
#return a k by n matrix#
compdens_conv.unimix = function(m, x, s, FUN="+"){#
  if(FUN!="+") stop("Error; compdens_conv not implemented for uniform with FUN!=+")#
  compdens= t(pnorm(outer(x,m$a,FUN="-")/s)-pnorm(outer(x,m$b,FUN="-")/s))/(m$b-m$a)#
  compdens[m$a==m$b,]=t(dnorm(outer(x,m$a,FUN="-")/s)/s)[m$a==m$b,]#
  return(compdens)#
}#
#c is a scalar#
#m a mixture with k components#
#betahat a vector of n observations#
#sebetahat an n vector of standard errors#
#return a k by n matrix of the posterior cdf#
compcdf_post.unimix=function(m,c,betahat,sebetahat){#
  k = length(m$pi)#
  n=length(betahat)#
  tmp = matrix(1,nrow=k,ncol=n)#
  tmp[m$a > c,] = 0#
  subset = m$a<=c & m$b>c # subset of components (1..k) with nontrivial cdf#
  if(sum(subset)>0){#
    pna = pnorm(outer(betahat,m$a[subset],FUN="-")/sebetahat)#
    pnc = pnorm(outer(betahat,rep(c,sum(subset)),FUN="-")/sebetahat)#
    pnb = pnorm(outer(betahat,m$b[subset],FUN="-")/sebetahat)#
    tmp[subset,] = t((pnc-pna)/(pnb-pna))#
  }#
  subset = (m$a == m$b) #subset of components with trivial cdf#
  tmp[subset,]= rep(m$a[subset] <= c,n)#
  tmp#
}#
#
my_etruncnorm= function(a,b,mean=0,sd=1){#
  alpha = (a-mean)/sd#
  beta =  (b-mean)/sd#
 #Flip the onese where both are positive, as the computations are more stable#
  #when both negative#
  flip = (alpha>0 & beta>0)#
  flip[is.na(flip)]=FALSE #deal with NAs#
  alpha[flip]= -alpha[flip]#
  beta[flip]=-beta[flip]#
  tmp= (-1)^flip * (mean+sd*etruncnorm(alpha,beta,0,1))#
  max_alphabeta = ifelse(alpha<beta, beta,alpha)#
  max_ab = ifelse(alpha<beta,b,a)#
  toobig = max_alphabeta<(-30)#
  toobig[is.na(toobig)]=FALSE #
  tmp[toobig] = max_ab[toobig]#
  tmp#
}#
#return posterior mean for each component of prior m, given observations betahat and sebetahat#
#input, m is a mixture with k components#
#betahat, sebetahat are n vectors#
#output is a k by n matrix#
#note that with uniform prior, posterior is truncated normal, so#
#this is computed using formula for mean of truncated normal #
comp_postmean.unimix = function(m,betahat,sebetahat){#
#   k= ncomp(m)#
#   n=length(betahat)#
#   a = matrix(m$a,nrow=n,ncol=k,byrow=TRUE)#
#   b = matrix(m$b,nrow=n,ncol=k,byrow=TRUE)#
#   matrix(etruncnorm(a,b,betahat,sebetahat),nrow=k,byrow=TRUE)#
  #note: etruncnorm is more stable for a and b negative than positive#
  #so maybe use this, and standardize to make the whole more stable.#
  alpha = outer(-betahat, m$a,FUN="+")/sebetahat#
  beta = outer(-betahat, m$b, FUN="+")/sebetahat#
  tmp = betahat + sebetahat*my_etruncnorm(alpha,beta,0,1)#
  ismissing = is.na(betahat) | is.na(sebetahat)#
  tmp[ismissing,]= (m$a+m$b)/2#
  t(tmp)#
#   t(#
#     betahat + sebetahat* #
#       exp(dnorm(alpha,log=TRUE)- pnorm(alpha,log=TRUE))#
#    * #
#       (-expm1(dnorm(beta,log=TRUE)-dnorm(alpha,log=TRUE)))#
#     /#
#       (expm1(pnorm(beta,log=TRUE)-pnorm(alpha,log=TRUE)))#
#   )#
}#
#
#not yet implemented!#
#just returns 0s for now#
comp_postsd.unimix = function(m,betahat,sebetahat){#
  print("Warning: Posterior SDs not yet implemented for uniform components")#
  k= ncomp(m)#
  n=length(betahat)#
  return(matrix(NA,nrow=k,ncol=n)) #
}#
######################################################################################
########################## functions for student.t likelihood ########################
#compute the density of the components of the mixture m#
#when convoluted with a scaled (se) student.t with df v #
compdens_conv_t = function(m, x, s, v, FUN="+"){#
  UseMethod("compdens_conv_t")#
}#
compdens_conv_t.default = function(m,x, s,v,FUN="+"){#
  stop(paste("Invalid class", class(m), "for first argument in",  match.call()))#
}#
#density of convolution of each component of a normal mixture with s*t(v) at x#
# x an n-vector at which density is to be evaluated#
#return a k by n matrix#
#Note that convolution of two normals is normal, so it works that way#
compdens_conv_t.normalmix = function(m, x, s, v,FUN="+"){#
  stop("Error: normal mixture for student-t likelihood is not yet implemented")#
}#
#density of convolution of each component of a unif mixture with s*t(v) at x#
# x an n-vector#
#return a k by n matrix#
compdens_conv_t.unimix = function(m, x, s, v , FUN="+"){#
  if(FUN!="+") stop("Error; compdens_conv_t not implemented for uniform with FUN!=+")#
  compdens= t(pt(outer(x,m$a,FUN="-")/s,df=v)-pt(outer(x,m$b,FUN="-")/s,df=v))/(m$b-m$a)#
  compdens[m$a==m$b,]=t(dt(outer(x,m$a,FUN="-")/s,df=v)/s)[m$a==m$b,]#
  return(compdens)#
}#
#
#compute density of mixture m convoluted with student t with df v#
#at locations x#
#m is a mixture#
#x is an n vector#
#s is an n vector or integer#
dens_conv_t = function(m,x,s,v,FUN="+"){#
  UseMethod("dens_conv_t")#
}#
dens_conv_t.default = function(m,x,s,v,FUN="+"){#
  colSums(m$pi * compdens_conv_t(m,x,s,v,FUN))#
}#
#
#compute the posterior prob that each observation#
#came from each component of the mixture m#
#output a k by n vector of probabilities#
#computed by weighting the component densities by pi#
#and then normalizing#
comppostprob_t=function(m,x,s,v){#
  UseMethod("comppostprob_t") #
}#
comppostprob_t.default = function(m,x,s,v){#
  tmp= (t(m$pi * compdens_conv_t(m,x,s,v))/dens_conv_t(m,x,s,v))#
  ismissing = (is.na(x) | is.na(s))#
  tmp[ismissing,]=m$pi#
  t(tmp)#
}#
cdf_post_t = function(m,c,betahat,sebetahat,v){#
  UseMethod("cdf_post_t")#
}#
cdf_post_t.default=function(m,c,betahat,sebetahat,v){#
  colSums(comppostprob_t(m,betahat,sebetahat,v)*compcdf_post_t(m,c,betahat,sebetahat,v))#
}#
#
compcdf_post_t=function(m,c,betahat,sebetahat,v){#
  UseMethod("compcdf_post_t")#
}#
compcdf_post_t.default=function(m,c,betahat,sebetahat,v){#
  stop("method compcdf_post_t not written for this class")#
}#
#
compcdf_post_t.unimix=function(m,c,betahat,sebetahat,v){#
  k = length(m$pi)#
  n=length(betahat)#
  tmp = matrix(1,nrow=k,ncol=n)#
  tmp[m$a > c,] = 0#
  subset = m$a<=c & m$b>c # subset of components (1..k) with nontrivial cdf#
  if(sum(subset)>0){#
    pna = pt(outer(betahat,m$a[subset],FUN="-")/sebetahat, df=v)#
    pnc = pt(outer(betahat,rep(c,sum(subset)),FUN="-")/sebetahat, df=v)#
    pnb = pt(outer(betahat,m$b[subset],FUN="-")/sebetahat, df=v)#
    tmp[subset,] = t((pnc-pna)/(pnb-pna))#
  }#
  subset = (m$a == m$b) #subset of components with trivial cdf#
  tmp[subset,]= rep(m$a[subset] <= c,n)#
  tmp#
}#
#
#output posterior mean for beta for prior mixture m,#
#given observations betahat, sebetahat#
postmean_t = function(m, betahat,sebetahat,v){#
  UseMethod("postmean_t")#
}#
postmean_t.default = function(m,betahat,sebetahat,v){#
  colSums(comppostprob_t(m,betahat,sebetahat,v) * comp_postmean_t(m,betahat,sebetahat,v))#
}#
#
#output posterior mean for beta for each component of prior mixture m,#
#given observations betahat, sebetahat#
comp_postmean_t = function(m, betahat,sebetahat,v){#
  UseMethod("comp_postmean_t")#
}#
comp_postmean_t.default = function(m,betahat,sebetahat,v){#
  stop("method comp_postmean_t not written for this class")#
}#
#
#return posterior mean for each component of prior m, given observations betahat and sebetahat#
#input, m is a mixture with k components#
#betahat, sebetahat are n vectors#
#output is a k by n matrix#
#note that with uniform prior, posterior is truncated student.t, so#
#this is computed using formula for mean of truncated student.t#
comp_postmean_t.unimix = function(m,betahat,sebetahat,v){#
  alpha = outer(-betahat, m$a, FUN="+")/sebetahat#
  beta = outer(-betahat, m$b, FUN="+")/sebetahat#
  tmp = betahat + sebetahat*my_etrunct(alpha,beta,v)#
  ismissing = is.na(betahat) | is.na(sebetahat)#
  tmp[ismissing,]= (m$a+m$b)/2#
  t(tmp)#
}#
# the mean of a truncated student.t#
# the result is from the paper 'Moments of truncated Student-t distribution' by H.-J Kim #
#
my_etrunct= function(a,b,v){#
  A = v+a^2#
  B = v+b^2#
  F_a = pt(a,df=v)#
  F_b = pt(b,df=v)#
  G = gamma((v-1)/2)*v^(v/2)/(2*(F_b-F_a)*gamma(v/2)*gamma(1/2))#
  tmp = ifelse(a==b,a,G*(A^(-(v-1)/2)-B^(-(v-1)/2)))#
  tmp#
}#
################# PostSD is not implemented for uniform mixture#
#
#output posterior sd for beta for prior mixture m,#
#given observations betahat, sebetahat#
postsd_t = function(m, betahat,sebetahat,v){#
  UseMethod("postsd_t")#
}#
postsd_t.default = function(m,betahat,sebetahat,v){#
  sqrt(postmean2_t(m,betahat,sebetahat,v)-postmean_t(m,betahat,sebetahat,v)^2)#
}#
postmean2_t = function(m, betahat,sebetahat,v){#
  UseMethod("postmean2_t")#
}#
postmean2_t.default = function(m,betahat,sebetahat,v){#
  colSums(comppostprob_t(m,betahat,sebetahat,v) * comp_postmean2_t(m,betahat,sebetahat,v))#
}#
#
comp_postmean2_t = function(m, betahat,sebetahat,v){#
  UseMethod("comp_postmean2_t")#
}#
comp_postmean2_t.default = function(m,betahat,sebetahat,v){#
  comp_postsd_t(m,betahat,sebetahat,v)^2 + comp_postmean_t(m,betahat,sebetahat,v)^2#
}#
comp_postsd_t = function(m, betahat,sebetahat,v){#
  UseMethod("comp_postsd_t")#
}#
comp_postsd_t.default = function(m,betahat,sebetahat,v){#
  stop("method comp_postsd not written for this class")#
}#
#not yet implemented!#
#just returns 0s for now#
comp_postsd_t.unimix = function(m,betahat,sebetahat,v){#
  print("Warning: Posterior SDs not yet implemented for uniform components")#
  k= ncomp(m)#
  n=length(betahat)#
  return(matrix(NA,nrow=k,ncol=n)) #
}
test=simdata(1000,1)#
test.ash=ash(test$betahat,test$betahatsd,method="fdr")
test=simdata(1000,1)#
test.ash=ash(test$betahat,test$betahatsd,method="fdr")#
test=simdata(1000,1)#
test.ash=ash(test$betahat,test$betahatsd,method="fdr")
test=simdata(1000,1)#
test.ash=ash(test$betahat,test$betahatsd,method="fdr")
v
test=simdata(1000,1)#
test.ash=ash(test$betahat,test$betahatsd,method="fdr")
a
a=2292945
b=2315434
b/a
88.9/114
88.1/125/8
88.1/125.8
0.98*1.7
1.7
0.98*1.72
1.7/1,6856
1.7/1.6856
0.92^4
0.716*0.875*0.9
0.564*0.9
' @useDynLib ashr#
#todo#
##
#' @title Main Adaptive SHrinkage function#
#'#
#' @description Takes vectors of estimates (betahat) and their standard errors (sebetahat), and applies#
#' shrinkage to them, using Empirical Bayes methods, to compute shrunk estimates for beta.#
#'#
#' @details See readme for more details#
#' #
#' @param betahat, a p vector of estimates #
#' @param sebetahat, a p vector of corresponding standard errors#
#' @param method: specifies how ash is to be run. Can be "shrinkage" (if main aim is shrinkage) or "fdr" (if main aim is to assess fdr or fsr)#
#' This is simply a convenient way to specify certain combinations of parameters: "shrinkage" sets pointmass=FALSE and prior="uniform";#
#' "fdr" sets pointmass=TRUE and prior="nullbiased".#
#' @param mixcompdist: distribution of components in mixture ("normal", "uniform" or "halfuniform")#
#'#
#' @param lambda1: multiplicative "inflation factor" for standard errors (like Genomic Control)#
#' @param lambda2: additive "inflation factor" for standard errors (like Genomic Control)#
#' @param nullcheck: whether to check that any fitted model exceeds the "null" likelihood#
#' in which all weight is on the first component#
#' @param df: appropriate degrees of freedom for (t) distribution of betahat/sebetahat#
#' @param randomstart: bool, indicating whether to initialize EM randomly. If FALSE, then initializes to prior mean (for EM algorithm) or prior (for VBEM)#
#' @param nullweight: scalar, the weight put on the prior of null under "fdr" method#
#' @param nonzeromean: bool, indicating whether to use a nonzero mean unimodal mixture(defaults to "FALSE")#
#' @param pointmass: bool, indicating whether to use a point mass at zero as one of components for a mixture distribution#
#' @param onlylogLR: bool, indicating whether to use this function to get logLR. Skip posterior prob, posterior mean, lfdr...#
#' @param prior: string, or numeric vector indicating Dirichlet prior on mixture proportions (defaults to "uniform", or 1,1...,1; also can be "nullbiased" 1,1/k-1,...,1/k-1 to put more weight on first component)#
#' @param mixsd: vector of sds for underlying mixture components #
#' @param VB: whether to use Variational Bayes to estimate mixture proportions (instead of EM to find MAP estimate)#
#' @param gridmult: the multiplier by which the default grid values for mixsd differ by one another. (Smaller values produce finer grids)#
#' @param minimal_output: if TRUE, just outputs the fitted g and the lfsr (useful for very big data sets where memory is an issue) #
#' @param g: the prior distribution for beta (usually estimated from the data; this is used primarily in simulated data to do computations with the "true" g)#
#' @param maxiter: maximum number of iterations of the EM algorithm#
#' @param cxx: flag to indicate whether to use the c++ (Rcpp) version#
#' #
#'#
#' @return a list with elements fitted.g is fitted mixture#
#' logLR : logP(D|mle(pi)) - logP(D|null)#
#' #
#' @export#
#' #
#' @examples #
#' beta = c(rep(0,100),rnorm(100))#
#' sebetahat = abs(rnorm(200,0,1))#
#' betahat = rnorm(200,beta,sebetahat)#
#' beta.ash = ash(betahat, sebetahat)#
#' summary(beta.ash)#
#' plot(betahat,beta.ash$PosteriorMean,xlim=c(-4,4),ylim=c(-4,4))#
#' #
#' #
#Things to do:#
# check sampling routine#
# check number of iterations#
ash = function(betahat,sebetahat,method = c("shrink","fdr"), #
               mixcompdist = c("normal","uniform","halfuniform"),#
               lambda1=1,lambda2=0,nullcheck=TRUE,df=NULL,randomstart=FALSE,#
               nullweight=10,nonzeromean=FALSE, #
               pointmass = FALSE, #
               onlylogLR = FALSE, #
               prior=c("uniform","nullbiased"), #
               mixsd=NULL, VB=FALSE,gridmult=sqrt(2),#
               minimaloutput=FALSE,#
               multiseqoutput=FALSE,#
               g=NULL,#
               maxiter = 5000,#
               cxx=FALSE){#
  #method provides a convenient interface to set a particular combinations of parameters for prior an#
  #If method is supplied, use it to set up specific values for these parameters; provide warning if values#
  #are also specified by user#
  #If method is not supplied use the user-supplied values (or defaults if user does not specify them)#
  if(!missing(method)){#
    method = match.arg(method) #
    if(method=="shrink"){#
      if(missing(prior)){#
        prior = "uniform"#
      } else {#
        warning("Specification of prior overrides default for method shrink")#
      }#
      if(missing(pointmass)){#
        pointmass=FALSE#
      } else {#
        warning("Specification of pointmass overrides default for method shrink")#
      }#
    }#
    if(method=="fdr"){#
      if(missing(prior)){#
        prior = "nullbiased"#
      } else {#
        warning("Specification of prior overrides default for method fdr")#
      }#
      if(missing(pointmass)){#
        pointmass=TRUE#
      } else {#
        warning("Specification of pointmass overrides default for method fdr")#
      }#
    }  #
  }#
  if(gridmult<=1&multiseqoutput!=TRUE)#
    stop("gridmult must be > 1")#
  mixcompdist = match.arg(mixcompdist)#
  # if(mixcompdist=="uniform" & pointmass==TRUE){#
  #    stop("point mass not yet implemented for uniform or half-uniform")#
  #  }#
  #  if(mixcompdist=="halfuniform" & pointmass==TRUE){#
  #    stop("point mass not yet implemented for uniform or half-uniform")#
  #  }#
  if(!is.numeric(prior)){#
    prior = match.arg(prior)#
  }  #
  if(length(sebetahat)==1){#
    sebetahat = rep(sebetahat,length(betahat))#
  }#
  if(length(sebetahat) != length(betahat)){#
    stop("Error: sebetahat must have length 1, or same length as betahat")#
  }#
  completeobs = (!is.na(betahat) & !is.na(sebetahat))#
  n=sum(completeobs)#
  if(n==0){#
    if(onlylogLR){#
      return(list(pi=NULL, logLR = 0))#
    }#
    else{#
      stop("Error: all input values are missing")#
    }#
  }  #
  if(!is.null(g)){#
    maxiter = 1 # if g is specified, don't iterate the EM#
    prior = rep(1,ncomp(g)) #prior is not actually used if g specified, but required to make sure EM doesn't produce warning#
    null.comp=1 #null.comp also not used, but required #
  } else {#
    if(is.null(mixsd)){#
      mixsd = autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)#
    }#
    if(pointmass){#
      mixsd = c(0,mixsd)#
    }#
    null.comp = which.min(mixsd) #which component is the "null"#
    k = length(mixsd)#
#DCX add EM for non zero mean    #
#    #
    if(!is.numeric(prior)){#
      if(prior=="nullbiased"){ # set up prior to favour "null"#
        prior = rep(1,k)#
        prior[null.comp] = nullweight #prior 10-1 in favour of null by default#
      }else if(prior=="uniform"){#
        prior = rep(1,k)#
      }#
    }#
    if(length(prior)!=k | !is.numeric(prior)){#
      stop("invalid prior specification")#
    }#
    if(randomstart){#
      pi = rgamma(k,1,1)#
    } else {#
      if(k<n){#
        pi=rep(1,k)/n #default initialization strongly favours null; puts weight 1/n on everything except null#
        pi[null.comp] = (n-k+1)/n #the motivation is data can quickly drive away from null, but tend to drive only slowly toward null.#
      } else {#
        pi=rep(1,k)/k#
      }#
    }#
    pi=normalize(pi)#
    if(!is.element(mixcompdist,c("normal","uniform","halfuniform"))) stop("Error: invalid type of mixcompdist")#
    if(mixcompdist=="normal") g=normalmix(pi,rep(0,k),mixsd)#
    if(mixcompdist=="uniform") g=unimix(pi,-mixsd,mixsd)#
    if(mixcompdist=="halfuniform"){#
      g = unimix(c(pi,pi)/2,c(-mixsd,rep(0,k)),c(rep(0,k),mixsd))#
      prior = rep(prior, 2)#
      pi = rep(pi, 2)#
    }#
  }#
  pi.fit=EMest(betahat[completeobs],lambda1*sebetahat[completeobs]+lambda2,g,prior,null.comp=null.comp,nullcheck=nullcheck,VB=VB,maxiter = maxiter, cxx=cxx, df=df)  #
  if (!onlylogLR){#
      n=length(betahat)#
      if (!multiseqoutput){#
          ZeroProb = rep(0,length=n)#
          NegativeProb = rep(0,length=n)#
      }#
      if (!minimaloutput){#
          PosteriorMean = rep(0,length=n)#
          PosteriorSD = rep(0,length=n)#
      }#
      if(is.null(df)){#
                                        #print("normal likelihood")#
          if (!multiseqoutput){  #
              ZeroProb[completeobs] = colSums(comppostprob(pi.fit$g,betahat[completeobs],sebetahat[completeobs])[comp_sd(pi.fit$g)==0,,drop=FALSE])     #
              NegativeProb[completeobs] = cdf_post(pi.fit$g, 0, betahat[completeobs],sebetahat[completeobs]) - ZeroProb[completeobs]#
          }#
          if (!minimaloutput){#
              PosteriorMean[completeobs] = postmean(pi.fit$g,betahat[completeobs],sebetahat[completeobs])#
              PosteriorSD[completeobs] = postsd(pi.fit$g,betahat[completeobs],sebetahat[completeobs])#
          }#
      }#
      else{#
                                        #print("student-t likelihood")#
          if (!multiseqoutput){#
              ZeroProb[completeobs] = colSums(comppostprob_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)[comp_sd(pi.fit$g)==0,,drop=FALSE])     #
              NegativeProb[completeobs] = cdf_post_t(pi.fit$g, 0, betahat[completeobs],sebetahat[completeobs],df) - ZeroProb[completeobs]#
          }#
          if (!minimaloutput){#
              PosteriorMean[completeobs] = postmean_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
              PosteriorSD[completeobs] = postsd_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
          }#
      }#
                                        #FOR MISSING OBSERVATIONS, USE THE PRIOR INSTEAD OF THE POSTERIOR#
      if (!multiseqoutput){#
          ZeroProb[!completeobs] = sum(mixprop(pi.fit$g)[comp_sd(pi.fit$g)==0])#
          NegativeProb[!completeobs] = mixcdf(pi.fit$g,0)#
          lfsr = compute_lfsr(NegativeProb,ZeroProb)#
      }#
      if (!minimaloutput){#
          PosteriorMean[!completeobs] = mixmean(pi.fit$g)#
          PosteriorSD[!completeobs] = mixsd(pi.fit$g)#
      }#
      if (!minimaloutput & !multiseqoutput){#
          PositiveProb = 1- NegativeProb-ZeroProb#
          lfsra = compute_lfsra(PositiveProb,NegativeProb,ZeroProb) #
          lfdr = ZeroProb#
          qvalue = qval.from.lfdr(lfdr)#
      }#
  }#
  if (!minimaloutput)#
      logLR = tail(pi.fit$loglik,1) - pi.fit$null.loglik#
  if (onlylogLR)#
      return(list(fitted.g=pi.fit$g, logLR = logLR, df=df))#
  else if (minimaloutput)#
      return(list(fitted.g = pi.fit$g, lfsr = lfsr, fit = pi.fit, df=df))#
  else if (multiseqoutput)#
      return(list(fitted.g = pi.fit$g, logLR = logLR, PosteriorMean = PosteriorMean, PosteriorSD = PosteriorSD, call= match.call(), df=df))#
  else{#
      result = list(fitted.g = pi.fit$g, logLR = logLR, PosteriorMean = PosteriorMean, PosteriorSD = PosteriorSD, PositiveProb = PositiveProb, NegativeProb = NegativeProb, ZeroProb = ZeroProb, lfsr = lfsr,lfsra = lfsra, lfdr = lfdr, qvalue = qvalue, fit = pi.fit, lambda1 = lambda1, lambda2 = lambda2, call = match.call(), data = list(betahat = betahat, sebetahat=sebetahat), df=df)#
      class(result) = "ash"#
      return(result)#
  }#
}#
  #if(nsamp>0){#
  #  sample = posterior_sample(post,nsamp)#
  #}#
#
# #' @title Faster version of function ash#
# #'#
# #' @description This function has similar functionality as ash, but only returns some of the outputs.#
# #'#
# #' @param betahat, a p vector of estimates#
# #' @param sebetahat, a p vector of corresponding standard errors#
# #' @param nullcheck: whether to check that any fitted model exceeds the "null" likelihood in which all weight is on the first component#
# #' @param randomstart: bool, indicating whether to initialize EM randomly. If FALSE, then initializes to prior mean (for EM algorithm) or prior (for VBEM)#
# #' @param pointmass: bool, indicating whether to use a point mass at zero as one of components for a mixture distribution#
# #' @param onlylogLR: bool, indicating whether to use this function to get logLR. Skip posterior prob, posterior mean, lfdr...#
# #' @param prior: string, or numeric vector indicating Dirichlet prior on mixture proportions (defaults to "uniform", or 1,1...,1; also can be "nullbiased" 1,1/k-1,...,1/k-1 to put more weight on first component)#
# #' @param mixsd: vector of sds for underlying mixture components#
# #' @param VB: whether to use Variational Bayes to estimate mixture proportions (instead of EM to find MAP estimate)#
# #' @param gridmult: the multiplier by which the default grid values for mixsd differ by one another. (Smaller values produce finer grids)#
# #' @param g: the prior distribution for beta (usually estimated from the data; this is used primarily in simulated data to do computations with the "true" g)#
# #' @param cxx: flag to indicate whether to use the c++ (Rcpp) version#
# #'#
# #' @return a list with elements fitted.g is fitted mixture#
# #' logLR : logP(D|mle(pi)) - logP(D|null)#
# #'#
# #' @export#
# fast.ash = function(betahat,sebetahat, #
#                     nullcheck=TRUE,randomstart=FALSE, #
#                     pointmass = TRUE,    #
#                     prior=c("nullbiased","uniform"), #
#                     mixsd=NULL, VB=FALSE,gridmult=4,#
#                     g=NULL, cxx=TRUE,#
#                     onlylogLR = FALSE,df=NULL){#
#   #
#   if(onlylogLR){#
#     pointmass <- TRUE  #
#   }#
#   #
#   #If method is supplied, use it to set up defaults; provide warning if these default values#
#   #are also specified by user#
#   if(!is.numeric(prior)){#
#     prior = match.arg(prior)#
#   }#
#   #
#   if(length(sebetahat)==1){#
#     sebetahat = rep(sebetahat,length(betahat))#
#   }#
#   if(length(sebetahat) != length(betahat)){#
#     stop("Error: sebetahat must have length 1, or same length as betahat")#
#   }#
#   #
#   completeobs = (!is.na(betahat) & !is.na(sebetahat))#
#   if(sum(completeobs)==0){#
#     if(onlylogLR){#
#       return(list(pi=NULL, logLR = 0))#
#     }else{#
#       stop("Error: all input values are missing")#
#     }#
#   }  #
#   #
#   if(is.null(mixsd)){#
#     mixsd= autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)#
#   }#
#   if(pointmass){#
#     mixsd = c(0,mixsd)#
#   }#
#   #
#   k=length(mixsd)  #
#   null.comp = which.min(mixsd) #which component is the "null"#
#   #
#   if(!is.numeric(prior)){#
#     if(prior=="nullbiased"){ # set up prior to favour "null"#
#       prior = rep(1,k)#
#       prior[null.comp] = 10 #prior 10-1 in favour of null#
#     }else if(prior=="uniform"){#
#       prior = rep(1,k)#
#     }#
#   }#
#   #
#   if(length(prior)!=k | !is.numeric(prior)){#
#     stop("invalid prior specification")#
#   }#
#   #
#   if(missing(g)){#
#     pi = prior^2 #default is to initialize pi at prior (mean)#
#     if(randomstart){pi=rgamma(k,1,1)}#
#     pi=normalize(pi)#
#     g=normalmix(pi,rep(0,k),mixsd)#
#     maxiter = 5000#
#   } else {#
#     maxiter = 1; # if g is specified, don't iterate the EM #
#   }#
#   #
#   pi.fit=EMest(betahat[completeobs],sebetahat[completeobs],g,prior,null.comp=null.comp,nullcheck=nullcheck,VB=VB,maxiter = maxiter, cxx=cxx, df=df)  #
#   #
#   if(onlylogLR){#
#     logLR = tail(pi.fit$loglik,1) - pi.fit$null.loglik#
#     return(list(pi=pi.fit$pi, logLR = logLR))#
#   }else{#
#     #
#     n=length(betahat)#
#     PosteriorMean = rep(0,length=n)#
#     PosteriorSD=rep(0,length=n)#
#     #
#     if(is.null(df)){#
#       PosteriorMean[completeobs] = postmean(pi.fit$g,betahat[completeobs],sebetahat[completeobs])#
#       PosteriorSD[completeobs] =postsd(pi.fit$g,betahat[completeobs],sebetahat[completeobs]) #
#     }#
#     else{#
#       PosteriorMean[completeobs] = postmean_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
#       PosteriorSD[completeobs] =postsd_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
#     }#
#     #FOR MISSING OBSERVATIONS, USE THE PRIOR INSTEAD OF THE POSTERIOR#
#     PosteriorMean[!completeobs] = mixmean(pi.fit$g)#
#     PosteriorSD[!completeobs] =mixsd(pi.fit$g)  #
#     #
#     result = list(fitted.g=pi.fit$g,PosteriorMean = PosteriorMean,PosteriorSD=PosteriorSD,call=match.call(),data=list(betahat = betahat, sebetahat=sebetahat))#
#     return(result)#
#   }#
#   #if(nsamp>0){#
#   #  sample = posterior_sample(post,nsamp)#
#   #}#
# }#
compute_lfsr = function(NegativeProb,ZeroProb){#
  ifelse(NegativeProb> 0.5*(1-ZeroProb),1-NegativeProb,NegativeProb+ZeroProb)#
}#
#
compute_lfsra = function(PositiveProb, NegativeProb,ZeroProb){#
  ifelse(PositiveProb<NegativeProb,2*PositiveProb+ZeroProb,2*NegativeProb+ZeroProb)  #
}  #
#' @title Estimate posterior distribution on mixture proportions of a mixture model by a Variational Bayes EM algorithm#
#'#
#' @description Given the individual component likelihoods for a mixture model, estimates the posterior on #
#' the mixture proportions by an VBEM algorithm. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates posterior on mixture proportions \eqn{\pi} by Variational Bayes, #
#' with a Dirichlet prior on \eqn{\pi}. #
#' Algorithm adapted from Bishop (2009), Pattern Recognition and Machine Learning, Chapter 10.#
#' #
#' @param matrix_lik: a n by k matrix with (j,k)th element equal to \eqn{f_k(x_j)}.#
#' @param prior: a k vector of the parameters of the Dirichlet prior on \eqn{\pi}. Recommended to be rep(1,k)#
#' @param post.init: the initial value of the posterior parameters. If not specified defaults to the prior parameters.#
#' @param tol: the tolerance for convergence of log-likelihood bound.#
#' @param maxiter: the maximum number of iterations performed#
#' #
#' @return A list, whose components include point estimates (pihat), #
#' the parameters of the fitted posterior on \eqn{\pi} (pipost),#
#' the bound on the log likelihood for each iteration (B)#
#' and a flag to indicate convergence (converged).#
#'  #
#' @export#
#' #
mixVBEM = function(matrix_lik, prior, pi.init = NULL,tol=1e-7, maxiter=5000){#
  k=ncol(matrix_lik)#
  if(is.null(pi.init)){#
    pi.init = rep(1,k)# Use as starting point for pi#
  } #
  res = squarem(par=pi.init,fixptfn=VBfixpoint, objfn=VBnegpenloglik,matrix_lik=matrix_lik, prior=prior, control=list(maxiter=maxiter,tol=tol))#
  return(list(pihat = res$par/sum(res$par), B=res$value.objfn, niter = res$iter, converged=res$convergence,post=res$par))#
}#
VBfixpoint = function(pipost, matrix_lik, prior){  #
  n=nrow(matrix_lik)#
  k=ncol(matrix_lik)#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost*matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  pipostnew = colSums(classprob) + prior#
  return(pipostnew)#
}#
#
VBnegpenloglik=function(pipost,matrix_lik,prior){#
  return(-VBpenloglik(pipost,matrix_lik,prior))#
}#
#
VBpenloglik = function(pipost, matrix_lik, prior){#
  n=nrow(matrix_lik)#
  k=ncol(matrix_lik)#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost*matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  B= sum(classprob*log(avgpipost*matrix_lik),na.rm=TRUE) - diriKL(prior,pipost) - sum(classprob*log(classprob)) #
  return(B)#
}#
#' @title Estimate mixture proportions of a mixture model by EM algorithm#
#'#
#' @description Given the individual component likelihoods for a mixture model, estimates the mixture proportions by an EM algorithm.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates mixture proportions \eqn{\pi} by maximum likelihood, or by maximum a posteriori (MAP) estimation for a Dirichlet prior on $\pi$ #
#' (if a prior is specified).  Uses the SQUAREM package to accelerate convergence of EM. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' #
#' @param matrix_lik, a n by k matrix with (j,k)th element equal to \eqn{f_k(x_j)}.#
#' @param prior, a k vector of the parameters of the Dirichlet prior on \eqn{\pi}. Recommended to be rep(1,k)#
#' @param pi.init, the initial value of \eqn{\pi} to use. If not specified defaults to (1/k,...,1/k).#
#' @param tol, the tolerance for convergence of log-likelihood.#
#' @param maxiter the maximum number of iterations performed#
#' #
#' @return A list, including the estimates (pihat), the log likelihood for each interation (B)#
#' and a flag to indicate convergence#
#'  #
#' @export#
#' #
#' #
mixEM = function(matrix_lik, prior, pi.init = NULL,tol=1e-7, maxiter=5000){#
  if(is.null(pi.init)){#
    pi.init = rep(1/k,k)# Use as starting point for pi#
  } #
  res = squarem(par=pi.init,fixptfn=fixpoint, objfn=negpenloglik,matrix_lik=matrix_lik, prior=prior, control=list(maxiter=maxiter,tol=tol))#
  return(list(pihat = normalize(pmax(0,res$par)), B=res$value.objfn, #
              niter = res$iter, converged=res$convergence))#
}#
#
# helper functions used by mixEM#
normalize = function(x){return(x/sum(x))}#
#
fixpoint = function(pi, matrix_lik, prior){  #
  pi = normalize(pmax(0,pi)) #avoid occasional problems with negative pis due to rounding#
  m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
  m.rowsum = rowSums(m)#
  classprob = m/m.rowsum #an n by k matrix#
  pinew = normalize(colSums(classprob) + prior - 1)#
  return(pinew)#
}#
#
negpenloglik = function(pi,matrix_lik,prior){return(-penloglik(pi,matrix_lik,prior))}#
#
penloglik = function(pi, matrix_lik, prior){#
  pi = normalize(pmax(0,pi))#
  m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
  m.rowsum = rowSums(m)#
  loglik = sum(log(m.rowsum))#
  subset = (prior != 1.0)#
  priordens = sum((prior-1)[subset]*log(pi[subset]))#
  return(loglik+priordens)#
}#
#
#The kth element of this vector is the derivative #
#of the loglik for $\pi=(\pi_0,...,1-\pi_0,...)$ with respect to $\pi_0$ at $\pi_0=1$.#
gradient = function(matrix_lik){#
  n = nrow(matrix_lik)#
  grad = n - colSums(matrix_lik/matrix_lik[,1]) #
  return(grad)#
}#
#
# mixEM = function(matrix_lik, prior, pi.init = NULL,tol=0.0001, maxiter=5000){#
#   n=nrow(matrix_lik)#
#   k=ncol(matrix_lik)#
#   B = rep(0,maxiter)#
#   pi = pi.init#
#   if(is.null(pi.init)){#
#     pi = rep(1/k,k)# Use as starting point for pi#
#   } #
#   pi = ifelse(pi<1e-5,1e-5,pi) #set any estimates that are too small to be just very small#
#   pi = normalize(pi)#
#   #
#   loglik = rep(0,maxiter)#
#   priordens= rep(0,maxiter)#
#   m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
#   m.rowsum = rowSums(m)#
#   loglik[1] = sum(log(m.rowsum))#
#   priordens[1] = sum((prior-1)*log(pi)) #
#   classprob = m/m.rowsum #an n by k matrix#
#   i=1#
#   if(maxiter >= 2){#
#     for(i in 2:maxiter){  #
#       pi = colSums(classprob) + prior-1#
#       pi = ifelse(pi<1e-5,1e-5,pi) #set any estimates that are less than zero, which can happen with prior<1, to 0#
#       pi = normalize(pi)#
#         #
#       #Now re-estimate pi#
#       m  = t(pi * t(matrix_lik)) #
#       m.rowsum = rowSums(m)#
#       loglik[i] = sum(log(m.rowsum))#
#       priordens[i] = sum((prior-1)*log(pi)) #
#       classprob = m/m.rowsum#
#     #
#     #
#       if(abs(loglik[i]+priordens[i]-loglik[i-1]-priordens[i-1])<tol) break;#
#     }#
#   }#
#   converged=(abs(loglik[i]+priordens[i]-loglik[i-1]-priordens[i-1])<tol)#
#   if(!converged){#
#       warning("EM algorithm in function mixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
#   }#
#   return(list(pihat = pi, B=loglik[1:i], #
#               niter = i, converged=converged))#
# }#
#estimate mixture proportions of sigmaa by EM algorithm#
#prior gives the parameter of a Dirichlet prior on pi#
#(prior is used to encourage results towards smallest value of sigma when#
#likelihood is flat)#
#nullcheck indicates whether to check whether the loglike exceeds the null#
#(may not want to use if prior is used)#
#VB provides an approach to estimate the approximate posterior distribution#
#of mixture proportions of sigmaa by variational Bayes method#
#(use Dirichlet prior and approximate Dirichlet posterior)#
#if cxx TRUE use cpp version of R function mixEM#
EMest = function(betahat,sebetahat,g,prior,null.comp=1,nullcheck=TRUE,VB=FALSE, maxiter=5000, cxx=TRUE, df=NULL){ #
  pi.init = g$pi#
  k=ncomp(g)#
  n = length(betahat)#
  tol = min(0.1/n,1e-5) # set convergence criteria to be more stringent for larger samples#
  if(is.null(df)){#
    matrix_lik = t(compdens_conv(g,betahat,sebetahat))#
  }#
  else{#
    matrix_lik = t(compdens_conv_t(g,betahat,sebetahat,df))#
  }#
  #checks whether the gradient at pi0=1 is positive (suggesting that this is a fixed point)#
  #if(nullcheck){#
  #  if(all(gradient(matrix_lik)>=0)){#
  #    pi.init=rep(0,k)#
  #    pi.init[null.comp]=1 #this will make pi.init=(1,0,0...,0) which is a fixed point of the EM#
  #  }#
  #}#
  if(VB==TRUE){#
    EMfit=mixVBEM(matrix_lik,prior,maxiter=maxiter)}#
  else{#
    if (cxx==TRUE){#
      EMfit = cxxMixEM(matrix_lik,prior,pi.init,1e-5, maxiter) #currently use different convergence criteria for cxx version #
      if(!EMfit$converged){#
        warning("EM algorithm in function cxxMixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
      }#
    }#
    else{#
      EMfit = mixEM(matrix_lik,prior,pi.init,tol, maxiter)#
      if(!EMfit$converged & !(maxiter==1)){#
        warning("EM algorithm in function mixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
      }#
    }#
  }#
  pi = EMfit$pihat     #
  penloglik = EMfit$B #
  converged = EMfit$converged#
  niter = EMfit$niter#
  loglik.final =  penloglik(pi,matrix_lik,1) #compute penloglik without penalty#
  null.loglik = sum(log(matrix_lik[,null.comp]))  #
  if(nullcheck==TRUE & VB==FALSE){ #null check doesn't work with VB yet#
    pinull = rep(0,k)#
    pinull[null.comp]=1#
    null.penloglik = penloglik(pinull,matrix_lik,prior)#
    final.penloglik = penloglik(pi,matrix_lik,prior)#
    if(null.penloglik > final.penloglik){ #check whether exceeded "null" likelihood where everything is null#
      pi=pinull#
      loglik.final=penloglik(pi,matrix_lik,1)#
    }#
  }#
  g$pi=pi#
  return(list(loglik=loglik.final,null.loglik=null.loglik,#
              matrix_lik=matrix_lik,converged=converged,g=g))#
}#
#' @title Compute Posterior#
#'#
#' @description Return the posterior on beta given a prior (g) that is a mixture of normals (class normalmix) #
#' and observation betahat \sim N(beta,sebetahat)#
#'#
#' @details This can be used to obt#
#'#
#' @param g: a normalmix with components indicating the prior; works only if g has means 0#
#' @param betahat (n vector of observations) #
#' @param sebetahat (n vector of standard errors/deviations of observations)#
#' #
#' @return A list, (pi1,mu1,sigma1) whose components are each k by n matrices#
#' where k is number of mixture components in g, n is number of observations in betahat#
#' #
#' @export#
#' #
#' #
posterior_dist = function(g,betahat,sebetahat){#
  if(class(g)!="normalmix"){#
    stop("Error: posterior_dist implemented only for g of class normalmix")#
  }#
  pi0 = g$pi#
  mu0 = g$mean#
  sigma0 = g$sd  #
  k= length(pi0)#
  n= length(betahat)#
  if(!all.equal(g$mean,rep(0,k))) stop("Error: posterior_dist currently only implemented for zero-centered priors")#
  pi1 = pi0 * t(matrix_dens(betahat,sebetahat,sigma0))#
  pi1 = apply(pi1, 2, normalize) #pi1 is now an k by n matrix#
  #make k by n matrix versions of sigma0^2 and sebetahat^2#
  # and mu0 and betahat#
  s0m2 = matrix(sigma0^2,nrow=k,ncol=n,byrow=FALSE)#
  sebm2 = matrix(sebetahat^2,nrow=k,ncol=n, byrow=TRUE)#
  mu0m = matrix(mu0,nrow=k,ncol=n,byrow=FALSE)#
  bhatm = matrix(betahat,nrow=k,ncol=n,byrow=TRUE)#
  sigma1 = (s0m2*sebm2/(s0m2 + sebm2))^(0.5)  #
  w = sebm2/(s0m2 + sebm2)#
  mu1 = w*mu0m + (1-w)*bhatm#
  #WHERE DATA ARE MISSING, SET POSTERIOR = PRIOR#
  ismiss = (is.na(betahat) | is.na(sebetahat)) #
  pi1[,ismiss] = pi0#
  mu1[,ismiss] = mu0#
  sigma1[,ismiss] = sigma0#
  return(list(pi=pi1,mu=mu1,sigma=sigma1))#
}#
#
#return matrix of densities of observations (betahat) #
# assuming betahat_j \sim N(0, sebetahat_j^2 + sigmaavec_k^2)#
#normalized by maximum of each column#
#INPUT#
#betahat is n vector, #
#sebetahat is n vector, #
#sigmaavec is k vector#
#return is n by k matrix of the normal likelihoods, #
# with (j,k)th element the density of N(betahat_j; mean=0, var = sebetahat_j^2 + sigmaavec_k^2)#
#normalized to have maximum 1 in each column#
matrix_dens = function(betahat, sebetahat, sigmaavec){#
  k = length(sigmaavec)#
  n = length(betahat)#
  ldens = dnorm(betahat,0,sqrt(outer(sebetahat^2,sigmaavec^2,FUN="+")),log=TRUE)#
  maxldens = apply(ldens, 1, max)#
  ldens = ldens - maxldens#
  return(exp(ldens))#
}#
#
#return the "effective" estimate#
#that is the effect size betanew whose z score betanew/se#
#would give the same p value as betahat/se compared to a t with df#
effective.effect=function(betahat,se,df){#
  p = pt(betahat/se,df)#
  qnorm(p,sd=se)#
}#
#' @title Function to compute q values from local false discovery rates#
#'#
#' @description Computes q values from a vector of local fdr estimates#
#'#
#' @details The q value for a given lfdr is an estimate of the (tail) False Discovery Rate #
#' for all findings with a smaller lfdr, and is found by the average of the lfdr for#
#' all more significant findings. See Storey (2003), Annals of Statistics, for definition of q value.  #
#' #
#' #
#' @param lfdr, a vector of local fdr estimates#
#'#
#' @return vector of q values#
#' #
#' @export#
qval.from.lfdr = function(lfdr){#
  o = order(lfdr)#
  qvalue=rep(NA,length(lfdr))#
  qvalue[o] = (cumsum(sort(lfdr))/(1:sum(!is.na(lfdr))))#
  return(qvalue)#
}#
#
# try to select a default range for the sigmaa values#
# that should be used, based on the values of betahat and sebetahat#
# mult is the multiplier by which the sds differ across the grid#
autoselect.mixsd = function(betahat,sebetahat,mult){#
  sigmaamin = min(sebetahat)/10 #so that the minimum is small compared with measurement precision#
  if(all(betahat^2<sebetahat^2)){#
    sigmaamax = 8*sigmaamin #to deal with the occassional odd case where this could happen; 8 is arbitrary#
  } else {#
    sigmaamax = 2*sqrt(max(betahat^2-sebetahat^2)) #this computes a rough largest value you'd want to use, based on idea that sigmaamax^2 + sebetahat^2 should be at least betahat^2   #
  }#
  if(mult==0){#
    return(c(0,sigmaamax/2))#
  }else{#
    npoint = ceiling(log2(sigmaamax/sigmaamin)/log2(mult))#
    return(mult^((-npoint):0) * sigmaamax)#
  }#
}#
#' @title Summary method for ash object#
#'#
#' @description Print summary of fitted ash object#
#'#
#' @details See readme for more details#
#' #
#' @export#
#' #
summary.ash=function(a){#
  print(a$fitted.g)#
  print(tail(a$fit$loglik,1),digits=10)#
  print(a$fit$converged)#
}#
#
#' @title Print method for ash object#
#'#
#' @description Print the fitted distribution of beta values in the EB hierarchical model#
#'#
#' @details None#
#' #
#' @export#
#' #
print.ash =function(a){#
  print(a$fitted.g)#
}#
#
#' @title Plot method for ash object#
#'#
#' @description Plot the density of the underlying fitted distribution#
#'#
#' @details None#
#' #
#' @export#
#' #
plot.ash = function(a,xmin,xmax,...){#
  x = seq(xmin,xmax,length=1000)#
  y = density(a,x)#
  plot(y,type="l",...)#
}#
#
#compute the predictive density of an observation#
#given the fitted ash object a and the vector se of standard errors#
#not implemented yet#
predictive=function(a,se){#
}#
#' @title Get fitted loglikelihood for ash object#
#'#
#' @description Return the log-likelihood of the data under the fitted distribution#
#'#
#' @param a the fitted ash object#
#'#
#' @details None#
#' #
#' @export#
#' #
#'#
get_loglik = function(a){#
  return(tail(a$fit$loglik,1))#
}#
#
#' @title Get pi0 estimate for ash object#
#'#
#' @description Return estimate of the null proportion, pi0#
#'#
#' @param a the fitted ash object#
#'#
#' @details Extracts the estimate of the null proportion, pi0, from the object a#
#' #
#' @export#
#' #
get_pi0 = function(a){#
  null.comp = comp_sd(a$fitted.g)==0#
  return(sum(a$fitted.g$pi[null.comp]))#
}#
#
#' @title Compute loglikelihood for data from ash fit#
#'#
#' @description Return the log-likelihood of the data betahat, with standard errors betahatsd, #
#' under the fitted distribution in the ash object. #
#' #
#'#
#' @param a the fitted ash object#
#' @param betahat the data#
#' @param betahatsd the observed standard errors#
#' @param zscores indicates whether ash object was originally fit to z scores #
#' @details None#
#' #
#' @export#
#' #
#'#
loglik.ash = function(a,betahat,betahatsd,zscores=FALSE){#
  g=a$fitted.g#
  FUN="+"#
  if(zscores==TRUE){#
    g$sd = sqrt(g$sd^2+1) #
    FUN="*"#
  }#
  return(loglik_conv(g,betahat, betahatsd,FUN))#
}#
#
#' @title Density method for ash object#
#'#
#' @description Return the density of the underlying fitted distribution#
#'#
#' @param a the fitted ash object#
#' @param x the vector of locations at which density is to be computed#
#'#
#' @details None#
#' #
#' @export#
#' #
#'#
density.ash=function(a,x){list(x=x,y=dens(a$fitted.g,x))}#
#
#' @title cdf method for ash object#
#'#
#' @description Computed the cdf of the underlying fitted distribution#
#'#
#' @param a the fitted ash object#
#' @param x the vector of locations at which cdf is to be computed#
#' @param lower.tail (default=TRUE) whether to compute the lower or upper tail#
#'#
#' @details None#
#' #
#' @export#
#' #
#'#
cdf.ash=function(a,x,lower.tail=TRUE){#
  return(list(x=x,y=mixcdf(a$fitted.g,x,lower.tail)))#
}#
#return the KL-divergence between 2 dirichlet distributions#
#p,q are the vectors of dirichlet parameters of same lengths#
diriKL = function(p,q){#
  p.sum = sum(p)#
  q.sum = sum(q)#
  k = length(q)#
  KL = lgamma(q.sum)-lgamma(p.sum)+sum((q-p)*(digamma(q)-digamma(rep(q.sum,k))))+sum(lgamma(p)-lgamma(q))#
  return(KL)#
}#
#
#helper function for VBEM#
VB.update = function(matrix_lik, pipost){#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost * matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  B = sum(classprob*log(avgpipost*matrix_lik),na.rm=TRUE) - diriKL(prior,pipost) #negative free energy#
  return(list(classprob=classprob,B=B))#
}#
EMnonzeromean = function(x, sebetahat, mixsd, reltol=1e-8){#
}
help(ash)
??ash
nonzeromean
nonzeromean=FALSE
nonzeromean
if(nonzeromean){print('yes')}else{print(wrong)}
if(nonzeromean){print('yes')}else{print(0)}
nonzeromeanEM(betahat, sebetahat, mixsd, maxiter=maxiter).nonzeromean
nonzeromean.fit=nonzeromeanEM(betahat, sebetahat, mixsd, maxiter=maxiter)
nonzeromeanEM = function(betahat, sebetahat, mixsd, pi.init=NULL,tol=1e-7,maxiter=5000){#
  if(is.null(pi.init)){#
    pi.init = rep(1/length(mixsd),length(mixsd))# Use as starting point for pi#
  }#
  mupi=c(mean(betahat),pi.init)#
  res=squarem(par=mupi,fixptfn=nonzeromeanEMfixpoint,objfn=nonzeromeanEMobj,betahat=betahat,sebetahat=sebetahat,mixsd=mixsd,control=list(maxiter=maxiter,tol=tol))#
  return(list(nonzeromean=res$par[1],pi=res$par[-1],NQ=-res$value.objfn,niter = res$iter, converged=res$convergence,post=res$par))#
}#
#
nonzeromeanEMfixpoint = function(mupi,betahat,sebetahat,mixsd){#
	#omegamatrix=matrix(NA,nrow=length(betahat),ncol=length(mixsd))#
	mu=mupi[1]#
	pimean=mupi[-1]#
	sdmat = sqrt(outer(sebetahat ^2,mixsd^2,"+")) #
	xmat=matrix(rep(betahat,length(mixsd)),ncol=length(mixsd))#
	omegamatrix=t(t(dnorm(xmat,mean=mu,sd=sdmat))*pimean)#
	omegamatrix=omegamatrix /rowSums(omegamatrix)#
	pinew=normalize(colSums(omegamatrix))#
	munew=sum(omegamatrix*xmat/(sdmat^2))/sum(omegamatrix/(sdmat^2))#
	mupi=c(munew,pinew)#
	return(mupi)#
}#
#
nonzeromeanEMobj = function(mupi,betahat,sebetahat,mixsd){#
	mu=mupi[1]#
	pimean=mupi[-1]#
	sdmat = sqrt(outer(sebetahat ^2,mixsd^2,"+")) #
	xmat=matrix(rep(betahat,length(mixsd)),ncol=length(mixsd))#
	omegamatrix=t(t(dnorm(xmat,mean=mu,sd=sdmat))*pimean)#
	omegamatrix=omegamatrix /rowSums(omegamatrix)#
	NegativeQ=-sum(omegamatrix*dnorm(xmat,mean=mu,sd=sdmat,log=TRUE))#
	return(NegativeQ)#
}
nonzeromean.fit=nonzeromeanEM(betahat, sebetahat, mixsd, maxiter=maxiter)
library(SQUAREM)
nonzeromean.fit=nonzeromeanEM(betahat, sebetahat, mixsd, maxiter=maxiter)
maxiter=5000
nonzeromean.fit=nonzeromeanEM(betahat, sebetahat, mixsd, maxiter=maxiter)
mixsd
mixsd = autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)
completeobs = (!is.na(betahat) & !is.na(sebetahat))
mixsd = autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)
gridmult=sqrt(2)
mixsd = autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)
nonzeromean.fit=nonzeromeanEM(betahat, sebetahat, mixsd, maxiter=maxiter)
test.ash=ash(betahat,sebetahat,method="shrink")
' @useDynLib ashr#
#todo#
##
#' @title Main Adaptive SHrinkage function#
#'#
#' @description Takes vectors of estimates (betahat) and their standard errors (sebetahat), and applies#
#' shrinkage to them, using Empirical Bayes methods, to compute shrunk estimates for beta.#
#'#
#' @details See readme for more details#
#' #
#' @param betahat, a p vector of estimates #
#' @param sebetahat, a p vector of corresponding standard errors#
#' @param method: specifies how ash is to be run. Can be "shrinkage" (if main aim is shrinkage) or "fdr" (if main aim is to assess fdr or fsr)#
#' This is simply a convenient way to specify certain combinations of parameters: "shrinkage" sets pointmass=FALSE and prior="uniform";#
#' "fdr" sets pointmass=TRUE and prior="nullbiased".#
#' @param mixcompdist: distribution of components in mixture ("normal", "uniform" or "halfuniform")#
#'#
#' @param lambda1: multiplicative "inflation factor" for standard errors (like Genomic Control)#
#' @param lambda2: additive "inflation factor" for standard errors (like Genomic Control)#
#' @param nullcheck: whether to check that any fitted model exceeds the "null" likelihood#
#' in which all weight is on the first component#
#' @param df: appropriate degrees of freedom for (t) distribution of betahat/sebetahat#
#' @param randomstart: bool, indicating whether to initialize EM randomly. If FALSE, then initializes to prior mean (for EM algorithm) or prior (for VBEM)#
#' @param nullweight: scalar, the weight put on the prior of null under "fdr" method#
#' @param nonzeromean: bool, indicating whether to use a nonzero mean unimodal mixture(defaults to "FALSE")#
#' @param pointmass: bool, indicating whether to use a point mass at zero as one of components for a mixture distribution#
#' @param onlylogLR: bool, indicating whether to use this function to get logLR. Skip posterior prob, posterior mean, lfdr...#
#' @param prior: string, or numeric vector indicating Dirichlet prior on mixture proportions (defaults to "uniform", or 1,1...,1; also can be "nullbiased" 1,1/k-1,...,1/k-1 to put more weight on first component)#
#' @param mixsd: vector of sds for underlying mixture components #
#' @param VB: whether to use Variational Bayes to estimate mixture proportions (instead of EM to find MAP estimate)#
#' @param gridmult: the multiplier by which the default grid values for mixsd differ by one another. (Smaller values produce finer grids)#
#' @param minimal_output: if TRUE, just outputs the fitted g and the lfsr (useful for very big data sets where memory is an issue) #
#' @param g: the prior distribution for beta (usually estimated from the data; this is used primarily in simulated data to do computations with the "true" g)#
#' @param maxiter: maximum number of iterations of the EM algorithm#
#' @param cxx: flag to indicate whether to use the c++ (Rcpp) version#
#' #
#'#
#' @return a list with elements fitted.g is fitted mixture#
#' logLR : logP(D|mle(pi)) - logP(D|null)#
#' #
#' @export#
#' #
#' @examples #
#' beta = c(rep(0,100),rnorm(100))#
#' sebetahat = abs(rnorm(200,0,1))#
#' betahat = rnorm(200,beta,sebetahat)#
#' beta.ash = ash(betahat, sebetahat)#
#' summary(beta.ash)#
#' plot(betahat,beta.ash$PosteriorMean,xlim=c(-4,4),ylim=c(-4,4))#
#' #
#' #
#Things to do:#
# check sampling routine#
# check number of iterations#
ash = function(betahat,sebetahat,method = c("shrink","fdr"), #
               mixcompdist = c("normal","uniform","halfuniform"),#
               lambda1=1,lambda2=0,nullcheck=TRUE,df=NULL,randomstart=FALSE,#
               nullweight=10,nonzeromean=FALSE, #
               pointmass = FALSE, #
               onlylogLR = FALSE, #
               prior=c("uniform","nullbiased"), #
               mixsd=NULL, VB=FALSE,gridmult=sqrt(2),#
               minimaloutput=FALSE,#
               multiseqoutput=FALSE,#
               g=NULL,#
               maxiter = 5000,#
               cxx=FALSE){#
  #method provides a convenient interface to set a particular combinations of parameters for prior an#
  #If method is supplied, use it to set up specific values for these parameters; provide warning if values#
  #are also specified by user#
  #If method is not supplied use the user-supplied values (or defaults if user does not specify them)#
  if(!missing(method)){#
    method = match.arg(method) #
    if(method=="shrink"){#
      if(missing(prior)){#
        prior = "uniform"#
      } else {#
        warning("Specification of prior overrides default for method shrink")#
      }#
      if(missing(pointmass)){#
        pointmass=FALSE#
      } else {#
        warning("Specification of pointmass overrides default for method shrink")#
      }#
    }#
    if(method=="fdr"){#
      if(missing(prior)){#
        prior = "nullbiased"#
      } else {#
        warning("Specification of prior overrides default for method fdr")#
      }#
      if(missing(pointmass)){#
        pointmass=TRUE#
      } else {#
        warning("Specification of pointmass overrides default for method fdr")#
      }#
    }  #
  }#
  if(gridmult<=1&multiseqoutput!=TRUE)#
    stop("gridmult must be > 1")#
  mixcompdist = match.arg(mixcompdist)#
  # if(mixcompdist=="uniform" & pointmass==TRUE){#
  #    stop("point mass not yet implemented for uniform or half-uniform")#
  #  }#
  #  if(mixcompdist=="halfuniform" & pointmass==TRUE){#
  #    stop("point mass not yet implemented for uniform or half-uniform")#
  #  }#
  if(!is.numeric(prior)){#
    prior = match.arg(prior)#
  }  #
  if(length(sebetahat)==1){#
    sebetahat = rep(sebetahat,length(betahat))#
  }#
  if(length(sebetahat) != length(betahat)){#
    stop("Error: sebetahat must have length 1, or same length as betahat")#
  }#
  completeobs = (!is.na(betahat) & !is.na(sebetahat))#
  n=sum(completeobs)#
  if(n==0){#
    if(onlylogLR){#
      return(list(pi=NULL, logLR = 0))#
    }#
    else{#
      stop("Error: all input values are missing")#
    }#
  }  #
  if(!is.null(g)){#
    maxiter = 1 # if g is specified, don't iterate the EM#
    prior = rep(1,ncomp(g)) #prior is not actually used if g specified, but required to make sure EM doesn't produce warning#
    null.comp=1 #null.comp also not used, but required #
  } else {#
    if(is.null(mixsd)){#
      mixsd = autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)#
    }#
    if(pointmass){#
      mixsd = c(0,mixsd)#
    }#
    null.comp = which.min(mixsd) #which component is the "null"#
    k = length(mixsd)#
#DCX add EM for non zero mean    #
#
	if(nonzeromean){#
		nonzeromean.fit=nonzeromeanEM(betahat, sebetahat, mixsd, maxiter=maxiter)#
		ashmean=nonzeromeanEM(betahat, sebetahat, mixsd, maxiter=maxiter).#
	}#
	else{#
		ashmean=list(nonzeromean=res$par[1],pi=res$par[-1],NQ=-res$value.objfn,niter = res$iter, converged=res$convergence,post=res$par))#
	}#
    if(!is.numeric(prior)){#
      if(prior=="nullbiased"){ # set up prior to favour "null"#
        prior = rep(1,k)#
        prior[null.comp] = nullweight #prior 10-1 in favour of null by default#
      }else if(prior=="uniform"){#
        prior = rep(1,k)#
      }#
    }#
    if(length(prior)!=k | !is.numeric(prior)){#
      stop("invalid prior specification")#
    }#
    if(randomstart){#
      pi = rgamma(k,1,1)#
    } else {#
      if(k<n){#
        pi=rep(1,k)/n #default initialization strongly favours null; puts weight 1/n on everything except null#
        pi[null.comp] = (n-k+1)/n #the motivation is data can quickly drive away from null, but tend to drive only slowly toward null.#
      } else {#
        pi=rep(1,k)/k#
      }#
    }#
    pi=normalize(pi)#
    if(!is.element(mixcompdist,c("normal","uniform","halfuniform"))) stop("Error: invalid type of mixcompdist")#
    if(mixcompdist=="normal") g=normalmix(pi,rep(0,k),mixsd)#
    if(mixcompdist=="uniform") g=unimix(pi,-mixsd,mixsd)#
    if(mixcompdist=="halfuniform"){#
      g = unimix(c(pi,pi)/2,c(-mixsd,rep(0,k)),c(rep(0,k),mixsd))#
      prior = rep(prior, 2)#
      pi = rep(pi, 2)#
    }#
  }#
  pi.fit=EMest(betahat[completeobs],lambda1*sebetahat[completeobs]+lambda2,g,prior,null.comp=null.comp,nullcheck=nullcheck,VB=VB,maxiter = maxiter, cxx=cxx, df=df)  #
  if (!onlylogLR){#
      n=length(betahat)#
      if (!multiseqoutput){#
          ZeroProb = rep(0,length=n)#
          NegativeProb = rep(0,length=n)#
      }#
      if (!minimaloutput){#
          PosteriorMean = rep(0,length=n)#
          PosteriorSD = rep(0,length=n)#
      }#
      if(is.null(df)){#
                                        #print("normal likelihood")#
          if (!multiseqoutput){  #
              ZeroProb[completeobs] = colSums(comppostprob(pi.fit$g,betahat[completeobs],sebetahat[completeobs])[comp_sd(pi.fit$g)==0,,drop=FALSE])     #
              NegativeProb[completeobs] = cdf_post(pi.fit$g, 0, betahat[completeobs],sebetahat[completeobs]) - ZeroProb[completeobs]#
          }#
          if (!minimaloutput){#
              PosteriorMean[completeobs] = postmean(pi.fit$g,betahat[completeobs],sebetahat[completeobs])#
              PosteriorSD[completeobs] = postsd(pi.fit$g,betahat[completeobs],sebetahat[completeobs])#
          }#
      }#
      else{#
                                        #print("student-t likelihood")#
          if (!multiseqoutput){#
              ZeroProb[completeobs] = colSums(comppostprob_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)[comp_sd(pi.fit$g)==0,,drop=FALSE])     #
              NegativeProb[completeobs] = cdf_post_t(pi.fit$g, 0, betahat[completeobs],sebetahat[completeobs],df) - ZeroProb[completeobs]#
          }#
          if (!minimaloutput){#
              PosteriorMean[completeobs] = postmean_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
              PosteriorSD[completeobs] = postsd_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
          }#
      }#
                                        #FOR MISSING OBSERVATIONS, USE THE PRIOR INSTEAD OF THE POSTERIOR#
      if (!multiseqoutput){#
          ZeroProb[!completeobs] = sum(mixprop(pi.fit$g)[comp_sd(pi.fit$g)==0])#
          NegativeProb[!completeobs] = mixcdf(pi.fit$g,0)#
          lfsr = compute_lfsr(NegativeProb,ZeroProb)#
      }#
      if (!minimaloutput){#
          PosteriorMean[!completeobs] = mixmean(pi.fit$g)#
          PosteriorSD[!completeobs] = mixsd(pi.fit$g)#
      }#
      if (!minimaloutput & !multiseqoutput){#
          PositiveProb = 1- NegativeProb-ZeroProb#
          lfsra = compute_lfsra(PositiveProb,NegativeProb,ZeroProb) #
          lfdr = ZeroProb#
          qvalue = qval.from.lfdr(lfdr)#
      }#
  }#
  if (!minimaloutput)#
      logLR = tail(pi.fit$loglik,1) - pi.fit$null.loglik#
  if (onlylogLR)#
      return(list(fitted.g=pi.fit$g, logLR = logLR, df=df))#
  else if (minimaloutput)#
      return(list(fitted.g = pi.fit$g, lfsr = lfsr, fit = pi.fit, df=df))#
  else if (multiseqoutput)#
      return(list(fitted.g = pi.fit$g, logLR = logLR, PosteriorMean = PosteriorMean, PosteriorSD = PosteriorSD, call= match.call(), df=df))#
  else{#
      result = list(fitted.g = pi.fit$g, logLR = logLR, PosteriorMean = PosteriorMean, PosteriorSD = PosteriorSD, PositiveProb = PositiveProb, NegativeProb = NegativeProb, ZeroProb = ZeroProb, lfsr = lfsr,lfsra = lfsra, lfdr = lfdr, qvalue = qvalue, fit = pi.fit, lambda1 = lambda1, lambda2 = lambda2, call = match.call(), data = list(betahat = betahat, sebetahat=sebetahat), df=df)#
      class(result) = "ash"#
      return(result)#
  }#
}#
  #if(nsamp>0){#
  #  sample = posterior_sample(post,nsamp)#
  #}#
#
# #' @title Faster version of function ash#
# #'#
# #' @description This function has similar functionality as ash, but only returns some of the outputs.#
# #'#
# #' @param betahat, a p vector of estimates#
# #' @param sebetahat, a p vector of corresponding standard errors#
# #' @param nullcheck: whether to check that any fitted model exceeds the "null" likelihood in which all weight is on the first component#
# #' @param randomstart: bool, indicating whether to initialize EM randomly. If FALSE, then initializes to prior mean (for EM algorithm) or prior (for VBEM)#
# #' @param pointmass: bool, indicating whether to use a point mass at zero as one of components for a mixture distribution#
# #' @param onlylogLR: bool, indicating whether to use this function to get logLR. Skip posterior prob, posterior mean, lfdr...#
# #' @param prior: string, or numeric vector indicating Dirichlet prior on mixture proportions (defaults to "uniform", or 1,1...,1; also can be "nullbiased" 1,1/k-1,...,1/k-1 to put more weight on first component)#
# #' @param mixsd: vector of sds for underlying mixture components#
# #' @param VB: whether to use Variational Bayes to estimate mixture proportions (instead of EM to find MAP estimate)#
# #' @param gridmult: the multiplier by which the default grid values for mixsd differ by one another. (Smaller values produce finer grids)#
# #' @param g: the prior distribution for beta (usually estimated from the data; this is used primarily in simulated data to do computations with the "true" g)#
# #' @param cxx: flag to indicate whether to use the c++ (Rcpp) version#
# #'#
# #' @return a list with elements fitted.g is fitted mixture#
# #' logLR : logP(D|mle(pi)) - logP(D|null)#
# #'#
# #' @export#
# fast.ash = function(betahat,sebetahat, #
#                     nullcheck=TRUE,randomstart=FALSE, #
#                     pointmass = TRUE,    #
#                     prior=c("nullbiased","uniform"), #
#                     mixsd=NULL, VB=FALSE,gridmult=4,#
#                     g=NULL, cxx=TRUE,#
#                     onlylogLR = FALSE,df=NULL){#
#   #
#   if(onlylogLR){#
#     pointmass <- TRUE  #
#   }#
#   #
#   #If method is supplied, use it to set up defaults; provide warning if these default values#
#   #are also specified by user#
#   if(!is.numeric(prior)){#
#     prior = match.arg(prior)#
#   }#
#   #
#   if(length(sebetahat)==1){#
#     sebetahat = rep(sebetahat,length(betahat))#
#   }#
#   if(length(sebetahat) != length(betahat)){#
#     stop("Error: sebetahat must have length 1, or same length as betahat")#
#   }#
#   #
#   completeobs = (!is.na(betahat) & !is.na(sebetahat))#
#   if(sum(completeobs)==0){#
#     if(onlylogLR){#
#       return(list(pi=NULL, logLR = 0))#
#     }else{#
#       stop("Error: all input values are missing")#
#     }#
#   }  #
#   #
#   if(is.null(mixsd)){#
#     mixsd= autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)#
#   }#
#   if(pointmass){#
#     mixsd = c(0,mixsd)#
#   }#
#   #
#   k=length(mixsd)  #
#   null.comp = which.min(mixsd) #which component is the "null"#
#   #
#   if(!is.numeric(prior)){#
#     if(prior=="nullbiased"){ # set up prior to favour "null"#
#       prior = rep(1,k)#
#       prior[null.comp] = 10 #prior 10-1 in favour of null#
#     }else if(prior=="uniform"){#
#       prior = rep(1,k)#
#     }#
#   }#
#   #
#   if(length(prior)!=k | !is.numeric(prior)){#
#     stop("invalid prior specification")#
#   }#
#   #
#   if(missing(g)){#
#     pi = prior^2 #default is to initialize pi at prior (mean)#
#     if(randomstart){pi=rgamma(k,1,1)}#
#     pi=normalize(pi)#
#     g=normalmix(pi,rep(0,k),mixsd)#
#     maxiter = 5000#
#   } else {#
#     maxiter = 1; # if g is specified, don't iterate the EM #
#   }#
#   #
#   pi.fit=EMest(betahat[completeobs],sebetahat[completeobs],g,prior,null.comp=null.comp,nullcheck=nullcheck,VB=VB,maxiter = maxiter, cxx=cxx, df=df)  #
#   #
#   if(onlylogLR){#
#     logLR = tail(pi.fit$loglik,1) - pi.fit$null.loglik#
#     return(list(pi=pi.fit$pi, logLR = logLR))#
#   }else{#
#     #
#     n=length(betahat)#
#     PosteriorMean = rep(0,length=n)#
#     PosteriorSD=rep(0,length=n)#
#     #
#     if(is.null(df)){#
#       PosteriorMean[completeobs] = postmean(pi.fit$g,betahat[completeobs],sebetahat[completeobs])#
#       PosteriorSD[completeobs] =postsd(pi.fit$g,betahat[completeobs],sebetahat[completeobs]) #
#     }#
#     else{#
#       PosteriorMean[completeobs] = postmean_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
#       PosteriorSD[completeobs] =postsd_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
#     }#
#     #FOR MISSING OBSERVATIONS, USE THE PRIOR INSTEAD OF THE POSTERIOR#
#     PosteriorMean[!completeobs] = mixmean(pi.fit$g)#
#     PosteriorSD[!completeobs] =mixsd(pi.fit$g)  #
#     #
#     result = list(fitted.g=pi.fit$g,PosteriorMean = PosteriorMean,PosteriorSD=PosteriorSD,call=match.call(),data=list(betahat = betahat, sebetahat=sebetahat))#
#     return(result)#
#   }#
#   #if(nsamp>0){#
#   #  sample = posterior_sample(post,nsamp)#
#   #}#
# }#
compute_lfsr = function(NegativeProb,ZeroProb){#
  ifelse(NegativeProb> 0.5*(1-ZeroProb),1-NegativeProb,NegativeProb+ZeroProb)#
}#
#
compute_lfsra = function(PositiveProb, NegativeProb,ZeroProb){#
  ifelse(PositiveProb<NegativeProb,2*PositiveProb+ZeroProb,2*NegativeProb+ZeroProb)  #
}  #
#
#' @title Estimate unimodal nonzero mean of a mixture model by EM algorithm#
#'#
#' @description Given the data, standard error of the data and standard deviations of the Gaussian mixture model, estimate the mean of a unimodal Gaussian mixture by an EM algorithm.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates unimodal mean \eqn{\mu} by EM algorithm. Uses the SQUAREM package to accelerate convergence of EM. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' #
#' @param betahat, a p vector of estimates #
#' @param sebetahat, a p vector of corresponding standard errors#
#' @param mixsd: vector of sds for underlying mixture components #
#' @param pi.init, the initial value of \eqn{\pi} to use. If not specified defaults to (1/k,...,1/k).#
#' @param tol, the tolerance for convergence of log-likelihood.#
#' @param maxiter the maximum number of iterations performed#
#' #
#' @return A list, including the estimates (\eqn{\mu}) and (\eqn{\pi}), the log likelihood for each iteration (NQ)#
#' and a flag to indicate convergence#
#'  #
#' @export#
#' #
#' #
nonzeromeanEM = function(betahat, sebetahat, mixsd, pi.init=NULL,tol=1e-7,maxiter=5000){#
  if(is.null(pi.init)){#
    pi.init = rep(1/length(mixsd),length(mixsd))# Use as starting point for pi#
  }#
  mupi=c(mean(betahat),pi.init)#
  res=squarem(par=mupi,fixptfn=nonzeromeanEMfixpoint,objfn=nonzeromeanEMobj,betahat=betahat,sebetahat=sebetahat,mixsd=mixsd,control=list(maxiter=maxiter,tol=tol))#
  return(list(nonzeromean=res$par[1],pi=res$par[-1],NQ=-res$value.objfn,niter = res$iter, converged=res$convergence,post=res$par))#
}#
#
nonzeromeanEMfixpoint = function(mupi,betahat,sebetahat,mixsd){#
	#omegamatrix=matrix(NA,nrow=length(betahat),ncol=length(mixsd))#
	mu=mupi[1]#
	pimean=mupi[-1]#
	sdmat = sqrt(outer(sebetahat ^2,mixsd^2,"+")) #
	xmat=matrix(rep(betahat,length(mixsd)),ncol=length(mixsd))#
	omegamatrix=t(t(dnorm(xmat,mean=mu,sd=sdmat))*pimean)#
	omegamatrix=omegamatrix /rowSums(omegamatrix)#
	pinew=normalize(colSums(omegamatrix))#
	munew=sum(omegamatrix*xmat/(sdmat^2))/sum(omegamatrix/(sdmat^2))#
	mupi=c(munew,pinew)#
	return(mupi)#
}#
#
nonzeromeanEMobj = function(mupi,betahat,sebetahat,mixsd){#
	mu=mupi[1]#
	pimean=mupi[-1]#
	sdmat = sqrt(outer(sebetahat ^2,mixsd^2,"+")) #
	xmat=matrix(rep(betahat,length(mixsd)),ncol=length(mixsd))#
	omegamatrix=t(t(dnorm(xmat,mean=mu,sd=sdmat))*pimean)#
	omegamatrix=omegamatrix /rowSums(omegamatrix)#
	NegativeQ=-sum(omegamatrix*dnorm(xmat,mean=mu,sd=sdmat,log=TRUE))#
	return(NegativeQ)#
}#
#' @title Estimate posterior distribution on mixture proportions of a mixture model by a Variational Bayes EM algorithm#
#'#
#' @description Given the individual component likelihoods for a mixture model, estimates the posterior on #
#' the mixture proportions by an VBEM algorithm. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates posterior on mixture proportions \eqn{\pi} by Variational Bayes, #
#' with a Dirichlet prior on \eqn{\pi}. #
#' Algorithm adapted from Bishop (2009), Pattern Recognition and Machine Learning, Chapter 10.#
#' #
#' @param matrix_lik: a n by k matrix with (j,k)th element equal to \eqn{f_k(x_j)}.#
#' @param prior: a k vector of the parameters of the Dirichlet prior on \eqn{\pi}. Recommended to be rep(1,k)#
#' @param post.init: the initial value of the posterior parameters. If not specified defaults to the prior parameters.#
#' @param tol: the tolerance for convergence of log-likelihood bound.#
#' @param maxiter: the maximum number of iterations performed#
#' #
#' @return A list, whose components include point estimates (pihat), #
#' the parameters of the fitted posterior on \eqn{\pi} (pipost),#
#' the bound on the log likelihood for each iteration (B)#
#' and a flag to indicate convergence (converged).#
#'  #
#' @export#
#' #
mixVBEM = function(matrix_lik, prior, pi.init = NULL,tol=1e-7, maxiter=5000){#
  k=ncol(matrix_lik)#
  if(is.null(pi.init)){#
    pi.init = rep(1,k)# Use as starting point for pi#
  } #
  res = squarem(par=pi.init,fixptfn=VBfixpoint, objfn=VBnegpenloglik,matrix_lik=matrix_lik, prior=prior, control=list(maxiter=maxiter,tol=tol))#
  return(list(pihat = res$par/sum(res$par), B=res$value.objfn, niter = res$iter, converged=res$convergence,post=res$par))#
}#
VBfixpoint = function(pipost, matrix_lik, prior){  #
  n=nrow(matrix_lik)#
  k=ncol(matrix_lik)#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost*matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  pipostnew = colSums(classprob) + prior#
  return(pipostnew)#
}#
#
VBnegpenloglik=function(pipost,matrix_lik,prior){#
  return(-VBpenloglik(pipost,matrix_lik,prior))#
}#
#
VBpenloglik = function(pipost, matrix_lik, prior){#
  n=nrow(matrix_lik)#
  k=ncol(matrix_lik)#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost*matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  B= sum(classprob*log(avgpipost*matrix_lik),na.rm=TRUE) - diriKL(prior,pipost) - sum(classprob*log(classprob)) #
  return(B)#
}#
#' @title Estimate mixture proportions of a mixture model by EM algorithm#
#'#
#' @description Given the individual component likelihoods for a mixture model, estimates the mixture proportions by an EM algorithm.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates mixture proportions \eqn{\pi} by maximum likelihood, or by maximum a posteriori (MAP) estimation for a Dirichlet prior on $\pi$ #
#' (if a prior is specified).  Uses the SQUAREM package to accelerate convergence of EM. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' #
#' @param matrix_lik, a n by k matrix with (j,k)th element equal to \eqn{f_k(x_j)}.#
#' @param prior, a k vector of the parameters of the Dirichlet prior on \eqn{\pi}. Recommended to be rep(1,k)#
#' @param pi.init, the initial value of \eqn{\pi} to use. If not specified defaults to (1/k,...,1/k).#
#' @param tol, the tolerance for convergence of log-likelihood.#
#' @param maxiter the maximum number of iterations performed#
#' #
#' @return A list, including the estimates (pihat), the log likelihood for each interation (B)#
#' and a flag to indicate convergence#
#'  #
#' @export#
#' #
#' #
mixEM = function(matrix_lik, prior, pi.init = NULL,tol=1e-7, maxiter=5000){#
  if(is.null(pi.init)){#
    pi.init = rep(1/k,k)# Use as starting point for pi#
  } #
  res = squarem(par=pi.init,fixptfn=fixpoint, objfn=negpenloglik,matrix_lik=matrix_lik, prior=prior, control=list(maxiter=maxiter,tol=tol))#
  return(list(pihat = normalize(pmax(0,res$par)), B=res$value.objfn, #
              niter = res$iter, converged=res$convergence))#
}#
#
# helper functions used by mixEM#
normalize = function(x){return(x/sum(x))}#
#
fixpoint = function(pi, matrix_lik, prior){  #
  pi = normalize(pmax(0,pi)) #avoid occasional problems with negative pis due to rounding#
  m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
  m.rowsum = rowSums(m)#
  classprob = m/m.rowsum #an n by k matrix#
  pinew = normalize(colSums(classprob) + prior - 1)#
  return(pinew)#
}#
#
negpenloglik = function(pi,matrix_lik,prior){return(-penloglik(pi,matrix_lik,prior))}#
#
penloglik = function(pi, matrix_lik, prior){#
  pi = normalize(pmax(0,pi))#
  m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
  m.rowsum = rowSums(m)#
  loglik = sum(log(m.rowsum))#
  subset = (prior != 1.0)#
  priordens = sum((prior-1)[subset]*log(pi[subset]))#
  return(loglik+priordens)#
}#
#
#The kth element of this vector is the derivative #
#of the loglik for $\pi=(\pi_0,...,1-\pi_0,...)$ with respect to $\pi_0$ at $\pi_0=1$.#
gradient = function(matrix_lik){#
  n = nrow(matrix_lik)#
  grad = n - colSums(matrix_lik/matrix_lik[,1]) #
  return(grad)#
}#
#
# mixEM = function(matrix_lik, prior, pi.init = NULL,tol=0.0001, maxiter=5000){#
#   n=nrow(matrix_lik)#
#   k=ncol(matrix_lik)#
#   B = rep(0,maxiter)#
#   pi = pi.init#
#   if(is.null(pi.init)){#
#     pi = rep(1/k,k)# Use as starting point for pi#
#   } #
#   pi = ifelse(pi<1e-5,1e-5,pi) #set any estimates that are too small to be just very small#
#   pi = normalize(pi)#
#   #
#   loglik = rep(0,maxiter)#
#   priordens= rep(0,maxiter)#
#   m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
#   m.rowsum = rowSums(m)#
#   loglik[1] = sum(log(m.rowsum))#
#   priordens[1] = sum((prior-1)*log(pi)) #
#   classprob = m/m.rowsum #an n by k matrix#
#   i=1#
#   if(maxiter >= 2){#
#     for(i in 2:maxiter){  #
#       pi = colSums(classprob) + prior-1#
#       pi = ifelse(pi<1e-5,1e-5,pi) #set any estimates that are less than zero, which can happen with prior<1, to 0#
#       pi = normalize(pi)#
#         #
#       #Now re-estimate pi#
#       m  = t(pi * t(matrix_lik)) #
#       m.rowsum = rowSums(m)#
#       loglik[i] = sum(log(m.rowsum))#
#       priordens[i] = sum((prior-1)*log(pi)) #
#       classprob = m/m.rowsum#
#     #
#     #
#       if(abs(loglik[i]+priordens[i]-loglik[i-1]-priordens[i-1])<tol) break;#
#     }#
#   }#
#   converged=(abs(loglik[i]+priordens[i]-loglik[i-1]-priordens[i-1])<tol)#
#   if(!converged){#
#       warning("EM algorithm in function mixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
#   }#
#   return(list(pihat = pi, B=loglik[1:i], #
#               niter = i, converged=converged))#
# }#
#estimate mixture proportions of sigmaa by EM algorithm#
#prior gives the parameter of a Dirichlet prior on pi#
#(prior is used to encourage results towards smallest value of sigma when#
#likelihood is flat)#
#nullcheck indicates whether to check whether the loglike exceeds the null#
#(may not want to use if prior is used)#
#VB provides an approach to estimate the approximate posterior distribution#
#of mixture proportions of sigmaa by variational Bayes method#
#(use Dirichlet prior and approximate Dirichlet posterior)#
#if cxx TRUE use cpp version of R function mixEM#
EMest = function(betahat,sebetahat,g,prior,null.comp=1,nullcheck=TRUE,VB=FALSE, maxiter=5000, cxx=TRUE, df=NULL){ #
  pi.init = g$pi#
  k=ncomp(g)#
  n = length(betahat)#
  tol = min(0.1/n,1e-5) # set convergence criteria to be more stringent for larger samples#
  if(is.null(df)){#
    matrix_lik = t(compdens_conv(g,betahat,sebetahat))#
  }#
  else{#
    matrix_lik = t(compdens_conv_t(g,betahat,sebetahat,df))#
  }#
  #checks whether the gradient at pi0=1 is positive (suggesting that this is a fixed point)#
  #if(nullcheck){#
  #  if(all(gradient(matrix_lik)>=0)){#
  #    pi.init=rep(0,k)#
  #    pi.init[null.comp]=1 #this will make pi.init=(1,0,0...,0) which is a fixed point of the EM#
  #  }#
  #}#
  if(VB==TRUE){#
    EMfit=mixVBEM(matrix_lik,prior,maxiter=maxiter)}#
  else{#
    if (cxx==TRUE){#
      EMfit = cxxMixEM(matrix_lik,prior,pi.init,1e-5, maxiter) #currently use different convergence criteria for cxx version #
      if(!EMfit$converged){#
        warning("EM algorithm in function cxxMixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
      }#
    }#
    else{#
      EMfit = mixEM(matrix_lik,prior,pi.init,tol, maxiter)#
      if(!EMfit$converged & !(maxiter==1)){#
        warning("EM algorithm in function mixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
      }#
    }#
  }#
  pi = EMfit$pihat     #
  penloglik = EMfit$B #
  converged = EMfit$converged#
  niter = EMfit$niter#
  loglik.final =  penloglik(pi,matrix_lik,1) #compute penloglik without penalty#
  null.loglik = sum(log(matrix_lik[,null.comp]))  #
  if(nullcheck==TRUE & VB==FALSE){ #null check doesn't work with VB yet#
    pinull = rep(0,k)#
    pinull[null.comp]=1#
    null.penloglik = penloglik(pinull,matrix_lik,prior)#
    final.penloglik = penloglik(pi,matrix_lik,prior)#
    if(null.penloglik > final.penloglik){ #check whether exceeded "null" likelihood where everything is null#
      pi=pinull#
      loglik.final=penloglik(pi,matrix_lik,1)#
    }#
  }#
  g$pi=pi#
  return(list(loglik=loglik.final,null.loglik=null.loglik,#
              matrix_lik=matrix_lik,converged=converged,g=g))#
}#
#' @title Compute Posterior#
#'#
#' @description Return the posterior on beta given a prior (g) that is a mixture of normals (class normalmix) #
#' and observation betahat \sim N(beta,sebetahat)#
#'#
#' @details This can be used to obt#
#'#
#' @param g: a normalmix with components indicating the prior; works only if g has means 0#
#' @param betahat (n vector of observations) #
#' @param sebetahat (n vector of standard errors/deviations of observations)#
#' #
#' @return A list, (pi1,mu1,sigma1) whose components are each k by n matrices#
#' where k is number of mixture components in g, n is number of observations in betahat#
#' #
#' @export#
#' #
#' #
posterior_dist = function(g,betahat,sebetahat){#
  if(class(g)!="normalmix"){#
    stop("Error: posterior_dist implemented only for g of class normalmix")#
  }#
  pi0 = g$pi#
  mu0 = g$mean#
  sigma0 = g$sd  #
  k= length(pi0)#
  n= length(betahat)#
  if(!all.equal(g$mean,rep(0,k))) stop("Error: posterior_dist currently only implemented for zero-centered priors")#
  pi1 = pi0 * t(matrix_dens(betahat,sebetahat,sigma0))#
  pi1 = apply(pi1, 2, normalize) #pi1 is now an k by n matrix#
  #make k by n matrix versions of sigma0^2 and sebetahat^2#
  # and mu0 and betahat#
  s0m2 = matrix(sigma0^2,nrow=k,ncol=n,byrow=FALSE)#
  sebm2 = matrix(sebetahat^2,nrow=k,ncol=n, byrow=TRUE)#
  mu0m = matrix(mu0,nrow=k,ncol=n,byrow=FALSE)#
  bhatm = matrix(betahat,nrow=k,ncol=n,byrow=TRUE)#
  sigma1 = (s0m2*sebm2/(s0m2 + sebm2))^(0.5)  #
  w = sebm2/(s0m2 + sebm2)#
  mu1 = w*mu0m + (1-w)*bhatm#
  #WHERE DATA ARE MISSING, SET POSTERIOR = PRIOR#
  ismiss = (is.na(betahat) | is.na(sebetahat)) #
  pi1[,ismiss] = pi0#
  mu1[,ismiss] = mu0#
  sigma1[,ismiss] = sigma0#
  return(list(pi=pi1,mu=mu1,sigma=sigma1))#
}#
#
#return matrix of densities of observations (betahat) #
# assuming betahat_j \sim N(0, sebetahat_j^2 + sigmaavec_k^2)#
#normalized by maximum of each column#
#INPUT#
#betahat is n vector, #
#sebetahat is n vector, #
#sigmaavec is k vector#
#return is n by k matrix of the normal likelihoods, #
# with (j,k)th element the density of N(betahat_j; mean=0, var = sebetahat_j^2 + sigmaavec_k^2)#
#normalized to have maximum 1 in each column#
matrix_dens = function(betahat, sebetahat, sigmaavec){#
  k = length(sigmaavec)#
  n = length(betahat)#
  ldens = dnorm(betahat,0,sqrt(outer(sebetahat^2,sigmaavec^2,FUN="+")),log=TRUE)#
  maxldens = apply(ldens, 1, max)#
  ldens = ldens - maxldens#
  return(exp(ldens))#
}#
#
#return the "effective" estimate#
#that is the effect size betanew whose z score betanew/se#
#would give the same p value as betahat/se compared to a t with df#
effective.effect=function(betahat,se,df){#
  p = pt(betahat/se,df)#
  qnorm(p,sd=se)#
}#
#' @title Function to compute q values from local false discovery rates#
#'#
#' @description Computes q values from a vector of local fdr estimates#
#'#
#' @details The q value for a given lfdr is an estimate of the (tail) False Discovery Rate #
#' for all findings with a smaller lfdr, and is found by the average of the lfdr for#
#' all more significant findings. See Storey (2003), Annals of Statistics, for definition of q value.  #
#' #
#' #
#' @param lfdr, a vector of local fdr estimates#
#'#
#' @return vector of q values#
#' #
#' @export#
qval.from.lfdr = function(lfdr){#
  o = order(lfdr)#
  qvalue=rep(NA,length(lfdr))#
  qvalue[o] = (cumsum(sort(lfdr))/(1:sum(!is.na(lfdr))))#
  return(qvalue)#
}#
#
# try to select a default range for the sigmaa values#
# that should be used, based on the values of betahat and sebetahat#
# mult is the multiplier by which the sds differ across the grid#
autoselect.mixsd = function(betahat,sebetahat,mult){#
  sigmaamin = min(sebetahat)/10 #so that the minimum is small compared with measurement precision#
  if(all(betahat^2<sebetahat^2)){#
    sigmaamax = 8*sigmaamin #to deal with the occassional odd case where this could happen; 8 is arbitrary#
  } else {#
    sigmaamax = 2*sqrt(max(betahat^2-sebetahat^2)) #this computes a rough largest value you'd want to use, based on idea that sigmaamax^2 + sebetahat^2 should be at least betahat^2   #
  }#
  if(mult==0){#
    return(c(0,sigmaamax/2))#
  }else{#
    npoint = ceiling(log2(sigmaamax/sigmaamin)/log2(mult))#
    return(mult^((-npoint):0) * sigmaamax)#
  }#
}#
#' @title Summary method for ash object#
#'#
#' @description Print summary of fitted ash object#
#'#
#' @details See readme for more details#
#' #
#' @export#
#' #
summary.ash=function(a){#
  print(a$fitted.g)#
  print(tail(a$fit$loglik,1),digits=10)#
  print(a$fit$converged)#
}#
#
#' @title Print method for ash object#
#'#
#' @description Print the fitted distribution of beta values in the EB hierarchical model#
#'#
#' @details None#
#' #
#' @export#
#' #
print.ash =function(a){#
  print(a$fitted.g)#
}#
#
#' @title Plot method for ash object#
#'#
#' @description Plot the density of the underlying fitted distribution#
#'#
#' @details None#
#' #
#' @export#
#' #
plot.ash = function(a,xmin,xmax,...){#
  x = seq(xmin,xmax,length=1000)#
  y = density(a,x)#
  plot(y,type="l",...)#
}#
#
#compute the predictive density of an observation#
#given the fitted ash object a and the vector se of standard errors#
#not implemented yet#
predictive=function(a,se){#
}#
#' @title Get fitted loglikelihood for ash object#
#'#
#' @description Return the log-likelihood of the data under the fitted distribution#
#'#
#' @param a the fitted ash object#
#'#
#' @details None#
#' #
#' @export#
#' #
#'#
get_loglik = function(a){#
  return(tail(a$fit$loglik,1))#
}#
#
#' @title Get pi0 estimate for ash object#
#'#
#' @description Return estimate of the null proportion, pi0#
#'#
#' @param a the fitted ash object#
#'#
#' @details Extracts the estimate of the null proportion, pi0, from the object a#
#' #
#' @export#
#' #
get_pi0 = function(a){#
  null.comp = comp_sd(a$fitted.g)==0#
  return(sum(a$fitted.g$pi[null.comp]))#
}#
#
#' @title Compute loglikelihood for data from ash fit#
#'#
#' @description Return the log-likelihood of the data betahat, with standard errors betahatsd, #
#' under the fitted distribution in the ash object. #
#' #
#'#
#' @param a the fitted ash object#
#' @param betahat the data#
#' @param betahatsd the observed standard errors#
#' @param zscores indicates whether ash object was originally fit to z scores #
#' @details None#
#' #
#' @export#
#' #
#'#
loglik.ash = function(a,betahat,betahatsd,zscores=FALSE){#
  g=a$fitted.g#
  FUN="+"#
  if(zscores==TRUE){#
    g$sd = sqrt(g$sd^2+1) #
    FUN="*"#
  }#
  return(loglik_conv(g,betahat, betahatsd,FUN))#
}#
#
#' @title Density method for ash object#
#'#
#' @description Return the density of the underlying fitted distribution#
#'#
#' @param a the fitted ash object#
#' @param x the vector of locations at which density is to be computed#
#'#
#' @details None#
#' #
#' @export#
#' #
#'#
density.ash=function(a,x){list(x=x,y=dens(a$fitted.g,x))}#
#
#' @title cdf method for ash object#
#'#
#' @description Computed the cdf of the underlying fitted distribution#
#'#
#' @param a the fitted ash object#
#' @param x the vector of locations at which cdf is to be computed#
#' @param lower.tail (default=TRUE) whether to compute the lower or upper tail#
#'#
#' @details None#
#' #
#' @export#
#' #
#'#
cdf.ash=function(a,x,lower.tail=TRUE){#
  return(list(x=x,y=mixcdf(a$fitted.g,x,lower.tail)))#
}#
#return the KL-divergence between 2 dirichlet distributions#
#p,q are the vectors of dirichlet parameters of same lengths#
diriKL = function(p,q){#
  p.sum = sum(p)#
  q.sum = sum(q)#
  k = length(q)#
  KL = lgamma(q.sum)-lgamma(p.sum)+sum((q-p)*(digamma(q)-digamma(rep(q.sum,k))))+sum(lgamma(p)-lgamma(q))#
  return(KL)#
}#
#
#helper function for VBEM#
VB.update = function(matrix_lik, pipost){#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost * matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  B = sum(classprob*log(avgpipost*matrix_lik),na.rm=TRUE) - diriKL(prior,pipost) #negative free energy#
  return(list(classprob=classprob,B=B))#
}
GENERIC FUNCTIONS #############################
# find matrix of densities at y, for each component of the mixture#
# INPUT y is an n-vector#
# OUTPUT k by n matrix of densities#
compdens = function(x,y,log=FALSE){#
  UseMethod("compdens")#
}#
compdens.default = function(x,y,log=FALSE){#
  stop(paste("Invalid class", class(m), "for first argument in",  match.call()))  #
}#
#
#standard deviations#
comp_sd = function(m){#
  UseMethod("comp_sd")#
}#
comp_sd.default = function(m){#
  stop("method comp_sd not written for this class")#
}#
#
#second moments#
comp_mean2 = function(m){#
  UseMethod("comp_mean2")#
}#
comp_mean2.default = function(m){#
  comp_sd(m)^2 + comp_mean(m)^2#
}#
#return the overall mean of the mixture#
mixmean = function(m){#
  UseMethod("mixmean")#
}#
mixmean.default = function(m){#
  sum(m$pi * comp_mean(m))#
}#
#
#return the overall second moment of the mixture#
mixmean2 = function(m){#
  UseMethod("mixmean2")#
}#
mixmean2.default = function(m){#
  sum(m$pi * comp_mean2(m))#
}#
#
#return the overall sd of the mixture#
mixsd = function(m){#
  UseMethod("mixsd")#
}#
mixsd.default = function(m){#
  sqrt(mixmean2(m)-mixmean(m)^2)#
}#
#
#means#
comp_mean = function(m){#
  UseMethod("comp_mean")#
}#
comp_mean.default = function(m){#
  stop("method comp_mean not written for this class")#
}#
#
#number of components#
ncomp = function(m){#
  UseMethod("ncomp")#
}#
ncomp.default = function(m){#
  return(length(m$pi))#
}#
#
#return mixture proportions, a generic function#
mixprop = function(m){#
  UseMethod("mixprop")#
}#
mixprop.default = function(m){#
  m$pi#
}#
#
#' @title mixcdf#
#'#
#' @description Returns cdf for a mixture (generic function)#
#' #
#' @details None#
#' #
#' @param x a mixture (eg of type normalmix or unimix)#
#' @param y locations at which cdf to be computed#
#' @param lower.tail: boolean indicating whether to report lower tail#
#' #
#' @return an object of class normalmix#
#' #
#' @export#
#' #
#' @examples mixcdf(normalmix(c(0.5,0.5),c(0,0),c(1,2)),seq(-4,4,length=100))#
#' #
mixcdf = function(x,y,lower.tail=TRUE){#
  UseMethod("mixcdf")#
}#
#' @title mixcdf.default#
#' @export#
#' #
mixcdf.default = function(x,y,lower.tail=TRUE){#
  x$pi %*% comp_cdf(x,y,lower.tail)#
}#
#
#find cdf for each component, a generic function#
comp_cdf = function(x,y,lower.tail=TRUE){#
  UseMethod("comp_cdf")#
}#
comp_cdf.default = function(x,y,lower.tail=TRUE){#
  stop("comp_cdf not implemented for this class")#
}#
#find density at y, a generic function#
dens = function(x,y){#
  UseMethod("dens")#
}#
dens.default = function(x,y){#
  return (x$pi %*% compdens(x, y))#
}#
#
#find log likelihood of data in x (a vector) for mixture in m#
loglik = function(m,x){#
  UseMethod("loglik")#
}#
loglik.default = function(m,x){#
  sum(log(dens(m,x)))#
}#
#
#find log likelihood of data in betahat, when #
#the mixture m is convolved with a normal with sd betahatsd#
#betahatsd is an n vector#
#betahat is an n vector#
#' @title loglik_conv#
#' #
#' @export#
#' #
loglik_conv = function(m,betahat,betahatsd,FUN="+"){#
  UseMethod("loglik_conv")#
}#
#' @title loglik_conv.default#
#' #
#' @export#
#' #
loglik_conv.default = function(m,betahat,betahatsd,FUN="+"){#
  sum(log(dens_conv(m,betahat,betahatsd,FUN)))#
}#
#
#compute the density of the components of the mixture m#
#when convoluted with a normal with standard deviation s#
#the density is evaluated at x#
#x and s are n-vectors#
#m is a mixture with k components#
#output is a k by n matrix of densities#
compdens_conv = function(m, x, s, FUN="+"){#
  UseMethod("compdens_conv")#
}#
compdens_conv.default = function(m,x, s,FUN="+"){#
  stop(paste("Invalid class", class(m), "for first argument in",  match.call()))  #
}#
#
#compute density of mixture m convoluted with normal of sd (s)#
#at locations x#
#m is a mixture#
#x is an n vector#
#s is an n vector or integer#
dens_conv = function(m,x,s,FUN="+"){#
  UseMethod("dens_conv")#
}#
dens_conv.default = function(m,x,s,FUN="+"){#
  colSums(m$pi * compdens_conv(m,x,s,FUN))#
}#
#
#compute the posterior prob that each observation#
#came from each component of the mixture m#
#output a k by n vector of probabilities#
#computed by weighting the component densities by pi#
#and then normalizing#
comppostprob=function(m,x,s){#
 UseMethod("comppostprob") #
}#
comppostprob.default = function(m,x,s){#
  tmp= (t(m$pi * compdens_conv(m,x,s))/dens_conv(m,x,s))#
  ismissing = (is.na(x) | is.na(s))#
  tmp[ismissing,]=m$pi#
  t(tmp)#
}#
# evaluate cdf of posterior distribution of beta at c#
# m is the prior on beta, a mixture#
# c is location of evaluation#
# assumption is betahat | beta \sim N(beta,sebetahat)#
# m is a mixture with k components#
# c a scalar#
# betahat, sebetahat are n vectors #
# output is a k by n matrix#
compcdf_post=function(m,c,betahat,sebetahat){#
  UseMethod("compcdf_post")#
}#
compcdf_post.default=function(m,c,betahat,sebetahat){#
  stop("method compcdf_post not written for this class")#
}#
cdf_post = function(m,c,betahat,sebetahat){#
  UseMethod("cdf_post")#
}#
cdf_post.default=function(m,c,betahat,sebetahat){#
  colSums(comppostprob(m,betahat,sebetahat)*compcdf_post(m,c,betahat,sebetahat))#
}#
#
#output posterior mean for beta for prior mixture m,#
#given observations betahat, sebetahat#
postmean = function(m, betahat,sebetahat){#
  UseMethod("postmean")#
}#
postmean.default = function(m,betahat,sebetahat){#
  colSums(comppostprob(m,betahat,sebetahat) * comp_postmean(m,betahat,sebetahat))#
}#
#
#output posterior mean-squared value for beta for prior mixture m,#
#given observations betahat, sebetahat#
postmean2 = function(m, betahat,sebetahat){#
  UseMethod("postmean2")#
}#
postmean2.default = function(m,betahat,sebetahat){#
  colSums(comppostprob(m,betahat,sebetahat) * comp_postmean2(m,betahat,sebetahat))#
}#
#
#output posterior sd for beta for prior mixture m,#
#given observations betahat, sebetahat#
postsd = function(m, betahat,sebetahat){#
  UseMethod("postsd")#
}#
postsd.default = function(m,betahat,sebetahat){#
  sqrt(postmean2(m,betahat,sebetahat)-postmean(m,betahat,sebetahat)^2)#
}#
#
#output posterior mean-squared value for beta for prior mixture m,#
#given observations betahat, sebetahat#
comp_postmean2 = function(m, betahat,sebetahat){#
  UseMethod("comp_postmean2")#
}#
comp_postmean2.default = function(m,betahat,sebetahat){#
  comp_postsd(m,betahat,sebetahat)^2 + comp_postmean(m,betahat,sebetahat)^2#
}#
#output posterior mean for beta for each component of prior mixture m,#
#given observations betahat, sebetahat#
comp_postmean = function(m, betahat,sebetahat){#
  UseMethod("comp_postmean")#
}#
comp_postmean.default = function(m,betahat,sebetahat){#
  stop("method comp_postmean not written for this class")#
}#
#
#output posterior sd for beta for each component of prior mixture m,#
#given observations betahat, sebetahat#
comp_postsd = function(m, betahat,sebetahat){#
  UseMethod("comp_postsd")#
}#
comp_postsd.default = function(m,betahat,sebetahat){#
  stop("method comp_postsd not written for this class")#
}#
#
#find nice limits of mixture m for plotting#
min_lim = function(m){#
  UseMethod("min_lim")#
}#
min_lim.default=function(m){#
  -5#
}#
#
max_lim = function(m){#
  UseMethod("max_lim")#
}#
max_lim.default=function(m){#
  5#
}#
#plot density of mixture#
plot_dens = function(m,npoints=100,...){#
  UseMethod("plot_dens")#
}#
plot_dens.default = function(m,npoints=100,...){#
  x = seq(min_lim(m),max_lim(m),length=npoints)#
  plot(x,dens(m,x),type="l",xlab="density",ylab="x",...)#
}#
#
plot_post_cdf = function(m,betahat,sebetahat,npoints=100,...){#
  UseMethod("plot_post_cdf")#
}#
plot_post_cdf.default = function(m,betahat,sebetahat,npoints=100,...){#
  x = seq(min_lim(m),max_lim(m),length=npoints)#
  x_cdf = vapply(x,cdf_post,FUN.VALUE=betahat,m=m,betahat=betahat,sebetahat=sebetahat)#
  plot(x,x_cdf,type="l",xlab="x",ylab="cdf",...)#
 # for(i in 2:nrow(x_cdf)){#
 #   lines(x,x_cdf[i,],col=i)#
 # }#
}#
#
############################### METHODS FOR normalmix class ############################
#
#' @title Constructor for normalmix class#
#'#
#' @description Creates an object of class normalmix (finite mixture of univariate normals)#
#' #
#' @details None#
#' #
#' @param pi vector of mixture proportions#
#' @param mean vector of means#
#' @param sd: vector of standard deviations#
#' #
#' @return an object of class normalmix#
#' #
#' @export#
#' #
#' @examples normalmix(c(0.5,0.5),c(0,0),c(1,2))#
#' #
normalmix = function(pi,mean,sd){#
  structure(data.frame(pi,mean,sd),class="normalmix")#
}#
#
comp_sd.normalmix = function(m){#
  m$sd#
}#
#
comp_mean.normalmix = function(m){#
  m$mean#
}#
#
compdens.normalmix = function(x,y,log=FALSE){#
  k=ncomp(x)#
  n=length(y)#
  d = matrix(rep(y,rep(k,n)),nrow=k)#
  return(matrix(dnorm(d, x$mean, x$sd, log),nrow=k))  #
}#
#
#density of convolution of each component of a normal mixture with N(0,s^2) at x#
# x an n-vector at which density is to be evaluated#
#return a k by n matrix#
#Note that convolution of two normals is normal, so it works that way#
compdens_conv.normalmix = function(m, x, s,FUN="+"){#
  if(length(s)==1){s=rep(s,length(x))}#
  sdmat = sqrt(outer(s^2,m$sd^2,FUN)) #n by k matrix of standard deviations of convolutions#
  return(t(dnorm(outer(x,m$mean,FUN="-")/sdmat)/sdmat))#
}#
comp_cdf.normalmix = function(x,y,lower.tail=TRUE){#
  vapply(y,pnorm,x$mean,x$mean,x$sd,lower.tail)#
}#
#
#c is a scalar#
#m a mixture with k components#
#betahat a vector of n observations#
#sebetahat an n vector of standard errors#
#return a k by n matrix of the posterior cdf#
compcdf_post.normalmix=function(m,c,betahat,sebetahat){#
  k = length(m$pi)#
  n=length(betahat)#
  #compute posterior standard deviation (s1) and posterior mean (m1)#
  s1 = sqrt(outer(sebetahat^2,m$sd^2,FUN="*")/outer(sebetahat^2,m$sd^2,FUN="+"))#
  ismissing = (is.na(betahat) | is.na(sebetahat))#
  s1[ismissing,]=m$sd#
  m1 = t(comp_postmean(m,betahat,sebetahat))#
  t(pnorm(c,mean=m1,sd=s1))#
}#
#
#return posterior mean for each component of prior m, given observations betahat and sebetahat#
#input, m is a mixture with k components#
#betahat, sebetahat are n vectors#
#output is a k by n matrix#
comp_postmean.normalmix = function(m,betahat,sebetahat){#
  tmp=(outer(sebetahat^2,m$mean, FUN="*") + outer(betahat,m$sd^2, FUN="*"))/outer(sebetahat^2,m$sd^2,FUN="+")#
  ismissing = (is.na(betahat) | is.na(sebetahat))#
  tmp[ismissing,]=m$mean #return prior mean when missing data#
  t(tmp)#
}#
#
#return posterior standard deviation for each component of prior m, given observations betahat and sebetahat#
#input, m is a mixture with k components#
#betahat, sebetahat are n vectors#
#output is a k by n matrix#
comp_postsd.normalmix = function(m,betahat,sebetahat){#
  t(sqrt(outer(sebetahat^2,m$sd^2,FUN="*")/outer(sebetahat^2,m$sd^2,FUN="+")))#
}#
############################### METHODS FOR unimix class ############################
#
#constructor; pi, a and b are vectors; kth component is Uniform(a[k],b[k])#
unimix = function(pi,a,b){#
  structure(data.frame(pi,a,b),class="unimix")#
}#
#
comp_cdf.unimix = function(m,y,lower.tail=TRUE){#
  vapply(y,punif,m$a,min=m$a,max=m$b,lower.tail)#
}#
#
comp_sd.unimix = function(m){#
  (m$b-m$a)/sqrt(12)#
}#
#
comp_mean.unimix = function(m){#
  (m$a+m$b)/2#
}#
compdens.unimix = function(x,y,log=FALSE){#
  k=ncomp(x)#
  n=length(y)#
  d = matrix(rep(y,rep(k,n)),nrow=k)#
  return(matrix(dunif(d, x$a, x$b, log),nrow=k))  #
}#
#
#density of convolution of each component of a unif mixture with N(0,s) at x#
# x an n-vector#
#return a k by n matrix#
compdens_conv.unimix = function(m, x, s, FUN="+"){#
  if(FUN!="+") stop("Error; compdens_conv not implemented for uniform with FUN!=+")#
  compdens= t(pnorm(outer(x,m$a,FUN="-")/s)-pnorm(outer(x,m$b,FUN="-")/s))/(m$b-m$a)#
  compdens[m$a==m$b,]=t(dnorm(outer(x,m$a,FUN="-")/s)/s)[m$a==m$b,]#
  return(compdens)#
}#
#c is a scalar#
#m a mixture with k components#
#betahat a vector of n observations#
#sebetahat an n vector of standard errors#
#return a k by n matrix of the posterior cdf#
compcdf_post.unimix=function(m,c,betahat,sebetahat){#
  k = length(m$pi)#
  n=length(betahat)#
  tmp = matrix(1,nrow=k,ncol=n)#
  tmp[m$a > c,] = 0#
  subset = m$a<=c & m$b>c # subset of components (1..k) with nontrivial cdf#
  if(sum(subset)>0){#
    pna = pnorm(outer(betahat,m$a[subset],FUN="-")/sebetahat)#
    pnc = pnorm(outer(betahat,rep(c,sum(subset)),FUN="-")/sebetahat)#
    pnb = pnorm(outer(betahat,m$b[subset],FUN="-")/sebetahat)#
    tmp[subset,] = t((pnc-pna)/(pnb-pna))#
  }#
  subset = (m$a == m$b) #subset of components with trivial cdf#
  tmp[subset,]= rep(m$a[subset] <= c,n)#
  tmp#
}#
#
my_etruncnorm= function(a,b,mean=0,sd=1){#
  alpha = (a-mean)/sd#
  beta =  (b-mean)/sd#
 #Flip the onese where both are positive, as the computations are more stable#
  #when both negative#
  flip = (alpha>0 & beta>0)#
  flip[is.na(flip)]=FALSE #deal with NAs#
  alpha[flip]= -alpha[flip]#
  beta[flip]=-beta[flip]#
  tmp= (-1)^flip * (mean+sd*etruncnorm(alpha,beta,0,1))#
  max_alphabeta = ifelse(alpha<beta, beta,alpha)#
  max_ab = ifelse(alpha<beta,b,a)#
  toobig = max_alphabeta<(-30)#
  toobig[is.na(toobig)]=FALSE #
  tmp[toobig] = max_ab[toobig]#
  tmp#
}#
#return posterior mean for each component of prior m, given observations betahat and sebetahat#
#input, m is a mixture with k components#
#betahat, sebetahat are n vectors#
#output is a k by n matrix#
#note that with uniform prior, posterior is truncated normal, so#
#this is computed using formula for mean of truncated normal #
comp_postmean.unimix = function(m,betahat,sebetahat){#
#   k= ncomp(m)#
#   n=length(betahat)#
#   a = matrix(m$a,nrow=n,ncol=k,byrow=TRUE)#
#   b = matrix(m$b,nrow=n,ncol=k,byrow=TRUE)#
#   matrix(etruncnorm(a,b,betahat,sebetahat),nrow=k,byrow=TRUE)#
  #note: etruncnorm is more stable for a and b negative than positive#
  #so maybe use this, and standardize to make the whole more stable.#
  alpha = outer(-betahat, m$a,FUN="+")/sebetahat#
  beta = outer(-betahat, m$b, FUN="+")/sebetahat#
  tmp = betahat + sebetahat*my_etruncnorm(alpha,beta,0,1)#
  ismissing = is.na(betahat) | is.na(sebetahat)#
  tmp[ismissing,]= (m$a+m$b)/2#
  t(tmp)#
#   t(#
#     betahat + sebetahat* #
#       exp(dnorm(alpha,log=TRUE)- pnorm(alpha,log=TRUE))#
#    * #
#       (-expm1(dnorm(beta,log=TRUE)-dnorm(alpha,log=TRUE)))#
#     /#
#       (expm1(pnorm(beta,log=TRUE)-pnorm(alpha,log=TRUE)))#
#   )#
}#
#
#not yet implemented!#
#just returns 0s for now#
comp_postsd.unimix = function(m,betahat,sebetahat){#
  print("Warning: Posterior SDs not yet implemented for uniform components")#
  k= ncomp(m)#
  n=length(betahat)#
  return(matrix(NA,nrow=k,ncol=n)) #
}#
######################################################################################
########################## functions for student.t likelihood ########################
#compute the density of the components of the mixture m#
#when convoluted with a scaled (se) student.t with df v #
compdens_conv_t = function(m, x, s, v, FUN="+"){#
  UseMethod("compdens_conv_t")#
}#
compdens_conv_t.default = function(m,x, s,v,FUN="+"){#
  stop(paste("Invalid class", class(m), "for first argument in",  match.call()))#
}#
#density of convolution of each component of a normal mixture with s*t(v) at x#
# x an n-vector at which density is to be evaluated#
#return a k by n matrix#
#Note that convolution of two normals is normal, so it works that way#
compdens_conv_t.normalmix = function(m, x, s, v,FUN="+"){#
  stop("Error: normal mixture for student-t likelihood is not yet implemented")#
}#
#density of convolution of each component of a unif mixture with s*t(v) at x#
# x an n-vector#
#return a k by n matrix#
compdens_conv_t.unimix = function(m, x, s, v , FUN="+"){#
  if(FUN!="+") stop("Error; compdens_conv_t not implemented for uniform with FUN!=+")#
  compdens= t(pt(outer(x,m$a,FUN="-")/s,df=v)-pt(outer(x,m$b,FUN="-")/s,df=v))/(m$b-m$a)#
  compdens[m$a==m$b,]=t(dt(outer(x,m$a,FUN="-")/s,df=v)/s)[m$a==m$b,]#
  return(compdens)#
}#
#
#compute density of mixture m convoluted with student t with df v#
#at locations x#
#m is a mixture#
#x is an n vector#
#s is an n vector or integer#
dens_conv_t = function(m,x,s,v,FUN="+"){#
  UseMethod("dens_conv_t")#
}#
dens_conv_t.default = function(m,x,s,v,FUN="+"){#
  colSums(m$pi * compdens_conv_t(m,x,s,v,FUN))#
}#
#
#compute the posterior prob that each observation#
#came from each component of the mixture m#
#output a k by n vector of probabilities#
#computed by weighting the component densities by pi#
#and then normalizing#
comppostprob_t=function(m,x,s,v){#
  UseMethod("comppostprob_t") #
}#
comppostprob_t.default = function(m,x,s,v){#
  tmp= (t(m$pi * compdens_conv_t(m,x,s,v))/dens_conv_t(m,x,s,v))#
  ismissing = (is.na(x) | is.na(s))#
  tmp[ismissing,]=m$pi#
  t(tmp)#
}#
cdf_post_t = function(m,c,betahat,sebetahat,v){#
  UseMethod("cdf_post_t")#
}#
cdf_post_t.default=function(m,c,betahat,sebetahat,v){#
  colSums(comppostprob_t(m,betahat,sebetahat,v)*compcdf_post_t(m,c,betahat,sebetahat,v))#
}#
#
compcdf_post_t=function(m,c,betahat,sebetahat,v){#
  UseMethod("compcdf_post_t")#
}#
compcdf_post_t.default=function(m,c,betahat,sebetahat,v){#
  stop("method compcdf_post_t not written for this class")#
}#
#
compcdf_post_t.unimix=function(m,c,betahat,sebetahat,v){#
  k = length(m$pi)#
  n=length(betahat)#
  tmp = matrix(1,nrow=k,ncol=n)#
  tmp[m$a > c,] = 0#
  subset = m$a<=c & m$b>c # subset of components (1..k) with nontrivial cdf#
  if(sum(subset)>0){#
    pna = pt(outer(betahat,m$a[subset],FUN="-")/sebetahat, df=v)#
    pnc = pt(outer(betahat,rep(c,sum(subset)),FUN="-")/sebetahat, df=v)#
    pnb = pt(outer(betahat,m$b[subset],FUN="-")/sebetahat, df=v)#
    tmp[subset,] = t((pnc-pna)/(pnb-pna))#
  }#
  subset = (m$a == m$b) #subset of components with trivial cdf#
  tmp[subset,]= rep(m$a[subset] <= c,n)#
  tmp#
}#
#
#output posterior mean for beta for prior mixture m,#
#given observations betahat, sebetahat#
postmean_t = function(m, betahat,sebetahat,v){#
  UseMethod("postmean_t")#
}#
postmean_t.default = function(m,betahat,sebetahat,v){#
  colSums(comppostprob_t(m,betahat,sebetahat,v) * comp_postmean_t(m,betahat,sebetahat,v))#
}#
#
#output posterior mean for beta for each component of prior mixture m,#
#given observations betahat, sebetahat#
comp_postmean_t = function(m, betahat,sebetahat,v){#
  UseMethod("comp_postmean_t")#
}#
comp_postmean_t.default = function(m,betahat,sebetahat,v){#
  stop("method comp_postmean_t not written for this class")#
}#
#
#return posterior mean for each component of prior m, given observations betahat and sebetahat#
#input, m is a mixture with k components#
#betahat, sebetahat are n vectors#
#output is a k by n matrix#
#note that with uniform prior, posterior is truncated student.t, so#
#this is computed using formula for mean of truncated student.t#
comp_postmean_t.unimix = function(m,betahat,sebetahat,v){#
  alpha = outer(-betahat, m$a, FUN="+")/sebetahat#
  beta = outer(-betahat, m$b, FUN="+")/sebetahat#
  tmp = betahat + sebetahat*my_etrunct(alpha,beta,v)#
  ismissing = is.na(betahat) | is.na(sebetahat)#
  tmp[ismissing,]= (m$a+m$b)/2#
  t(tmp)#
}#
# the mean of a truncated student.t#
# the result is from the paper 'Moments of truncated Student-t distribution' by H.-J Kim #
#
my_etrunct= function(a,b,v){#
  A = v+a^2#
  B = v+b^2#
  F_a = pt(a,df=v)#
  F_b = pt(b,df=v)#
  G = gamma((v-1)/2)*v^(v/2)/(2*(F_b-F_a)*gamma(v/2)*gamma(1/2))#
  tmp = ifelse(a==b,a,G*(A^(-(v-1)/2)-B^(-(v-1)/2)))#
  tmp#
}#
################# PostSD is not implemented for uniform mixture#
#
#output posterior sd for beta for prior mixture m,#
#given observations betahat, sebetahat#
postsd_t = function(m, betahat,sebetahat,v){#
  UseMethod("postsd_t")#
}#
postsd_t.default = function(m,betahat,sebetahat,v){#
  sqrt(postmean2_t(m,betahat,sebetahat,v)-postmean_t(m,betahat,sebetahat,v)^2)#
}#
postmean2_t = function(m, betahat,sebetahat,v){#
  UseMethod("postmean2_t")#
}#
postmean2_t.default = function(m,betahat,sebetahat,v){#
  colSums(comppostprob_t(m,betahat,sebetahat,v) * comp_postmean2_t(m,betahat,sebetahat,v))#
}#
#
comp_postmean2_t = function(m, betahat,sebetahat,v){#
  UseMethod("comp_postmean2_t")#
}#
comp_postmean2_t.default = function(m,betahat,sebetahat,v){#
  comp_postsd_t(m,betahat,sebetahat,v)^2 + comp_postmean_t(m,betahat,sebetahat,v)^2#
}#
comp_postsd_t = function(m, betahat,sebetahat,v){#
  UseMethod("comp_postsd_t")#
}#
comp_postsd_t.default = function(m,betahat,sebetahat,v){#
  stop("method comp_postsd not written for this class")#
}#
#not yet implemented!#
#just returns 0s for now#
comp_postsd_t.unimix = function(m,betahat,sebetahat,v){#
  print("Warning: Posterior SDs not yet implemented for uniform components")#
  k= ncomp(m)#
  n=length(betahat)#
  return(matrix(NA,nrow=k,ncol=n)) #
}
test.ash=ash(betahat,sebetahat,method="shrink")
test.ash
attributes(test.ash)
GENERIC FUNCTIONS #############################
# find matrix of densities at y, for each component of the mixture#
# INPUT y is an n-vector#
# OUTPUT k by n matrix of densities#
compdens = function(x,y,log=FALSE){#
  UseMethod("compdens")#
}#
compdens.default = function(x,y,log=FALSE){#
  stop(paste("Invalid class", class(m), "for first argument in",  match.call()))  #
}#
#
#standard deviations#
comp_sd = function(m){#
  UseMethod("comp_sd")#
}#
comp_sd.default = function(m){#
  stop("method comp_sd not written for this class")#
}#
#
#second moments#
comp_mean2 = function(m){#
  UseMethod("comp_mean2")#
}#
comp_mean2.default = function(m){#
  comp_sd(m)^2 + comp_mean(m)^2#
}#
#return the overall mean of the mixture#
mixmean = function(m){#
  UseMethod("mixmean")#
}#
mixmean.default = function(m){#
  sum(m$pi * comp_mean(m))#
}#
#
#return the overall second moment of the mixture#
mixmean2 = function(m){#
  UseMethod("mixmean2")#
}#
mixmean2.default = function(m){#
  sum(m$pi * comp_mean2(m))#
}#
#
#return the overall sd of the mixture#
mixsd = function(m){#
  UseMethod("mixsd")#
}#
mixsd.default = function(m){#
  sqrt(mixmean2(m)-mixmean(m)^2)#
}#
#
#means#
comp_mean = function(m){#
  UseMethod("comp_mean")#
}#
comp_mean.default = function(m){#
  stop("method comp_mean not written for this class")#
}#
#
#number of components#
ncomp = function(m){#
  UseMethod("ncomp")#
}#
ncomp.default = function(m){#
  return(length(m$pi))#
}#
#
#return mixture proportions, a generic function#
mixprop = function(m){#
  UseMethod("mixprop")#
}#
mixprop.default = function(m){#
  m$pi#
}#
#
#' @title mixcdf#
#'#
#' @description Returns cdf for a mixture (generic function)#
#' #
#' @details None#
#' #
#' @param x a mixture (eg of type normalmix or unimix)#
#' @param y locations at which cdf to be computed#
#' @param lower.tail: boolean indicating whether to report lower tail#
#' #
#' @return an object of class normalmix#
#' #
#' @export#
#' #
#' @examples mixcdf(normalmix(c(0.5,0.5),c(0,0),c(1,2)),seq(-4,4,length=100))#
#' #
mixcdf = function(x,y,lower.tail=TRUE){#
  UseMethod("mixcdf")#
}#
#' @title mixcdf.default#
#' @export#
#' #
mixcdf.default = function(x,y,lower.tail=TRUE){#
  x$pi %*% comp_cdf(x,y,lower.tail)#
}#
#
#find cdf for each component, a generic function#
comp_cdf = function(x,y,lower.tail=TRUE){#
  UseMethod("comp_cdf")#
}#
comp_cdf.default = function(x,y,lower.tail=TRUE){#
  stop("comp_cdf not implemented for this class")#
}#
#find density at y, a generic function#
dens = function(x,y){#
  UseMethod("dens")#
}#
dens.default = function(x,y){#
  return (x$pi %*% compdens(x, y))#
}#
#
#find log likelihood of data in x (a vector) for mixture in m#
loglik = function(m,x){#
  UseMethod("loglik")#
}#
loglik.default = function(m,x){#
  sum(log(dens(m,x)))#
}#
#
#find log likelihood of data in betahat, when #
#the mixture m is convolved with a normal with sd betahatsd#
#betahatsd is an n vector#
#betahat is an n vector#
#' @title loglik_conv#
#' #
#' @export#
#' #
loglik_conv = function(m,betahat,betahatsd,FUN="+"){#
  UseMethod("loglik_conv")#
}#
#' @title loglik_conv.default#
#' #
#' @export#
#' #
loglik_conv.default = function(m,betahat,betahatsd,FUN="+"){#
  sum(log(dens_conv(m,betahat,betahatsd,FUN)))#
}#
#
#compute the density of the components of the mixture m#
#when convoluted with a normal with standard deviation s#
#the density is evaluated at x#
#x and s are n-vectors#
#m is a mixture with k components#
#output is a k by n matrix of densities#
compdens_conv = function(m, x, s, FUN="+"){#
  UseMethod("compdens_conv")#
}#
compdens_conv.default = function(m,x, s,FUN="+"){#
  stop(paste("Invalid class", class(m), "for first argument in",  match.call()))  #
}#
#
#compute density of mixture m convoluted with normal of sd (s)#
#at locations x#
#m is a mixture#
#x is an n vector#
#s is an n vector or integer#
dens_conv = function(m,x,s,FUN="+"){#
  UseMethod("dens_conv")#
}#
dens_conv.default = function(m,x,s,FUN="+"){#
  colSums(m$pi * compdens_conv(m,x,s,FUN))#
}#
#
#compute the posterior prob that each observation#
#came from each component of the mixture m#
#output a k by n vector of probabilities#
#computed by weighting the component densities by pi#
#and then normalizing#
comppostprob=function(m,x,s){#
 UseMethod("comppostprob") #
}#
comppostprob.default = function(m,x,s){#
  tmp= (t(m$pi * compdens_conv(m,x,s))/dens_conv(m,x,s))#
  ismissing = (is.na(x) | is.na(s))#
  tmp[ismissing,]=m$pi#
  t(tmp)#
}#
# evaluate cdf of posterior distribution of beta at c#
# m is the prior on beta, a mixture#
# c is location of evaluation#
# assumption is betahat | beta \sim N(beta,sebetahat)#
# m is a mixture with k components#
# c a scalar#
# betahat, sebetahat are n vectors #
# output is a k by n matrix#
compcdf_post=function(m,c,betahat,sebetahat){#
  UseMethod("compcdf_post")#
}#
compcdf_post.default=function(m,c,betahat,sebetahat){#
  stop("method compcdf_post not written for this class")#
}#
cdf_post = function(m,c,betahat,sebetahat){#
  UseMethod("cdf_post")#
}#
cdf_post.default=function(m,c,betahat,sebetahat){#
  colSums(comppostprob(m,betahat,sebetahat)*compcdf_post(m,c,betahat,sebetahat))#
}#
#
#output posterior mean for beta for prior mixture m,#
#given observations betahat, sebetahat#
postmean = function(m, betahat,sebetahat){#
  UseMethod("postmean")#
}#
postmean.default = function(m,betahat,sebetahat){#
  colSums(comppostprob(m,betahat,sebetahat) * comp_postmean(m,betahat,sebetahat))#
}#
#
#output posterior mean-squared value for beta for prior mixture m,#
#given observations betahat, sebetahat#
postmean2 = function(m, betahat,sebetahat){#
  UseMethod("postmean2")#
}#
postmean2.default = function(m,betahat,sebetahat){#
  colSums(comppostprob(m,betahat,sebetahat) * comp_postmean2(m,betahat,sebetahat))#
}#
#
#output posterior sd for beta for prior mixture m,#
#given observations betahat, sebetahat#
postsd = function(m, betahat,sebetahat){#
  UseMethod("postsd")#
}#
postsd.default = function(m,betahat,sebetahat){#
  sqrt(postmean2(m,betahat,sebetahat)-postmean(m,betahat,sebetahat)^2)#
}#
#
#output posterior mean-squared value for beta for prior mixture m,#
#given observations betahat, sebetahat#
comp_postmean2 = function(m, betahat,sebetahat){#
  UseMethod("comp_postmean2")#
}#
comp_postmean2.default = function(m,betahat,sebetahat){#
  comp_postsd(m,betahat,sebetahat)^2 + comp_postmean(m,betahat,sebetahat)^2#
}#
#output posterior mean for beta for each component of prior mixture m,#
#given observations betahat, sebetahat#
comp_postmean = function(m, betahat,sebetahat){#
  UseMethod("comp_postmean")#
}#
comp_postmean.default = function(m,betahat,sebetahat){#
  stop("method comp_postmean not written for this class")#
}#
#
#output posterior sd for beta for each component of prior mixture m,#
#given observations betahat, sebetahat#
comp_postsd = function(m, betahat,sebetahat){#
  UseMethod("comp_postsd")#
}#
comp_postsd.default = function(m,betahat,sebetahat){#
  stop("method comp_postsd not written for this class")#
}#
#
#find nice limits of mixture m for plotting#
min_lim = function(m){#
  UseMethod("min_lim")#
}#
min_lim.default=function(m){#
  -5#
}#
#
max_lim = function(m){#
  UseMethod("max_lim")#
}#
max_lim.default=function(m){#
  5#
}#
#plot density of mixture#
plot_dens = function(m,npoints=100,...){#
  UseMethod("plot_dens")#
}#
plot_dens.default = function(m,npoints=100,...){#
  x = seq(min_lim(m),max_lim(m),length=npoints)#
  plot(x,dens(m,x),type="l",xlab="density",ylab="x",...)#
}#
#
plot_post_cdf = function(m,betahat,sebetahat,npoints=100,...){#
  UseMethod("plot_post_cdf")#
}#
plot_post_cdf.default = function(m,betahat,sebetahat,npoints=100,...){#
  x = seq(min_lim(m),max_lim(m),length=npoints)#
  x_cdf = vapply(x,cdf_post,FUN.VALUE=betahat,m=m,betahat=betahat,sebetahat=sebetahat)#
  plot(x,x_cdf,type="l",xlab="x",ylab="cdf",...)#
 # for(i in 2:nrow(x_cdf)){#
 #   lines(x,x_cdf[i,],col=i)#
 # }#
}#
#
############################### METHODS FOR normalmix class ############################
#
#' @title Constructor for normalmix class#
#'#
#' @description Creates an object of class normalmix (finite mixture of univariate normals)#
#' #
#' @details None#
#' #
#' @param pi vector of mixture proportions#
#' @param mean vector of means#
#' @param sd: vector of standard deviations#
#' #
#' @return an object of class normalmix#
#' #
#' @export#
#' #
#' @examples normalmix(c(0.5,0.5),c(0,0),c(1,2))#
#' #
normalmix = function(pi,mean,sd){#
  structure(data.frame(pi,mean,sd),class="normalmix")#
}#
#
comp_sd.normalmix = function(m){#
  m$sd#
}#
#
comp_mean.normalmix = function(m){#
  m$mean#
}#
#
compdens.normalmix = function(x,y,log=FALSE){#
  k=ncomp(x)#
  n=length(y)#
  d = matrix(rep(y,rep(k,n)),nrow=k)#
  return(matrix(dnorm(d, x$mean, x$sd, log),nrow=k))  #
}#
#
#density of convolution of each component of a normal mixture with N(0,s^2) at x#
# x an n-vector at which density is to be evaluated#
#return a k by n matrix#
#Note that convolution of two normals is normal, so it works that way#
compdens_conv.normalmix = function(m, x, s,FUN="+"){#
  if(length(s)==1){s=rep(s,length(x))}#
  sdmat = sqrt(outer(s^2,m$sd^2,FUN)) #n by k matrix of standard deviations of convolutions#
  return(t(dnorm(outer(x,m$mean,FUN="-")/sdmat)/sdmat))#
}#
comp_cdf.normalmix = function(x,y,lower.tail=TRUE){#
  vapply(y,pnorm,x$mean,x$mean,x$sd,lower.tail)#
}#
#
#c is a scalar#
#m a mixture with k components#
#betahat a vector of n observations#
#sebetahat an n vector of standard errors#
#return a k by n matrix of the posterior cdf#
compcdf_post.normalmix=function(m,c,betahat,sebetahat){#
  k = length(m$pi)#
  n=length(betahat)#
  #compute posterior standard deviation (s1) and posterior mean (m1)#
  s1 = sqrt(outer(sebetahat^2,m$sd^2,FUN="*")/outer(sebetahat^2,m$sd^2,FUN="+"))#
  ismissing = (is.na(betahat) | is.na(sebetahat))#
  s1[ismissing,]=m$sd#
  m1 = t(comp_postmean(m,betahat,sebetahat))#
  t(pnorm(c,mean=m1,sd=s1))#
}#
#
#return posterior mean for each component of prior m, given observations betahat and sebetahat#
#input, m is a mixture with k components#
#betahat, sebetahat are n vectors#
#output is a k by n matrix#
comp_postmean.normalmix = function(m,betahat,sebetahat){#
  tmp=(outer(sebetahat^2,m$mean, FUN="*") + outer(betahat,m$sd^2, FUN="*"))/outer(sebetahat^2,m$sd^2,FUN="+")#
  ismissing = (is.na(betahat) | is.na(sebetahat))#
  tmp[ismissing,]=m$mean #return prior mean when missing data#
  t(tmp)#
}#
#
#return posterior standard deviation for each component of prior m, given observations betahat and sebetahat#
#input, m is a mixture with k components#
#betahat, sebetahat are n vectors#
#output is a k by n matrix#
comp_postsd.normalmix = function(m,betahat,sebetahat){#
  t(sqrt(outer(sebetahat^2,m$sd^2,FUN="*")/outer(sebetahat^2,m$sd^2,FUN="+")))#
}#
############################### METHODS FOR unimix class ############################
#
#constructor; pi, a and b are vectors; kth component is Uniform(a[k],b[k])#
unimix = function(pi,a,b){#
  structure(data.frame(pi,a,b),class="unimix")#
}#
#
comp_cdf.unimix = function(m,y,lower.tail=TRUE){#
  vapply(y,punif,m$a,min=m$a,max=m$b,lower.tail)#
}#
#
comp_sd.unimix = function(m){#
  (m$b-m$a)/sqrt(12)#
}#
#
comp_mean.unimix = function(m){#
  (m$a+m$b)/2#
}#
compdens.unimix = function(x,y,log=FALSE){#
  k=ncomp(x)#
  n=length(y)#
  d = matrix(rep(y,rep(k,n)),nrow=k)#
  return(matrix(dunif(d, x$a, x$b, log),nrow=k))  #
}#
#
#density of convolution of each component of a unif mixture with N(0,s) at x#
# x an n-vector#
#return a k by n matrix#
compdens_conv.unimix = function(m, x, s, FUN="+"){#
  if(FUN!="+") stop("Error; compdens_conv not implemented for uniform with FUN!=+")#
  compdens= t(pnorm(outer(x,m$a,FUN="-")/s)-pnorm(outer(x,m$b,FUN="-")/s))/(m$b-m$a)#
  compdens[m$a==m$b,]=t(dnorm(outer(x,m$a,FUN="-")/s)/s)[m$a==m$b,]#
  return(compdens)#
}#
#c is a scalar#
#m a mixture with k components#
#betahat a vector of n observations#
#sebetahat an n vector of standard errors#
#return a k by n matrix of the posterior cdf#
compcdf_post.unimix=function(m,c,betahat,sebetahat){#
  k = length(m$pi)#
  n=length(betahat)#
  tmp = matrix(1,nrow=k,ncol=n)#
  tmp[m$a > c,] = 0#
  subset = m$a<=c & m$b>c # subset of components (1..k) with nontrivial cdf#
  if(sum(subset)>0){#
    pna = pnorm(outer(betahat,m$a[subset],FUN="-")/sebetahat)#
    pnc = pnorm(outer(betahat,rep(c,sum(subset)),FUN="-")/sebetahat)#
    pnb = pnorm(outer(betahat,m$b[subset],FUN="-")/sebetahat)#
    tmp[subset,] = t((pnc-pna)/(pnb-pna))#
  }#
  subset = (m$a == m$b) #subset of components with trivial cdf#
  tmp[subset,]= rep(m$a[subset] <= c,n)#
  tmp#
}#
#
my_etruncnorm= function(a,b,mean=0,sd=1){#
  alpha = (a-mean)/sd#
  beta =  (b-mean)/sd#
 #Flip the onese where both are positive, as the computations are more stable#
  #when both negative#
  flip = (alpha>0 & beta>0)#
  flip[is.na(flip)]=FALSE #deal with NAs#
  alpha[flip]= -alpha[flip]#
  beta[flip]=-beta[flip]#
  tmp= (-1)^flip * (mean+sd*etruncnorm(alpha,beta,0,1))#
  max_alphabeta = ifelse(alpha<beta, beta,alpha)#
  max_ab = ifelse(alpha<beta,b,a)#
  toobig = max_alphabeta<(-30)#
  toobig[is.na(toobig)]=FALSE #
  tmp[toobig] = max_ab[toobig]#
  tmp#
}#
#return posterior mean for each component of prior m, given observations betahat and sebetahat#
#input, m is a mixture with k components#
#betahat, sebetahat are n vectors#
#output is a k by n matrix#
#note that with uniform prior, posterior is truncated normal, so#
#this is computed using formula for mean of truncated normal #
comp_postmean.unimix = function(m,betahat,sebetahat){#
#   k= ncomp(m)#
#   n=length(betahat)#
#   a = matrix(m$a,nrow=n,ncol=k,byrow=TRUE)#
#   b = matrix(m$b,nrow=n,ncol=k,byrow=TRUE)#
#   matrix(etruncnorm(a,b,betahat,sebetahat),nrow=k,byrow=TRUE)#
  #note: etruncnorm is more stable for a and b negative than positive#
  #so maybe use this, and standardize to make the whole more stable.#
  alpha = outer(-betahat, m$a,FUN="+")/sebetahat#
  beta = outer(-betahat, m$b, FUN="+")/sebetahat#
  tmp = betahat + sebetahat*my_etruncnorm(alpha,beta,0,1)#
  ismissing = is.na(betahat) | is.na(sebetahat)#
  tmp[ismissing,]= (m$a+m$b)/2#
  t(tmp)#
#   t(#
#     betahat + sebetahat* #
#       exp(dnorm(alpha,log=TRUE)- pnorm(alpha,log=TRUE))#
#    * #
#       (-expm1(dnorm(beta,log=TRUE)-dnorm(alpha,log=TRUE)))#
#     /#
#       (expm1(pnorm(beta,log=TRUE)-pnorm(alpha,log=TRUE)))#
#   )#
}#
#
#not yet implemented!#
#just returns 0s for now#
comp_postsd.unimix = function(m,betahat,sebetahat){#
  print("Warning: Posterior SDs not yet implemented for uniform components")#
  k= ncomp(m)#
  n=length(betahat)#
  return(matrix(NA,nrow=k,ncol=n)) #
}#
######################################################################################
########################## functions for student.t likelihood ########################
#compute the density of the components of the mixture m#
#when convoluted with a scaled (se) student.t with df v #
compdens_conv_t = function(m, x, s, v, FUN="+"){#
  UseMethod("compdens_conv_t")#
}#
compdens_conv_t.default = function(m,x, s,v,FUN="+"){#
  stop(paste("Invalid class", class(m), "for first argument in",  match.call()))#
}#
#density of convolution of each component of a normal mixture with s*t(v) at x#
# x an n-vector at which density is to be evaluated#
#return a k by n matrix#
#Note that convolution of two normals is normal, so it works that way#
compdens_conv_t.normalmix = function(m, x, s, v,FUN="+"){#
  stop("Error: normal mixture for student-t likelihood is not yet implemented")#
}#
#density of convolution of each component of a unif mixture with s*t(v) at x#
# x an n-vector#
#return a k by n matrix#
compdens_conv_t.unimix = function(m, x, s, v , FUN="+"){#
  if(FUN!="+") stop("Error; compdens_conv_t not implemented for uniform with FUN!=+")#
  compdens= t(pt(outer(x,m$a,FUN="-")/s,df=v)-pt(outer(x,m$b,FUN="-")/s,df=v))/(m$b-m$a)#
  compdens[m$a==m$b,]=t(dt(outer(x,m$a,FUN="-")/s,df=v)/s)[m$a==m$b,]#
  return(compdens)#
}#
#
#compute density of mixture m convoluted with student t with df v#
#at locations x#
#m is a mixture#
#x is an n vector#
#s is an n vector or integer#
dens_conv_t = function(m,x,s,v,FUN="+"){#
  UseMethod("dens_conv_t")#
}#
dens_conv_t.default = function(m,x,s,v,FUN="+"){#
  colSums(m$pi * compdens_conv_t(m,x,s,v,FUN))#
}#
#
#compute the posterior prob that each observation#
#came from each component of the mixture m#
#output a k by n vector of probabilities#
#computed by weighting the component densities by pi#
#and then normalizing#
comppostprob_t=function(m,x,s,v){#
  UseMethod("comppostprob_t") #
}#
comppostprob_t.default = function(m,x,s,v){#
  tmp= (t(m$pi * compdens_conv_t(m,x,s,v))/dens_conv_t(m,x,s,v))#
  ismissing = (is.na(x) | is.na(s))#
  tmp[ismissing,]=m$pi#
  t(tmp)#
}#
cdf_post_t = function(m,c,betahat,sebetahat,v){#
  UseMethod("cdf_post_t")#
}#
cdf_post_t.default=function(m,c,betahat,sebetahat,v){#
  colSums(comppostprob_t(m,betahat,sebetahat,v)*compcdf_post_t(m,c,betahat,sebetahat,v))#
}#
#
compcdf_post_t=function(m,c,betahat,sebetahat,v){#
  UseMethod("compcdf_post_t")#
}#
compcdf_post_t.default=function(m,c,betahat,sebetahat,v){#
  stop("method compcdf_post_t not written for this class")#
}#
#
compcdf_post_t.unimix=function(m,c,betahat,sebetahat,v){#
  k = length(m$pi)#
  n=length(betahat)#
  tmp = matrix(1,nrow=k,ncol=n)#
  tmp[m$a > c,] = 0#
  subset = m$a<=c & m$b>c # subset of components (1..k) with nontrivial cdf#
  if(sum(subset)>0){#
    pna = pt(outer(betahat,m$a[subset],FUN="-")/sebetahat, df=v)#
    pnc = pt(outer(betahat,rep(c,sum(subset)),FUN="-")/sebetahat, df=v)#
    pnb = pt(outer(betahat,m$b[subset],FUN="-")/sebetahat, df=v)#
    tmp[subset,] = t((pnc-pna)/(pnb-pna))#
  }#
  subset = (m$a == m$b) #subset of components with trivial cdf#
  tmp[subset,]= rep(m$a[subset] <= c,n)#
  tmp#
}#
#
#output posterior mean for beta for prior mixture m,#
#given observations betahat, sebetahat#
postmean_t = function(m, betahat,sebetahat,v){#
  UseMethod("postmean_t")#
}#
postmean_t.default = function(m,betahat,sebetahat,v){#
  colSums(comppostprob_t(m,betahat,sebetahat,v) * comp_postmean_t(m,betahat,sebetahat,v))#
}#
#
#output posterior mean for beta for each component of prior mixture m,#
#given observations betahat, sebetahat#
comp_postmean_t = function(m, betahat,sebetahat,v){#
  UseMethod("comp_postmean_t")#
}#
comp_postmean_t.default = function(m,betahat,sebetahat,v){#
  stop("method comp_postmean_t not written for this class")#
}#
#
#return posterior mean for each component of prior m, given observations betahat and sebetahat#
#input, m is a mixture with k components#
#betahat, sebetahat are n vectors#
#output is a k by n matrix#
#note that with uniform prior, posterior is truncated student.t, so#
#this is computed using formula for mean of truncated student.t#
comp_postmean_t.unimix = function(m,betahat,sebetahat,v){#
  alpha = outer(-betahat, m$a, FUN="+")/sebetahat#
  beta = outer(-betahat, m$b, FUN="+")/sebetahat#
  tmp = betahat + sebetahat*my_etrunct(alpha,beta,v)#
  ismissing = is.na(betahat) | is.na(sebetahat)#
  tmp[ismissing,]= (m$a+m$b)/2#
  t(tmp)#
}#
# the mean of a truncated student.t#
# the result is from the paper 'Moments of truncated Student-t distribution' by H.-J Kim #
#
my_etrunct= function(a,b,v){#
  A = v+a^2#
  B = v+b^2#
  F_a = pt(a,df=v)#
  F_b = pt(b,df=v)#
  G = gamma((v-1)/2)*v^(v/2)/(2*(F_b-F_a)*gamma(v/2)*gamma(1/2))#
  tmp = ifelse(a==b,a,G*(A^(-(v-1)/2)-B^(-(v-1)/2)))#
  tmp#
}#
################# PostSD is not implemented for uniform mixture#
#
#output posterior sd for beta for prior mixture m,#
#given observations betahat, sebetahat#
postsd_t = function(m, betahat,sebetahat,v){#
  UseMethod("postsd_t")#
}#
postsd_t.default = function(m,betahat,sebetahat,v){#
  sqrt(postmean2_t(m,betahat,sebetahat,v)-postmean_t(m,betahat,sebetahat,v)^2)#
}#
postmean2_t = function(m, betahat,sebetahat,v){#
  UseMethod("postmean2_t")#
}#
postmean2_t.default = function(m,betahat,sebetahat,v){#
  colSums(comppostprob_t(m,betahat,sebetahat,v) * comp_postmean2_t(m,betahat,sebetahat,v))#
}#
#
comp_postmean2_t = function(m, betahat,sebetahat,v){#
  UseMethod("comp_postmean2_t")#
}#
comp_postmean2_t.default = function(m,betahat,sebetahat,v){#
  comp_postsd_t(m,betahat,sebetahat,v)^2 + comp_postmean_t(m,betahat,sebetahat,v)^2#
}#
comp_postsd_t = function(m, betahat,sebetahat,v){#
  UseMethod("comp_postsd_t")#
}#
comp_postsd_t.default = function(m,betahat,sebetahat,v){#
  stop("method comp_postsd not written for this class")#
}#
#not yet implemented!#
#just returns 0s for now#
comp_postsd_t.unimix = function(m,betahat,sebetahat,v){#
  print("Warning: Posterior SDs not yet implemented for uniform components")#
  k= ncomp(m)#
  n=length(betahat)#
  return(matrix(NA,nrow=k,ncol=n)) #
}
nonzeromean.fit=nonzeromeanEM(betahat, sebetahat, mixsd, maxiter=maxiter)
mixcompdist = c("normal","uniform","halfuniform"),#
               lambda1=1,lambda2=0,nullcheck=TRUE,df=NULL,randomstart=FALSE,#
               nullweight=10,nonzeromean=FALSE, #
               pointmass = FALSE, #
               onlylogLR = FALSE, #
               prior=c("uniform","nullbiased"), #
               mixsd=NULL, VB=FALSE,gridmult=sqrt(2),#
               minimaloutput=FALSE,#
               multiseqoutput=FALSE,#
               g=NULL,#
               maxiter = 5000,#
               cxx=FALSE
mixsd = autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)
nonzeromean.fit=nonzeromeanEM(betahat, sebetahat, mixsd, maxiter=maxiter)
nonzeromean.fit
nonzeromean.fit$nonzeromean
teest.ash
test.ash
attributes(test.ash)
test.ash$fit
test.ash$fitted.g
attributes(test.ash$fit)
attributes(test.ash)
test.ash$call
test.ash$data
library(SQUAREM)
' @useDynLib ashr#
#todo#
##
#' @title Main Adaptive SHrinkage function#
#'#
#' @description Takes vectors of estimates (betahat) and their standard errors (sebetahat), and applies#
#' shrinkage to them, using Empirical Bayes methods, to compute shrunk estimates for beta.#
#'#
#' @details See readme for more details#
#' #
#' @param betahat, a p vector of estimates #
#' @param sebetahat, a p vector of corresponding standard errors#
#' @param method: specifies how ash is to be run. Can be "shrinkage" (if main aim is shrinkage) or "fdr" (if main aim is to assess fdr or fsr)#
#' This is simply a convenient way to specify certain combinations of parameters: "shrinkage" sets pointmass=FALSE and prior="uniform";#
#' "fdr" sets pointmass=TRUE and prior="nullbiased".#
#' @param mixcompdist: distribution of components in mixture ("normal", "uniform" or "halfuniform")#
#'#
#' @param lambda1: multiplicative "inflation factor" for standard errors (like Genomic Control)#
#' @param lambda2: additive "inflation factor" for standard errors (like Genomic Control)#
#' @param nullcheck: whether to check that any fitted model exceeds the "null" likelihood#
#' in which all weight is on the first component#
#' @param df: appropriate degrees of freedom for (t) distribution of betahat/sebetahat#
#' @param randomstart: bool, indicating whether to initialize EM randomly. If FALSE, then initializes to prior mean (for EM algorithm) or prior (for VBEM)#
#' @param nullweight: scalar, the weight put on the prior of null under "fdr" method#
#' @param nonzeromean: bool, indicating whether to use a nonzero mean unimodal mixture(defaults to "FALSE")#
#' @param pointmass: bool, indicating whether to use a point mass at zero as one of components for a mixture distribution#
#' @param onlylogLR: bool, indicating whether to use this function to get logLR. Skip posterior prob, posterior mean, lfdr...#
#' @param prior: string, or numeric vector indicating Dirichlet prior on mixture proportions (defaults to "uniform", or 1,1...,1; also can be "nullbiased" 1,1/k-1,...,1/k-1 to put more weight on first component)#
#' @param mixsd: vector of sds for underlying mixture components #
#' @param VB: whether to use Variational Bayes to estimate mixture proportions (instead of EM to find MAP estimate)#
#' @param gridmult: the multiplier by which the default grid values for mixsd differ by one another. (Smaller values produce finer grids)#
#' @param minimal_output: if TRUE, just outputs the fitted g and the lfsr (useful for very big data sets where memory is an issue) #
#' @param g: the prior distribution for beta (usually estimated from the data; this is used primarily in simulated data to do computations with the "true" g)#
#' @param maxiter: maximum number of iterations of the EM algorithm#
#' @param cxx: flag to indicate whether to use the c++ (Rcpp) version#
#' #
#'#
#' @return a list with elements fitted.g is fitted mixture#
#' logLR : logP(D|mle(pi)) - logP(D|null)#
#' #
#' @export#
#' #
#' @examples #
#' beta = c(rep(0,100),rnorm(100))#
#' sebetahat = abs(rnorm(200,0,1))#
#' betahat = rnorm(200,beta,sebetahat)#
#' beta.ash = ash(betahat, sebetahat)#
#' summary(beta.ash)#
#' plot(betahat,beta.ash$PosteriorMean,xlim=c(-4,4),ylim=c(-4,4))#
#' #
#' #
#Things to do:#
# check sampling routine#
# check number of iterations#
ash = function(betahat,sebetahat,method = c("shrink","fdr"), #
               mixcompdist = c("normal","uniform","halfuniform"),#
               lambda1=1,lambda2=0,nullcheck=TRUE,df=NULL,randomstart=FALSE,#
               nullweight=10,nonzeromean=FALSE, #
               pointmass = FALSE, #
               onlylogLR = FALSE, #
               prior=c("uniform","nullbiased"), #
               mixsd=NULL, VB=FALSE,gridmult=sqrt(2),#
               minimaloutput=FALSE,#
               multiseqoutput=FALSE,#
               g=NULL,#
               maxiter = 5000,#
               cxx=FALSE){#
  #method provides a convenient interface to set a particular combinations of parameters for prior an#
  #If method is supplied, use it to set up specific values for these parameters; provide warning if values#
  #are also specified by user#
  #If method is not supplied use the user-supplied values (or defaults if user does not specify them)#
  if(!missing(method)){#
    method = match.arg(method) #
    if(method=="shrink"){#
      if(missing(prior)){#
        prior = "uniform"#
      } else {#
        warning("Specification of prior overrides default for method shrink")#
      }#
      if(missing(pointmass)){#
        pointmass=FALSE#
      } else {#
        warning("Specification of pointmass overrides default for method shrink")#
      }#
    }#
    if(method=="fdr"){#
      if(missing(prior)){#
        prior = "nullbiased"#
      } else {#
        warning("Specification of prior overrides default for method fdr")#
      }#
      if(missing(pointmass)){#
        pointmass=TRUE#
      } else {#
        warning("Specification of pointmass overrides default for method fdr")#
      }#
    }  #
  }#
  if(gridmult<=1&multiseqoutput!=TRUE)#
    stop("gridmult must be > 1")#
  mixcompdist = match.arg(mixcompdist)#
  # if(mixcompdist=="uniform" & pointmass==TRUE){#
  #    stop("point mass not yet implemented for uniform or half-uniform")#
  #  }#
  #  if(mixcompdist=="halfuniform" & pointmass==TRUE){#
  #    stop("point mass not yet implemented for uniform or half-uniform")#
  #  }#
  if(!is.numeric(prior)){#
    prior = match.arg(prior)#
  }  #
  if(length(sebetahat)==1){#
    sebetahat = rep(sebetahat,length(betahat))#
  }#
  if(length(sebetahat) != length(betahat)){#
    stop("Error: sebetahat must have length 1, or same length as betahat")#
  }#
  completeobs = (!is.na(betahat) & !is.na(sebetahat))#
  n=sum(completeobs)#
  if(n==0){#
    if(onlylogLR){#
      return(list(pi=NULL, logLR = 0))#
    }#
    else{#
      stop("Error: all input values are missing")#
    }#
  }  #
  if(!is.null(g)){#
    maxiter = 1 # if g is specified, don't iterate the EM#
    prior = rep(1,ncomp(g)) #prior is not actually used if g specified, but required to make sure EM doesn't produce warning#
    null.comp=1 #null.comp also not used, but required #
  } else {#
    if(is.null(mixsd)){#
      mixsd = autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)#
    }#
    if(pointmass){#
      mixsd = c(0,mixsd)#
    }#
    null.comp = which.min(mixsd) #which component is the "null"#
    k = length(mixsd)#
	if(nonzeromean & is.null(df)){#
		nonzeromean.fit=nonzeromeanEM(betahat[completeobs], sebetahat[completeobs], mixsd, maxiter=maxiter)#
		betahat[completeobs]= betahat[completeobs] - nonzeromean.fit$nonzeromean#
	}#
	elseif(nonzeromean & !is.null(df) ){#
		stop("Error: Nonzero mean only implemented for df=NULL")#
	}#
    if(!is.numeric(prior)){#
      if(prior=="nullbiased"){ # set up prior to favour "null"#
        prior = rep(1,k)#
        prior[null.comp] = nullweight #prior 10-1 in favour of null by default#
      }else if(prior=="uniform"){#
        prior = rep(1,k)#
      }#
    }#
    if(length(prior)!=k | !is.numeric(prior)){#
      stop("invalid prior specification")#
    }#
    if(randomstart){#
      pi = rgamma(k,1,1)#
    } else {#
      if(k<n){#
        pi=rep(1,k)/n #default initialization strongly favours null; puts weight 1/n on everything except null#
        pi[null.comp] = (n-k+1)/n #the motivation is data can quickly drive away from null, but tend to drive only slowly toward null.#
      } else {#
        pi=rep(1,k)/k#
      }#
    }#
    pi=normalize(pi)#
    if(!is.element(mixcompdist,c("normal","uniform","halfuniform"))) stop("Error: invalid type of mixcompdist")#
    if(mixcompdist=="normal") g=normalmix(pi,rep(0,k),mixsd)#
    if(mixcompdist=="uniform") g=unimix(pi,-mixsd,mixsd)#
    if(mixcompdist=="halfuniform"){#
      g = unimix(c(pi,pi)/2,c(-mixsd,rep(0,k)),c(rep(0,k),mixsd))#
      prior = rep(prior, 2)#
      pi = rep(pi, 2)#
    }#
  }#
  pi.fit=EMest(betahat[completeobs],lambda1*sebetahat[completeobs]+lambda2,g,prior,null.comp=null.comp,nullcheck=nullcheck,VB=VB,maxiter = maxiter, cxx=cxx, df=df)  #
  if (!onlylogLR){#
      n=length(betahat)#
      if (!multiseqoutput){#
          ZeroProb = rep(0,length=n)#
          NegativeProb = rep(0,length=n)#
      }#
      if (!minimaloutput){#
          PosteriorMean = rep(0,length=n)#
          PosteriorSD = rep(0,length=n)#
      }#
      if (!multiseqoutput){#
            ZeroProb[completeobs] = colSums(comppostprob(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)[comp_sd(pi.fit$g)==0,,drop=FALSE])     #
              NegativeProb[completeobs] = cdf_post(pi.fit$g, 0, betahat[completeobs],sebetahat[completeobs],df) - ZeroProb[completeobs]#
          }#
          if (!minimaloutput){#
              PosteriorMean[completeobs] = postmean(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
              PosteriorSD[completeobs] = postsd(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
      }#
                                        #FOR MISSING OBSERVATIONS, USE THE PRIOR INSTEAD OF THE POSTERIOR#
      if (!multiseqoutput){#
          ZeroProb[!completeobs] = sum(mixprop(pi.fit$g)[comp_sd(pi.fit$g)==0])#
          NegativeProb[!completeobs] = mixcdf(pi.fit$g,0)#
          lfsr = compute_lfsr(NegativeProb,ZeroProb)#
      }#
      if (!minimaloutput){#
          PosteriorMean[!completeobs] = mixmean(pi.fit$g)#
          PosteriorSD[!completeobs] = mixsd(pi.fit$g)#
      }#
      if (!minimaloutput & !multiseqoutput){#
          PositiveProb = 1- NegativeProb-ZeroProb#
          lfsra = compute_lfsra(PositiveProb,NegativeProb,ZeroProb) #
          lfdr = ZeroProb#
          qvalue = qval.from.lfdr(lfdr)#
      }#
  }#
  if (!minimaloutput)#
      logLR = tail(pi.fit$loglik,1) - pi.fit$null.loglik#
#
  if(nonzeromean & is.null(df)){#
      #Adding back the nonzero mean#
      betahat[completeobs]= betahat[completeobs]+nonzeromean.fit$nonzeromean#
      pi.fit$g$mean =nonzeromean.fit$nonzeromean#
      PosteriorMean= PosteriorMean + nonzeromean.fit$nonzeromean      #
  }	    #
  if (onlylogLR)#
      return(list(fitted.g=pi.fit$g, logLR = logLR, df=df))#
  else if (minimaloutput)#
      return(list(fitted.g = pi.fit$g, lfsr = lfsr, fit = pi.fit, df=df))#
  else if (multiseqoutput)#
      return(list(fitted.g = pi.fit$g, logLR = logLR, PosteriorMean = PosteriorMean, PosteriorSD = PosteriorSD, call= match.call(), df=df))#
  else{#
      result = list(fitted.g = pi.fit$g, logLR = logLR, PosteriorMean = PosteriorMean, PosteriorSD = PosteriorSD, PositiveProb = PositiveProb, NegativeProb = NegativeProb, ZeroProb = ZeroProb, lfsr = lfsr,lfsra = lfsra, lfdr = lfdr, qvalue = qvalue, fit = pi.fit, lambda1 = lambda1, lambda2 = lambda2, call = match.call(), data = list(betahat = betahat, sebetahat=sebetahat), df=df)#
      class(result) = "ash"#
      return(result)#
  }#
}#
  #if(nsamp>0){#
  #  sample = posterior_sample(post,nsamp)#
  #}#
#
# #' @title Faster version of function ash#
# #'#
# #' @description This function has similar functionality as ash, but only returns some of the outputs.#
# #'#
# #' @param betahat, a p vector of estimates#
# #' @param sebetahat, a p vector of corresponding standard errors#
# #' @param nullcheck: whether to check that any fitted model exceeds the "null" likelihood in which all weight is on the first component#
# #' @param randomstart: bool, indicating whether to initialize EM randomly. If FALSE, then initializes to prior mean (for EM algorithm) or prior (for VBEM)#
# #' @param pointmass: bool, indicating whether to use a point mass at zero as one of components for a mixture distribution#
# #' @param onlylogLR: bool, indicating whether to use this function to get logLR. Skip posterior prob, posterior mean, lfdr...#
# #' @param prior: string, or numeric vector indicating Dirichlet prior on mixture proportions (defaults to "uniform", or 1,1...,1; also can be "nullbiased" 1,1/k-1,...,1/k-1 to put more weight on first component)#
# #' @param mixsd: vector of sds for underlying mixture components#
# #' @param VB: whether to use Variational Bayes to estimate mixture proportions (instead of EM to find MAP estimate)#
# #' @param gridmult: the multiplier by which the default grid values for mixsd differ by one another. (Smaller values produce finer grids)#
# #' @param g: the prior distribution for beta (usually estimated from the data; this is used primarily in simulated data to do computations with the "true" g)#
# #' @param cxx: flag to indicate whether to use the c++ (Rcpp) version#
# #'#
# #' @return a list with elements fitted.g is fitted mixture#
# #' logLR : logP(D|mle(pi)) - logP(D|null)#
# #'#
# #' @export#
# fast.ash = function(betahat,sebetahat, #
#                     nullcheck=TRUE,randomstart=FALSE, #
#                     pointmass = TRUE,    #
#                     prior=c("nullbiased","uniform"), #
#                     mixsd=NULL, VB=FALSE,gridmult=4,#
#                     g=NULL, cxx=TRUE,#
#                     onlylogLR = FALSE,df=NULL){#
#   #
#   if(onlylogLR){#
#     pointmass <- TRUE  #
#   }#
#   #
#   #If method is supplied, use it to set up defaults; provide warning if these default values#
#   #are also specified by user#
#   if(!is.numeric(prior)){#
#     prior = match.arg(prior)#
#   }#
#   #
#   if(length(sebetahat)==1){#
#     sebetahat = rep(sebetahat,length(betahat))#
#   }#
#   if(length(sebetahat) != length(betahat)){#
#     stop("Error: sebetahat must have length 1, or same length as betahat")#
#   }#
#   #
#   completeobs = (!is.na(betahat) & !is.na(sebetahat))#
#   if(sum(completeobs)==0){#
#     if(onlylogLR){#
#       return(list(pi=NULL, logLR = 0))#
#     }else{#
#       stop("Error: all input values are missing")#
#     }#
#   }  #
#   #
#   if(is.null(mixsd)){#
#     mixsd= autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)#
#   }#
#   if(pointmass){#
#     mixsd = c(0,mixsd)#
#   }#
#   #
#   k=length(mixsd)  #
#   null.comp = which.min(mixsd) #which component is the "null"#
#   #
#   if(!is.numeric(prior)){#
#     if(prior=="nullbiased"){ # set up prior to favour "null"#
#       prior = rep(1,k)#
#       prior[null.comp] = 10 #prior 10-1 in favour of null#
#     }else if(prior=="uniform"){#
#       prior = rep(1,k)#
#     }#
#   }#
#   #
#   if(length(prior)!=k | !is.numeric(prior)){#
#     stop("invalid prior specification")#
#   }#
#   #
#   if(missing(g)){#
#     pi = prior^2 #default is to initialize pi at prior (mean)#
#     if(randomstart){pi=rgamma(k,1,1)}#
#     pi=normalize(pi)#
#     g=normalmix(pi,rep(0,k),mixsd)#
#     maxiter = 5000#
#   } else {#
#     maxiter = 1; # if g is specified, don't iterate the EM #
#   }#
#   #
#   pi.fit=EMest(betahat[completeobs],sebetahat[completeobs],g,prior,null.comp=null.comp,nullcheck=nullcheck,VB=VB,maxiter = maxiter, cxx=cxx, df=df)  #
#   #
#   if(onlylogLR){#
#     logLR = tail(pi.fit$loglik,1) - pi.fit$null.loglik#
#     return(list(pi=pi.fit$pi, logLR = logLR))#
#   }else{#
#     #
#     n=length(betahat)#
#     PosteriorMean = rep(0,length=n)#
#     PosteriorSD=rep(0,length=n)#
#     #
#     if(is.null(df)){#
#       PosteriorMean[completeobs] = postmean(pi.fit$g,betahat[completeobs],sebetahat[completeobs])#
#       PosteriorSD[completeobs] =postsd(pi.fit$g,betahat[completeobs],sebetahat[completeobs]) #
#     }#
#     else{#
#       PosteriorMean[completeobs] = postmean_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
#       PosteriorSD[completeobs] =postsd_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
#     }#
#     #FOR MISSING OBSERVATIONS, USE THE PRIOR INSTEAD OF THE POSTERIOR#
#     PosteriorMean[!completeobs] = mixmean(pi.fit$g)#
#     PosteriorSD[!completeobs] =mixsd(pi.fit$g)  #
#     #
#     result = list(fitted.g=pi.fit$g,PosteriorMean = PosteriorMean,PosteriorSD=PosteriorSD,call=match.call(),data=list(betahat = betahat, sebetahat=sebetahat))#
#     return(result)#
#   }#
#   #if(nsamp>0){#
#   #  sample = posterior_sample(post,nsamp)#
#   #}#
# }#
compute_lfsr = function(NegativeProb,ZeroProb){#
  ifelse(NegativeProb> 0.5*(1-ZeroProb),1-NegativeProb,NegativeProb+ZeroProb)#
}#
#
compute_lfsra = function(PositiveProb, NegativeProb,ZeroProb){#
  ifelse(PositiveProb<NegativeProb,2*PositiveProb+ZeroProb,2*NegativeProb+ZeroProb)  #
}  #
#
#' @title Estimate unimodal nonzero mean of a mixture model by EM algorithm#
#'#
#' @description Given the data, standard error of the data and standard deviations of the Gaussian mixture model, estimate the mean of a unimodal Gaussian mixture by an EM algorithm.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates unimodal mean \eqn{\mu} by EM algorithm. Uses the SQUAREM package to accelerate convergence of EM. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' #
#' @param betahat, a p vector of estimates #
#' @param sebetahat, a p vector of corresponding standard errors#
#' @param mixsd: vector of sds for underlying mixture components #
#' @param pi.init, the initial value of \eqn{\pi} to use. If not specified defaults to (1/k,...,1/k).#
#' @param tol, the tolerance for convergence of log-likelihood.#
#' @param maxiter the maximum number of iterations performed#
#' #
#' @return A list, including the estimates (\eqn{\mu}) and (\eqn{\pi}), the log likelihood for each iteration (NQ)#
#' and a flag to indicate convergence#
#'  #
#' @export#
#' #
#' #
nonzeromeanEM = function(betahat, sebetahat, mixsd, pi.init=NULL,tol=1e-7,maxiter=5000){#
  if(is.null(pi.init)){#
    pi.init = rep(1/length(mixsd),length(mixsd))# Use as starting point for pi#
  }#
  mupi=c(mean(betahat),pi.init)#
  res=squarem(par=mupi,fixptfn=nonzeromeanEMfixpoint,objfn=nonzeromeanEMobj,betahat=betahat,sebetahat=sebetahat,mixsd=mixsd,control=list(maxiter=maxiter,tol=tol))#
  return(list(nonzeromean=res$par[1],pi=res$par[-1],NQ=-res$value.objfn,niter = res$iter, converged=res$convergence,post=res$par))#
}#
#
nonzeromeanEMfixpoint = function(mupi,betahat,sebetahat,mixsd){#
	#omegamatrix=matrix(NA,nrow=length(betahat),ncol=length(mixsd))#
	mu=mupi[1]#
	pimean=mupi[-1]#
	sdmat = sqrt(outer(sebetahat ^2,mixsd^2,"+")) #
	xmat=matrix(rep(betahat,length(mixsd)),ncol=length(mixsd))#
	omegamatrix=t(t(dnorm(xmat,mean=mu,sd=sdmat))*pimean)#
	omegamatrix=omegamatrix /rowSums(omegamatrix)#
	pinew=normalize(colSums(omegamatrix))#
	munew=sum(omegamatrix*xmat/(sdmat^2))/sum(omegamatrix/(sdmat^2))#
	mupi=c(munew,pinew)#
	return(mupi)#
}#
#
nonzeromeanEMobj = function(mupi,betahat,sebetahat,mixsd){#
	mu=mupi[1]#
	pimean=mupi[-1]#
	sdmat = sqrt(outer(sebetahat ^2,mixsd^2,"+")) #
	xmat=matrix(rep(betahat,length(mixsd)),ncol=length(mixsd))#
	omegamatrix=t(t(dnorm(xmat,mean=mu,sd=sdmat))*pimean)#
	omegamatrix=omegamatrix /rowSums(omegamatrix)#
	NegativeQ=-sum(omegamatrix*dnorm(xmat,mean=mu,sd=sdmat,log=TRUE))#
	return(NegativeQ)#
}#
#' @title Estimate posterior distribution on mixture proportions of a mixture model by a Variational Bayes EM algorithm#
#'#
#' @description Given the individual component likelihoods for a mixture model, estimates the posterior on #
#' the mixture proportions by an VBEM algorithm. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates posterior on mixture proportions \eqn{\pi} by Variational Bayes, #
#' with a Dirichlet prior on \eqn{\pi}. #
#' Algorithm adapted from Bishop (2009), Pattern Recognition and Machine Learning, Chapter 10.#
#' #
#' @param matrix_lik: a n by k matrix with (j,k)th element equal to \eqn{f_k(x_j)}.#
#' @param prior: a k vector of the parameters of the Dirichlet prior on \eqn{\pi}. Recommended to be rep(1,k)#
#' @param post.init: the initial value of the posterior parameters. If not specified defaults to the prior parameters.#
#' @param tol: the tolerance for convergence of log-likelihood bound.#
#' @param maxiter: the maximum number of iterations performed#
#' #
#' @return A list, whose components include point estimates (pihat), #
#' the parameters of the fitted posterior on \eqn{\pi} (pipost),#
#' the bound on the log likelihood for each iteration (B)#
#' and a flag to indicate convergence (converged).#
#'  #
#' @export#
#' #
mixVBEM = function(matrix_lik, prior, pi.init = NULL,tol=1e-7, maxiter=5000){#
  k=ncol(matrix_lik)#
  if(is.null(pi.init)){#
    pi.init = rep(1,k)# Use as starting point for pi#
  } #
  res = squarem(par=pi.init,fixptfn=VBfixpoint, objfn=VBnegpenloglik,matrix_lik=matrix_lik, prior=prior, control=list(maxiter=maxiter,tol=tol))#
  return(list(pihat = res$par/sum(res$par), B=res$value.objfn, niter = res$iter, converged=res$convergence,post=res$par))#
}#
VBfixpoint = function(pipost, matrix_lik, prior){  #
  n=nrow(matrix_lik)#
  k=ncol(matrix_lik)#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost*matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  pipostnew = colSums(classprob) + prior#
  return(pipostnew)#
}#
#
VBnegpenloglik=function(pipost,matrix_lik,prior){#
  return(-VBpenloglik(pipost,matrix_lik,prior))#
}#
#
VBpenloglik = function(pipost, matrix_lik, prior){#
  n=nrow(matrix_lik)#
  k=ncol(matrix_lik)#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost*matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  B= sum(classprob*log(avgpipost*matrix_lik),na.rm=TRUE) - diriKL(prior,pipost) - sum(classprob*log(classprob)) #
  return(B)#
}#
#' @title Estimate mixture proportions of a mixture model by EM algorithm#
#'#
#' @description Given the individual component likelihoods for a mixture model, estimates the mixture proportions by an EM algorithm.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates mixture proportions \eqn{\pi} by maximum likelihood, or by maximum a posteriori (MAP) estimation for a Dirichlet prior on $\pi$ #
#' (if a prior is specified).  Uses the SQUAREM package to accelerate convergence of EM. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' #
#' @param matrix_lik, a n by k matrix with (j,k)th element equal to \eqn{f_k(x_j)}.#
#' @param prior, a k vector of the parameters of the Dirichlet prior on \eqn{\pi}. Recommended to be rep(1,k)#
#' @param pi.init, the initial value of \eqn{\pi} to use. If not specified defaults to (1/k,...,1/k).#
#' @param tol, the tolerance for convergence of log-likelihood.#
#' @param maxiter the maximum number of iterations performed#
#' #
#' @return A list, including the estimates (pihat), the log likelihood for each interation (B)#
#' and a flag to indicate convergence#
#'  #
#' @export#
#' #
#' #
mixEM = function(matrix_lik, prior, pi.init = NULL,tol=1e-7, maxiter=5000){#
  if(is.null(pi.init)){#
    pi.init = rep(1/k,k)# Use as starting point for pi#
  } #
  res = squarem(par=pi.init,fixptfn=fixpoint, objfn=negpenloglik,matrix_lik=matrix_lik, prior=prior, control=list(maxiter=maxiter,tol=tol))#
  return(list(pihat = normalize(pmax(0,res$par)), B=res$value.objfn, #
              niter = res$iter, converged=res$convergence))#
}#
#
# helper functions used by mixEM#
normalize = function(x){return(x/sum(x))}#
#
fixpoint = function(pi, matrix_lik, prior){  #
  pi = normalize(pmax(0,pi)) #avoid occasional problems with negative pis due to rounding#
  m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
  m.rowsum = rowSums(m)#
  classprob = m/m.rowsum #an n by k matrix#
  pinew = normalize(colSums(classprob) + prior - 1)#
  return(pinew)#
}#
#
negpenloglik = function(pi,matrix_lik,prior){return(-penloglik(pi,matrix_lik,prior))}#
#
penloglik = function(pi, matrix_lik, prior){#
  pi = normalize(pmax(0,pi))#
  m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
  m.rowsum = rowSums(m)#
  loglik = sum(log(m.rowsum))#
  subset = (prior != 1.0)#
  priordens = sum((prior-1)[subset]*log(pi[subset]))#
  return(loglik+priordens)#
}#
#
#The kth element of this vector is the derivative #
#of the loglik for $\pi=(\pi_0,...,1-\pi_0,...)$ with respect to $\pi_0$ at $\pi_0=1$.#
gradient = function(matrix_lik){#
  n = nrow(matrix_lik)#
  grad = n - colSums(matrix_lik/matrix_lik[,1]) #
  return(grad)#
}#
#
# mixEM = function(matrix_lik, prior, pi.init = NULL,tol=0.0001, maxiter=5000){#
#   n=nrow(matrix_lik)#
#   k=ncol(matrix_lik)#
#   B = rep(0,maxiter)#
#   pi = pi.init#
#   if(is.null(pi.init)){#
#     pi = rep(1/k,k)# Use as starting point for pi#
#   } #
#   pi = ifelse(pi<1e-5,1e-5,pi) #set any estimates that are too small to be just very small#
#   pi = normalize(pi)#
#   #
#   loglik = rep(0,maxiter)#
#   priordens= rep(0,maxiter)#
#   m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
#   m.rowsum = rowSums(m)#
#   loglik[1] = sum(log(m.rowsum))#
#   priordens[1] = sum((prior-1)*log(pi)) #
#   classprob = m/m.rowsum #an n by k matrix#
#   i=1#
#   if(maxiter >= 2){#
#     for(i in 2:maxiter){  #
#       pi = colSums(classprob) + prior-1#
#       pi = ifelse(pi<1e-5,1e-5,pi) #set any estimates that are less than zero, which can happen with prior<1, to 0#
#       pi = normalize(pi)#
#         #
#       #Now re-estimate pi#
#       m  = t(pi * t(matrix_lik)) #
#       m.rowsum = rowSums(m)#
#       loglik[i] = sum(log(m.rowsum))#
#       priordens[i] = sum((prior-1)*log(pi)) #
#       classprob = m/m.rowsum#
#     #
#     #
#       if(abs(loglik[i]+priordens[i]-loglik[i-1]-priordens[i-1])<tol) break;#
#     }#
#   }#
#   converged=(abs(loglik[i]+priordens[i]-loglik[i-1]-priordens[i-1])<tol)#
#   if(!converged){#
#       warning("EM algorithm in function mixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
#   }#
#   return(list(pihat = pi, B=loglik[1:i], #
#               niter = i, converged=converged))#
# }#
#estimate mixture proportions of sigmaa by EM algorithm#
#prior gives the parameter of a Dirichlet prior on pi#
#(prior is used to encourage results towards smallest value of sigma when#
#likelihood is flat)#
#nullcheck indicates whether to check whether the loglike exceeds the null#
#(may not want to use if prior is used)#
#VB provides an approach to estimate the approximate posterior distribution#
#of mixture proportions of sigmaa by variational Bayes method#
#(use Dirichlet prior and approximate Dirichlet posterior)#
#if cxx TRUE use cpp version of R function mixEM#
EMest = function(betahat,sebetahat,g,prior,null.comp=1,nullcheck=TRUE,VB=FALSE, maxiter=5000, cxx=TRUE, df=NULL){ #
  pi.init = g$pi#
  k=ncomp(g)#
  n = length(betahat)#
  tol = min(0.1/n,1e-5) # set convergence criteria to be more stringent for larger samples#
  if(is.null(df)){#
    matrix_lik = t(compdens_conv(g,betahat,sebetahat))#
  }#
  else{#
    matrix_lik = t(compdens_conv_t(g,betahat,sebetahat,df))#
  }#
  #checks whether the gradient at pi0=1 is positive (suggesting that this is a fixed point)#
  #if(nullcheck){#
  #  if(all(gradient(matrix_lik)>=0)){#
  #    pi.init=rep(0,k)#
  #    pi.init[null.comp]=1 #this will make pi.init=(1,0,0...,0) which is a fixed point of the EM#
  #  }#
  #}#
  if(VB==TRUE){#
    EMfit=mixVBEM(matrix_lik,prior,maxiter=maxiter)}#
  else{#
    if (cxx==TRUE){#
      EMfit = cxxMixEM(matrix_lik,prior,pi.init,1e-5, maxiter) #currently use different convergence criteria for cxx version #
      if(!EMfit$converged){#
        warning("EM algorithm in function cxxMixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
      }#
    }#
    else{#
      EMfit = mixEM(matrix_lik,prior,pi.init,tol, maxiter)#
      if(!EMfit$converged & !(maxiter==1)){#
        warning("EM algorithm in function mixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
      }#
    }#
  }#
  pi = EMfit$pihat     #
  penloglik = EMfit$B #
  converged = EMfit$converged#
  niter = EMfit$niter#
  loglik.final =  penloglik(pi,matrix_lik,1) #compute penloglik without penalty#
  null.loglik = sum(log(matrix_lik[,null.comp]))  #
  if(nullcheck==TRUE & VB==FALSE){ #null check doesn't work with VB yet#
    pinull = rep(0,k)#
    pinull[null.comp]=1#
    null.penloglik = penloglik(pinull,matrix_lik,prior)#
    final.penloglik = penloglik(pi,matrix_lik,prior)#
    if(null.penloglik > final.penloglik){ #check whether exceeded "null" likelihood where everything is null#
      pi=pinull#
      loglik.final=penloglik(pi,matrix_lik,1)#
    }#
  }#
  g$pi=pi#
  return(list(loglik=loglik.final,null.loglik=null.loglik,#
              matrix_lik=matrix_lik,converged=converged,g=g))#
}#
#' @title Compute Posterior#
#'#
#' @description Return the posterior on beta given a prior (g) that is a mixture of normals (class normalmix) #
#' and observation betahat \sim N(beta,sebetahat)#
#'#
#' @details This can be used to obt#
#'#
#' @param g: a normalmix with components indicating the prior; works only if g has means 0#
#' @param betahat (n vector of observations) #
#' @param sebetahat (n vector of standard errors/deviations of observations)#
#' #
#' @return A list, (pi1,mu1,sigma1) whose components are each k by n matrices#
#' where k is number of mixture components in g, n is number of observations in betahat#
#' #
#' @export#
#' #
#' #
posterior_dist = function(g,betahat,sebetahat){#
  if(class(g)!="normalmix"){#
    stop("Error: posterior_dist implemented only for g of class normalmix")#
  }#
  pi0 = g$pi#
  mu0 = g$mean#
  sigma0 = g$sd  #
  k= length(pi0)#
  n= length(betahat)#
  if(!all.equal(g$mean,rep(0,k))) stop("Error: posterior_dist currently only implemented for zero-centered priors")#
  pi1 = pi0 * t(matrix_dens(betahat,sebetahat,sigma0))#
  pi1 = apply(pi1, 2, normalize) #pi1 is now an k by n matrix#
  #make k by n matrix versions of sigma0^2 and sebetahat^2#
  # and mu0 and betahat#
  s0m2 = matrix(sigma0^2,nrow=k,ncol=n,byrow=FALSE)#
  sebm2 = matrix(sebetahat^2,nrow=k,ncol=n, byrow=TRUE)#
  mu0m = matrix(mu0,nrow=k,ncol=n,byrow=FALSE)#
  bhatm = matrix(betahat,nrow=k,ncol=n,byrow=TRUE)#
  sigma1 = (s0m2*sebm2/(s0m2 + sebm2))^(0.5)  #
  w = sebm2/(s0m2 + sebm2)#
  mu1 = w*mu0m + (1-w)*bhatm#
  #WHERE DATA ARE MISSING, SET POSTERIOR = PRIOR#
  ismiss = (is.na(betahat) | is.na(sebetahat)) #
  pi1[,ismiss] = pi0#
  mu1[,ismiss] = mu0#
  sigma1[,ismiss] = sigma0#
  return(list(pi=pi1,mu=mu1,sigma=sigma1))#
}#
#
#return matrix of densities of observations (betahat) #
# assuming betahat_j \sim N(0, sebetahat_j^2 + sigmaavec_k^2)#
#normalized by maximum of each column#
#INPUT#
#betahat is n vector, #
#sebetahat is n vector, #
#sigmaavec is k vector#
#return is n by k matrix of the normal likelihoods, #
# with (j,k)th element the density of N(betahat_j; mean=0, var = sebetahat_j^2 + sigmaavec_k^2)#
#normalized to have maximum 1 in each column#
matrix_dens = function(betahat, sebetahat, sigmaavec){#
  k = length(sigmaavec)#
  n = length(betahat)#
  ldens = dnorm(betahat,0,sqrt(outer(sebetahat^2,sigmaavec^2,FUN="+")),log=TRUE)#
  maxldens = apply(ldens, 1, max)#
  ldens = ldens - maxldens#
  return(exp(ldens))#
}#
#
#return the "effective" estimate#
#that is the effect size betanew whose z score betanew/se#
#would give the same p value as betahat/se compared to a t with df#
effective.effect=function(betahat,se,df){#
  p = pt(betahat/se,df)#
  qnorm(p,sd=se)#
}#
#' @title Function to compute q values from local false discovery rates#
#'#
#' @description Computes q values from a vector of local fdr estimates#
#'#
#' @details The q value for a given lfdr is an estimate of the (tail) False Discovery Rate #
#' for all findings with a smaller lfdr, and is found by the average of the lfdr for#
#' all more significant findings. See Storey (2003), Annals of Statistics, for definition of q value.  #
#' #
#' #
#' @param lfdr, a vector of local fdr estimates#
#'#
#' @return vector of q values#
#' #
#' @export#
qval.from.lfdr = function(lfdr){#
  o = order(lfdr)#
  qvalue=rep(NA,length(lfdr))#
  qvalue[o] = (cumsum(sort(lfdr))/(1:sum(!is.na(lfdr))))#
  return(qvalue)#
}#
#
# try to select a default range for the sigmaa values#
# that should be used, based on the values of betahat and sebetahat#
# mult is the multiplier by which the sds differ across the grid#
autoselect.mixsd = function(betahat,sebetahat,mult){#
  sigmaamin = min(sebetahat)/10 #so that the minimum is small compared with measurement precision#
  if(all(betahat^2<sebetahat^2)){#
    sigmaamax = 8*sigmaamin #to deal with the occassional odd case where this could happen; 8 is arbitrary#
  } else {#
    sigmaamax = 2*sqrt(max(betahat^2-sebetahat^2)) #this computes a rough largest value you'd want to use, based on idea that sigmaamax^2 + sebetahat^2 should be at least betahat^2   #
  }#
  if(mult==0){#
    return(c(0,sigmaamax/2))#
  }else{#
    npoint = ceiling(log2(sigmaamax/sigmaamin)/log2(mult))#
    return(mult^((-npoint):0) * sigmaamax)#
  }#
}#
#return the KL-divergence between 2 dirichlet distributions#
#p,q are the vectors of dirichlet parameters of same lengths#
diriKL = function(p,q){#
  p.sum = sum(p)#
  q.sum = sum(q)#
  k = length(q)#
  KL = lgamma(q.sum)-lgamma(p.sum)+sum((q-p)*(digamma(q)-digamma(rep(q.sum,k))))+sum(lgamma(p)-lgamma(q))#
  return(KL)#
}#
#
#helper function for VBEM#
VB.update = function(matrix_lik, pipost){#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost * matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  B = sum(classprob*log(avgpipost*matrix_lik),na.rm=TRUE) - diriKL(prior,pipost) #negative free energy#
  return(list(classprob=classprob,B=B))#
}
ash = function(betahat,sebetahat,method = c("shrink","fdr"), #
               mixcompdist = c("normal","uniform","halfuniform"),#
               lambda1=1,lambda2=0,nullcheck=TRUE,df=NULL,randomstart=FALSE,#
               nullweight=10,nonzeromean=FALSE, #
               pointmass = FALSE, #
               onlylogLR = FALSE, #
               prior=c("uniform","nullbiased"), #
               mixsd=NULL, VB=FALSE,gridmult=sqrt(2),#
               minimaloutput=FALSE,#
               multiseqoutput=FALSE,#
               g=NULL,#
               maxiter = 5000,#
               cxx=FALSE){#
  #method provides a convenient interface to set a particular combinations of parameters for prior an#
  #If method is supplied, use it to set up specific values for these parameters; provide warning if values#
  #are also specified by user#
  #If method is not supplied use the user-supplied values (or defaults if user does not specify them)#
  if(!missing(method)){#
    method = match.arg(method) #
    if(method=="shrink"){#
      if(missing(prior)){#
        prior = "uniform"#
      } else {#
        warning("Specification of prior overrides default for method shrink")#
      }#
      if(missing(pointmass)){#
        pointmass=FALSE#
      } else {#
        warning("Specification of pointmass overrides default for method shrink")#
      }#
    }#
    if(method=="fdr"){#
      if(missing(prior)){#
        prior = "nullbiased"#
      } else {#
        warning("Specification of prior overrides default for method fdr")#
      }#
      if(missing(pointmass)){#
        pointmass=TRUE#
      } else {#
        warning("Specification of pointmass overrides default for method fdr")#
      }#
    }  #
  }#
  if(gridmult<=1&multiseqoutput!=TRUE)#
    stop("gridmult must be > 1")#
  mixcompdist = match.arg(mixcompdist)#
  # if(mixcompdist=="uniform" & pointmass==TRUE){#
  #    stop("point mass not yet implemented for uniform or half-uniform")#
  #  }#
  #  if(mixcompdist=="halfuniform" & pointmass==TRUE){#
  #    stop("point mass not yet implemented for uniform or half-uniform")#
  #  }#
  if(!is.numeric(prior)){#
    prior = match.arg(prior)#
  }  #
  if(length(sebetahat)==1){#
    sebetahat = rep(sebetahat,length(betahat))#
  }#
  if(length(sebetahat) != length(betahat)){#
    stop("Error: sebetahat must have length 1, or same length as betahat")#
  }#
  completeobs = (!is.na(betahat) & !is.na(sebetahat))#
  n=sum(completeobs)#
  if(n==0){#
    if(onlylogLR){#
      return(list(pi=NULL, logLR = 0))#
    }#
    else{#
      stop("Error: all input values are missing")#
    }#
  }  #
  if(!is.null(g)){#
    maxiter = 1 # if g is specified, don't iterate the EM#
    prior = rep(1,ncomp(g)) #prior is not actually used if g specified, but required to make sure EM doesn't produce warning#
    null.comp=1 #null.comp also not used, but required #
  } else {#
    if(is.null(mixsd)){#
      mixsd = autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)#
    }#
    if(pointmass){#
      mixsd = c(0,mixsd)#
    }#
    null.comp = which.min(mixsd) #which component is the "null"#
    k = length(mixsd)#
	if(nonzeromean & is.null(df)){#
		nonzeromean.fit=nonzeromeanEM(betahat[completeobs], sebetahat[completeobs], mixsd, maxiter=maxiter)#
		betahat[completeobs]= betahat[completeobs] - nonzeromean.fit$nonzeromean#
	}#
	elseif(nonzeromean & !is.null(df) ){#
		stop("Error: Nonzero mean only implemented for df=NULL")#
	}#
    if(!is.numeric(prior)){#
      if(prior=="nullbiased"){ # set up prior to favour "null"#
        prior = rep(1,k)#
        prior[null.comp] = nullweight #prior 10-1 in favour of null by default#
      }else if(prior=="uniform"){#
        prior = rep(1,k)#
      }#
    }#
    if(length(prior)!=k | !is.numeric(prior)){#
      stop("invalid prior specification")#
    }#
    if(randomstart){#
      pi = rgamma(k,1,1)#
    } else {#
      if(k<n){#
        pi=rep(1,k)/n #default initialization strongly favours null; puts weight 1/n on everything except null#
        pi[null.comp] = (n-k+1)/n #the motivation is data can quickly drive away from null, but tend to drive only slowly toward null.#
      } else {#
        pi=rep(1,k)/k#
      }#
    }#
    pi=normalize(pi)#
    if(!is.element(mixcompdist,c("normal","uniform","halfuniform"))) stop("Error: invalid type of mixcompdist")#
    if(mixcompdist=="normal") g=normalmix(pi,rep(0,k),mixsd)#
    if(mixcompdist=="uniform") g=unimix(pi,-mixsd,mixsd)#
    if(mixcompdist=="halfuniform"){#
      g = unimix(c(pi,pi)/2,c(-mixsd,rep(0,k)),c(rep(0,k),mixsd))#
      prior = rep(prior, 2)#
      pi = rep(pi, 2)#
    }#
  }#
  pi.fit=EMest(betahat[completeobs],lambda1*sebetahat[completeobs]+lambda2,g,prior,null.comp=null.comp,nullcheck=nullcheck,VB=VB,maxiter = maxiter, cxx=cxx, df=df)  #
  if (!onlylogLR){#
      n=length(betahat)#
      if (!multiseqoutput){#
          ZeroProb = rep(0,length=n)#
          NegativeProb = rep(0,length=n)#
      }#
      if (!minimaloutput){#
          PosteriorMean = rep(0,length=n)#
          PosteriorSD = rep(0,length=n)#
      }#
      if (!multiseqoutput){#
            ZeroProb[completeobs] = colSums(comppostprob(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)[comp_sd(pi.fit$g)==0,,drop=FALSE])     #
              NegativeProb[completeobs] = cdf_post(pi.fit$g, 0, betahat[completeobs],sebetahat[completeobs],df) - ZeroProb[completeobs]#
          }#
          if (!minimaloutput){#
              PosteriorMean[completeobs] = postmean(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
              PosteriorSD[completeobs] = postsd(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
      }#
                                        #FOR MISSING OBSERVATIONS, USE THE PRIOR INSTEAD OF THE POSTERIOR#
      if (!multiseqoutput){#
          ZeroProb[!completeobs] = sum(mixprop(pi.fit$g)[comp_sd(pi.fit$g)==0])#
          NegativeProb[!completeobs] = mixcdf(pi.fit$g,0)#
          lfsr = compute_lfsr(NegativeProb,ZeroProb)#
      }#
      if (!minimaloutput){#
          PosteriorMean[!completeobs] = mixmean(pi.fit$g)#
          PosteriorSD[!completeobs] = mixsd(pi.fit$g)#
      }#
      if (!minimaloutput & !multiseqoutput){#
          PositiveProb = 1- NegativeProb-ZeroProb#
          lfsra = compute_lfsra(PositiveProb,NegativeProb,ZeroProb) #
          lfdr = ZeroProb#
          qvalue = qval.from.lfdr(lfdr)#
      }#
  }#
  if (!minimaloutput)#
      logLR = tail(pi.fit$loglik,1) - pi.fit$null.loglik#
#
  if(nonzeromean & is.null(df)){#
      #Adding back the nonzero mean#
      betahat[completeobs]= betahat[completeobs]+nonzeromean.fit$nonzeromean#
      pi.fit$g$mean =nonzeromean.fit$nonzeromean#
      PosteriorMean= PosteriorMean + nonzeromean.fit$nonzeromean      #
  }	    #
  if (onlylogLR){#
      return(list(fitted.g=pi.fit$g, logLR = logLR, df=df))}#
  else if (minimaloutput){#
      return(list(fitted.g = pi.fit$g, lfsr = lfsr, fit = pi.fit, df=df))}#
  else if (multiseqoutput){#
      return(list(fitted.g = pi.fit$g, logLR = logLR, PosteriorMean = PosteriorMean, PosteriorSD = PosteriorSD, call= match.call(), df=df))}#
  else{#
      result = list(fitted.g = pi.fit$g, logLR = logLR, PosteriorMean = PosteriorMean, PosteriorSD = PosteriorSD, PositiveProb = PositiveProb, NegativeProb = NegativeProb, ZeroProb = ZeroProb, lfsr = lfsr,lfsra = lfsra, lfdr = lfdr, qvalue = qvalue, fit = pi.fit, lambda1 = lambda1, lambda2 = lambda2, call = match.call(), data = list(betahat = betahat, sebetahat=sebetahat), df=df)#
      class(result) = "ash"#
      return(result)#
  }#
}
' @useDynLib ashr#
#todo#
##
#' @title Main Adaptive SHrinkage function#
#'#
#' @description Takes vectors of estimates (betahat) and their standard errors (sebetahat), and applies#
#' shrinkage to them, using Empirical Bayes methods, to compute shrunk estimates for beta.#
#'#
#' @details See readme for more details#
#' #
#' @param betahat, a p vector of estimates #
#' @param sebetahat, a p vector of corresponding standard errors#
#' @param method: specifies how ash is to be run. Can be "shrinkage" (if main aim is shrinkage) or "fdr" (if main aim is to assess fdr or fsr)#
#' This is simply a convenient way to specify certain combinations of parameters: "shrinkage" sets pointmass=FALSE and prior="uniform";#
#' "fdr" sets pointmass=TRUE and prior="nullbiased".#
#' @param mixcompdist: distribution of components in mixture ("normal", "uniform" or "halfuniform")#
#'#
#' @param lambda1: multiplicative "inflation factor" for standard errors (like Genomic Control)#
#' @param lambda2: additive "inflation factor" for standard errors (like Genomic Control)#
#' @param nullcheck: whether to check that any fitted model exceeds the "null" likelihood#
#' in which all weight is on the first component#
#' @param df: appropriate degrees of freedom for (t) distribution of betahat/sebetahat#
#' @param randomstart: bool, indicating whether to initialize EM randomly. If FALSE, then initializes to prior mean (for EM algorithm) or prior (for VBEM)#
#' @param nullweight: scalar, the weight put on the prior of null under "fdr" method#
#' @param nonzeromean: bool, indicating whether to use a nonzero mean unimodal mixture(defaults to "FALSE")#
#' @param pointmass: bool, indicating whether to use a point mass at zero as one of components for a mixture distribution#
#' @param onlylogLR: bool, indicating whether to use this function to get logLR. Skip posterior prob, posterior mean, lfdr...#
#' @param prior: string, or numeric vector indicating Dirichlet prior on mixture proportions (defaults to "uniform", or 1,1...,1; also can be "nullbiased" 1,1/k-1,...,1/k-1 to put more weight on first component)#
#' @param mixsd: vector of sds for underlying mixture components #
#' @param VB: whether to use Variational Bayes to estimate mixture proportions (instead of EM to find MAP estimate)#
#' @param gridmult: the multiplier by which the default grid values for mixsd differ by one another. (Smaller values produce finer grids)#
#' @param minimal_output: if TRUE, just outputs the fitted g and the lfsr (useful for very big data sets where memory is an issue) #
#' @param g: the prior distribution for beta (usually estimated from the data; this is used primarily in simulated data to do computations with the "true" g)#
#' @param maxiter: maximum number of iterations of the EM algorithm#
#' @param cxx: flag to indicate whether to use the c++ (Rcpp) version#
#' #
#'#
#' @return a list with elements fitted.g is fitted mixture#
#' logLR : logP(D|mle(pi)) - logP(D|null)#
#' #
#' @export#
#' #
#' @examples #
#' beta = c(rep(0,100),rnorm(100))#
#' sebetahat = abs(rnorm(200,0,1))#
#' betahat = rnorm(200,beta,sebetahat)#
#' beta.ash = ash(betahat, sebetahat)#
#' summary(beta.ash)#
#' plot(betahat,beta.ash$PosteriorMean,xlim=c(-4,4),ylim=c(-4,4))#
#' #
#' #
#Things to do:#
# check sampling routine#
# check number of iterations#
ash = function(betahat,sebetahat,method = c("shrink","fdr"), #
               mixcompdist = c("normal","uniform","halfuniform"),#
               lambda1=1,lambda2=0,nullcheck=TRUE,df=NULL,randomstart=FALSE,#
               nullweight=10,nonzeromean=FALSE, #
               pointmass = FALSE, #
               onlylogLR = FALSE, #
               prior=c("uniform","nullbiased"), #
               mixsd=NULL, VB=FALSE,gridmult=sqrt(2),#
               minimaloutput=FALSE,#
               multiseqoutput=FALSE,#
               g=NULL,#
               maxiter = 5000,#
               cxx=FALSE){#
  #method provides a convenient interface to set a particular combinations of parameters for prior an#
  #If method is supplied, use it to set up specific values for these parameters; provide warning if values#
  #are also specified by user#
  #If method is not supplied use the user-supplied values (or defaults if user does not specify them)#
  if(!missing(method)){#
    method = match.arg(method) #
    if(method=="shrink"){#
      if(missing(prior)){#
        prior = "uniform"#
      } else {#
        warning("Specification of prior overrides default for method shrink")#
      }#
      if(missing(pointmass)){#
        pointmass=FALSE#
      } else {#
        warning("Specification of pointmass overrides default for method shrink")#
      }#
    }#
    if(method=="fdr"){#
      if(missing(prior)){#
        prior = "nullbiased"#
      } else {#
        warning("Specification of prior overrides default for method fdr")#
      }#
      if(missing(pointmass)){#
        pointmass=TRUE#
      } else {#
        warning("Specification of pointmass overrides default for method fdr")#
      }#
    }  #
  }#
  if(gridmult<=1&multiseqoutput!=TRUE)#
    stop("gridmult must be > 1")#
  mixcompdist = match.arg(mixcompdist)#
  # if(mixcompdist=="uniform" & pointmass==TRUE){#
  #    stop("point mass not yet implemented for uniform or half-uniform")#
  #  }#
  #  if(mixcompdist=="halfuniform" & pointmass==TRUE){#
  #    stop("point mass not yet implemented for uniform or half-uniform")#
  #  }#
  if(!is.numeric(prior)){#
    prior = match.arg(prior)#
  }  #
  if(length(sebetahat)==1){#
    sebetahat = rep(sebetahat,length(betahat))#
  }#
  if(length(sebetahat) != length(betahat)){#
    stop("Error: sebetahat must have length 1, or same length as betahat")#
  }#
  completeobs = (!is.na(betahat) & !is.na(sebetahat))#
  n=sum(completeobs)#
  if(n==0){#
    if(onlylogLR){#
      return(list(pi=NULL, logLR = 0))#
    }#
    else{#
      stop("Error: all input values are missing")#
    }#
  }  #
  if(!is.null(g)){#
    maxiter = 1 # if g is specified, don't iterate the EM#
    prior = rep(1,ncomp(g)) #prior is not actually used if g specified, but required to make sure EM doesn't produce warning#
    null.comp=1 #null.comp also not used, but required #
  } else {#
    if(is.null(mixsd)){#
      mixsd = autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)#
    }#
    if(pointmass){#
      mixsd = c(0,mixsd)#
    }#
    null.comp = which.min(mixsd) #which component is the "null"#
    k = length(mixsd)#
#DCX add EM for non zero mean    #
#    #
    if(!is.numeric(prior)){#
      if(prior=="nullbiased"){ # set up prior to favour "null"#
        prior = rep(1,k)#
        prior[null.comp] = nullweight #prior 10-1 in favour of null by default#
      }else if(prior=="uniform"){#
        prior = rep(1,k)#
      }#
    }#
    if(length(prior)!=k | !is.numeric(prior)){#
      stop("invalid prior specification")#
    }#
    if(randomstart){#
      pi = rgamma(k,1,1)#
    } else {#
      if(k<n){#
        pi=rep(1,k)/n #default initialization strongly favours null; puts weight 1/n on everything except null#
        pi[null.comp] = (n-k+1)/n #the motivation is data can quickly drive away from null, but tend to drive only slowly toward null.#
      } else {#
        pi=rep(1,k)/k#
      }#
    }#
    pi=normalize(pi)#
    if(!is.element(mixcompdist,c("normal","uniform","halfuniform"))) stop("Error: invalid type of mixcompdist")#
    if(mixcompdist=="normal") g=normalmix(pi,rep(0,k),mixsd)#
    if(mixcompdist=="uniform") g=unimix(pi,-mixsd,mixsd)#
    if(mixcompdist=="halfuniform"){#
      g = unimix(c(pi,pi)/2,c(-mixsd,rep(0,k)),c(rep(0,k),mixsd))#
      prior = rep(prior, 2)#
      pi = rep(pi, 2)#
    }#
  }#
  pi.fit=EMest(betahat[completeobs],lambda1*sebetahat[completeobs]+lambda2,g,prior,null.comp=null.comp,nullcheck=nullcheck,VB=VB,maxiter = maxiter, cxx=cxx, df=df)  #
  if (!onlylogLR){#
      n=length(betahat)#
      if (!multiseqoutput){#
          ZeroProb = rep(0,length=n)#
          NegativeProb = rep(0,length=n)#
      }#
      if (!minimaloutput){#
          PosteriorMean = rep(0,length=n)#
          PosteriorSD = rep(0,length=n)#
      }#
      if(is.null(df)){#
                                        #print("normal likelihood")#
          if (!multiseqoutput){  #
              ZeroProb[completeobs] = colSums(comppostprob(pi.fit$g,betahat[completeobs],sebetahat[completeobs])[comp_sd(pi.fit$g)==0,,drop=FALSE])     #
              NegativeProb[completeobs] = cdf_post(pi.fit$g, 0, betahat[completeobs],sebetahat[completeobs]) - ZeroProb[completeobs]#
          }#
          if (!minimaloutput){#
              PosteriorMean[completeobs] = postmean(pi.fit$g,betahat[completeobs],sebetahat[completeobs])#
              PosteriorSD[completeobs] = postsd(pi.fit$g,betahat[completeobs],sebetahat[completeobs])#
          }#
      }#
      else{#
                                        #print("student-t likelihood")#
          if (!multiseqoutput){#
              ZeroProb[completeobs] = colSums(comppostprob_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)[comp_sd(pi.fit$g)==0,,drop=FALSE])     #
              NegativeProb[completeobs] = cdf_post_t(pi.fit$g, 0, betahat[completeobs],sebetahat[completeobs],df) - ZeroProb[completeobs]#
          }#
          if (!minimaloutput){#
              PosteriorMean[completeobs] = postmean_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
              PosteriorSD[completeobs] = postsd_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
          }#
      }#
                                        #FOR MISSING OBSERVATIONS, USE THE PRIOR INSTEAD OF THE POSTERIOR#
      if (!multiseqoutput){#
          ZeroProb[!completeobs] = sum(mixprop(pi.fit$g)[comp_sd(pi.fit$g)==0])#
          NegativeProb[!completeobs] = mixcdf(pi.fit$g,0)#
          lfsr = compute_lfsr(NegativeProb,ZeroProb)#
      }#
      if (!minimaloutput){#
          PosteriorMean[!completeobs] = mixmean(pi.fit$g)#
          PosteriorSD[!completeobs] = mixsd(pi.fit$g)#
      }#
      if (!minimaloutput & !multiseqoutput){#
          PositiveProb = 1- NegativeProb-ZeroProb#
          lfsra = compute_lfsra(PositiveProb,NegativeProb,ZeroProb) #
          lfdr = ZeroProb#
          qvalue = qval.from.lfdr(lfdr)#
      }#
  }#
  if (!minimaloutput)#
      logLR = tail(pi.fit$loglik,1) - pi.fit$null.loglik#
  if (onlylogLR)#
      return(list(fitted.g=pi.fit$g, logLR = logLR, df=df))#
  else if (minimaloutput)#
      return(list(fitted.g = pi.fit$g, lfsr = lfsr, fit = pi.fit, df=df))#
  else if (multiseqoutput)#
      return(list(fitted.g = pi.fit$g, logLR = logLR, PosteriorMean = PosteriorMean, PosteriorSD = PosteriorSD, call= match.call(), df=df))#
  else{#
      result = list(fitted.g = pi.fit$g, logLR = logLR, PosteriorMean = PosteriorMean, PosteriorSD = PosteriorSD, PositiveProb = PositiveProb, NegativeProb = NegativeProb, ZeroProb = ZeroProb, lfsr = lfsr,lfsra = lfsra, lfdr = lfdr, qvalue = qvalue, fit = pi.fit, lambda1 = lambda1, lambda2 = lambda2, call = match.call(), data = list(betahat = betahat, sebetahat=sebetahat), df=df)#
      class(result) = "ash"#
      return(result)#
  }#
}#
  #if(nsamp>0){#
  #  sample = posterior_sample(post,nsamp)#
  #}#
#
# #' @title Faster version of function ash#
# #'#
# #' @description This function has similar functionality as ash, but only returns some of the outputs.#
# #'#
# #' @param betahat, a p vector of estimates#
# #' @param sebetahat, a p vector of corresponding standard errors#
# #' @param nullcheck: whether to check that any fitted model exceeds the "null" likelihood in which all weight is on the first component#
# #' @param randomstart: bool, indicating whether to initialize EM randomly. If FALSE, then initializes to prior mean (for EM algorithm) or prior (for VBEM)#
# #' @param pointmass: bool, indicating whether to use a point mass at zero as one of components for a mixture distribution#
# #' @param onlylogLR: bool, indicating whether to use this function to get logLR. Skip posterior prob, posterior mean, lfdr...#
# #' @param prior: string, or numeric vector indicating Dirichlet prior on mixture proportions (defaults to "uniform", or 1,1...,1; also can be "nullbiased" 1,1/k-1,...,1/k-1 to put more weight on first component)#
# #' @param mixsd: vector of sds for underlying mixture components#
# #' @param VB: whether to use Variational Bayes to estimate mixture proportions (instead of EM to find MAP estimate)#
# #' @param gridmult: the multiplier by which the default grid values for mixsd differ by one another. (Smaller values produce finer grids)#
# #' @param g: the prior distribution for beta (usually estimated from the data; this is used primarily in simulated data to do computations with the "true" g)#
# #' @param cxx: flag to indicate whether to use the c++ (Rcpp) version#
# #'#
# #' @return a list with elements fitted.g is fitted mixture#
# #' logLR : logP(D|mle(pi)) - logP(D|null)#
# #'#
# #' @export#
# fast.ash = function(betahat,sebetahat, #
#                     nullcheck=TRUE,randomstart=FALSE, #
#                     pointmass = TRUE,    #
#                     prior=c("nullbiased","uniform"), #
#                     mixsd=NULL, VB=FALSE,gridmult=4,#
#                     g=NULL, cxx=TRUE,#
#                     onlylogLR = FALSE,df=NULL){#
#   #
#   if(onlylogLR){#
#     pointmass <- TRUE  #
#   }#
#   #
#   #If method is supplied, use it to set up defaults; provide warning if these default values#
#   #are also specified by user#
#   if(!is.numeric(prior)){#
#     prior = match.arg(prior)#
#   }#
#   #
#   if(length(sebetahat)==1){#
#     sebetahat = rep(sebetahat,length(betahat))#
#   }#
#   if(length(sebetahat) != length(betahat)){#
#     stop("Error: sebetahat must have length 1, or same length as betahat")#
#   }#
#   #
#   completeobs = (!is.na(betahat) & !is.na(sebetahat))#
#   if(sum(completeobs)==0){#
#     if(onlylogLR){#
#       return(list(pi=NULL, logLR = 0))#
#     }else{#
#       stop("Error: all input values are missing")#
#     }#
#   }  #
#   #
#   if(is.null(mixsd)){#
#     mixsd= autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)#
#   }#
#   if(pointmass){#
#     mixsd = c(0,mixsd)#
#   }#
#   #
#   k=length(mixsd)  #
#   null.comp = which.min(mixsd) #which component is the "null"#
#   #
#   if(!is.numeric(prior)){#
#     if(prior=="nullbiased"){ # set up prior to favour "null"#
#       prior = rep(1,k)#
#       prior[null.comp] = 10 #prior 10-1 in favour of null#
#     }else if(prior=="uniform"){#
#       prior = rep(1,k)#
#     }#
#   }#
#   #
#   if(length(prior)!=k | !is.numeric(prior)){#
#     stop("invalid prior specification")#
#   }#
#   #
#   if(missing(g)){#
#     pi = prior^2 #default is to initialize pi at prior (mean)#
#     if(randomstart){pi=rgamma(k,1,1)}#
#     pi=normalize(pi)#
#     g=normalmix(pi,rep(0,k),mixsd)#
#     maxiter = 5000#
#   } else {#
#     maxiter = 1; # if g is specified, don't iterate the EM #
#   }#
#   #
#   pi.fit=EMest(betahat[completeobs],sebetahat[completeobs],g,prior,null.comp=null.comp,nullcheck=nullcheck,VB=VB,maxiter = maxiter, cxx=cxx, df=df)  #
#   #
#   if(onlylogLR){#
#     logLR = tail(pi.fit$loglik,1) - pi.fit$null.loglik#
#     return(list(pi=pi.fit$pi, logLR = logLR))#
#   }else{#
#     #
#     n=length(betahat)#
#     PosteriorMean = rep(0,length=n)#
#     PosteriorSD=rep(0,length=n)#
#     #
#     if(is.null(df)){#
#       PosteriorMean[completeobs] = postmean(pi.fit$g,betahat[completeobs],sebetahat[completeobs])#
#       PosteriorSD[completeobs] =postsd(pi.fit$g,betahat[completeobs],sebetahat[completeobs]) #
#     }#
#     else{#
#       PosteriorMean[completeobs] = postmean_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
#       PosteriorSD[completeobs] =postsd_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
#     }#
#     #FOR MISSING OBSERVATIONS, USE THE PRIOR INSTEAD OF THE POSTERIOR#
#     PosteriorMean[!completeobs] = mixmean(pi.fit$g)#
#     PosteriorSD[!completeobs] =mixsd(pi.fit$g)  #
#     #
#     result = list(fitted.g=pi.fit$g,PosteriorMean = PosteriorMean,PosteriorSD=PosteriorSD,call=match.call(),data=list(betahat = betahat, sebetahat=sebetahat))#
#     return(result)#
#   }#
#   #if(nsamp>0){#
#   #  sample = posterior_sample(post,nsamp)#
#   #}#
# }#
compute_lfsr = function(NegativeProb,ZeroProb){#
  ifelse(NegativeProb> 0.5*(1-ZeroProb),1-NegativeProb,NegativeProb+ZeroProb)#
}#
#
compute_lfsra = function(PositiveProb, NegativeProb,ZeroProb){#
  ifelse(PositiveProb<NegativeProb,2*PositiveProb+ZeroProb,2*NegativeProb+ZeroProb)  #
}  #
#' @title Estimate posterior distribution on mixture proportions of a mixture model by a Variational Bayes EM algorithm#
#'#
#' @description Given the individual component likelihoods for a mixture model, estimates the posterior on #
#' the mixture proportions by an VBEM algorithm. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates posterior on mixture proportions \eqn{\pi} by Variational Bayes, #
#' with a Dirichlet prior on \eqn{\pi}. #
#' Algorithm adapted from Bishop (2009), Pattern Recognition and Machine Learning, Chapter 10.#
#' #
#' @param matrix_lik: a n by k matrix with (j,k)th element equal to \eqn{f_k(x_j)}.#
#' @param prior: a k vector of the parameters of the Dirichlet prior on \eqn{\pi}. Recommended to be rep(1,k)#
#' @param post.init: the initial value of the posterior parameters. If not specified defaults to the prior parameters.#
#' @param tol: the tolerance for convergence of log-likelihood bound.#
#' @param maxiter: the maximum number of iterations performed#
#' #
#' @return A list, whose components include point estimates (pihat), #
#' the parameters of the fitted posterior on \eqn{\pi} (pipost),#
#' the bound on the log likelihood for each iteration (B)#
#' and a flag to indicate convergence (converged).#
#'  #
#' @export#
#' #
mixVBEM = function(matrix_lik, prior, pi.init = NULL,tol=1e-7, maxiter=5000){#
  k=ncol(matrix_lik)#
  if(is.null(pi.init)){#
    pi.init = rep(1,k)# Use as starting point for pi#
  } #
  res = squarem(par=pi.init,fixptfn=VBfixpoint, objfn=VBnegpenloglik,matrix_lik=matrix_lik, prior=prior, control=list(maxiter=maxiter,tol=tol))#
  return(list(pihat = res$par/sum(res$par), B=res$value.objfn, niter = res$iter, converged=res$convergence,post=res$par))#
}#
VBfixpoint = function(pipost, matrix_lik, prior){  #
  n=nrow(matrix_lik)#
  k=ncol(matrix_lik)#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost*matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  pipostnew = colSums(classprob) + prior#
  return(pipostnew)#
}#
#
VBnegpenloglik=function(pipost,matrix_lik,prior){#
  return(-VBpenloglik(pipost,matrix_lik,prior))#
}#
#
VBpenloglik = function(pipost, matrix_lik, prior){#
  n=nrow(matrix_lik)#
  k=ncol(matrix_lik)#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost*matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  B= sum(classprob*log(avgpipost*matrix_lik),na.rm=TRUE) - diriKL(prior,pipost) - sum(classprob*log(classprob)) #
  return(B)#
}#
#' @title Estimate mixture proportions of a mixture model by EM algorithm#
#'#
#' @description Given the individual component likelihoods for a mixture model, estimates the mixture proportions by an EM algorithm.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates mixture proportions \eqn{\pi} by maximum likelihood, or by maximum a posteriori (MAP) estimation for a Dirichlet prior on $\pi$ #
#' (if a prior is specified).  Uses the SQUAREM package to accelerate convergence of EM. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' #
#' @param matrix_lik, a n by k matrix with (j,k)th element equal to \eqn{f_k(x_j)}.#
#' @param prior, a k vector of the parameters of the Dirichlet prior on \eqn{\pi}. Recommended to be rep(1,k)#
#' @param pi.init, the initial value of \eqn{\pi} to use. If not specified defaults to (1/k,...,1/k).#
#' @param tol, the tolerance for convergence of log-likelihood.#
#' @param maxiter the maximum number of iterations performed#
#' #
#' @return A list, including the estimates (pihat), the log likelihood for each interation (B)#
#' and a flag to indicate convergence#
#'  #
#' @export#
#' #
#' #
mixEM = function(matrix_lik, prior, pi.init = NULL,tol=1e-7, maxiter=5000){#
  if(is.null(pi.init)){#
    pi.init = rep(1/k,k)# Use as starting point for pi#
  } #
  res = squarem(par=pi.init,fixptfn=fixpoint, objfn=negpenloglik,matrix_lik=matrix_lik, prior=prior, control=list(maxiter=maxiter,tol=tol))#
  return(list(pihat = normalize(pmax(0,res$par)), B=res$value.objfn, #
              niter = res$iter, converged=res$convergence))#
}#
#
# helper functions used by mixEM#
normalize = function(x){return(x/sum(x))}#
#
fixpoint = function(pi, matrix_lik, prior){  #
  pi = normalize(pmax(0,pi)) #avoid occasional problems with negative pis due to rounding#
  m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
  m.rowsum = rowSums(m)#
  classprob = m/m.rowsum #an n by k matrix#
  pinew = normalize(colSums(classprob) + prior - 1)#
  return(pinew)#
}#
#
negpenloglik = function(pi,matrix_lik,prior){return(-penloglik(pi,matrix_lik,prior))}#
#
penloglik = function(pi, matrix_lik, prior){#
  pi = normalize(pmax(0,pi))#
  m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
  m.rowsum = rowSums(m)#
  loglik = sum(log(m.rowsum))#
  subset = (prior != 1.0)#
  priordens = sum((prior-1)[subset]*log(pi[subset]))#
  return(loglik+priordens)#
}#
#
#The kth element of this vector is the derivative #
#of the loglik for $\pi=(\pi_0,...,1-\pi_0,...)$ with respect to $\pi_0$ at $\pi_0=1$.#
gradient = function(matrix_lik){#
  n = nrow(matrix_lik)#
  grad = n - colSums(matrix_lik/matrix_lik[,1]) #
  return(grad)#
}#
#
# mixEM = function(matrix_lik, prior, pi.init = NULL,tol=0.0001, maxiter=5000){#
#   n=nrow(matrix_lik)#
#   k=ncol(matrix_lik)#
#   B = rep(0,maxiter)#
#   pi = pi.init#
#   if(is.null(pi.init)){#
#     pi = rep(1/k,k)# Use as starting point for pi#
#   } #
#   pi = ifelse(pi<1e-5,1e-5,pi) #set any estimates that are too small to be just very small#
#   pi = normalize(pi)#
#   #
#   loglik = rep(0,maxiter)#
#   priordens= rep(0,maxiter)#
#   m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
#   m.rowsum = rowSums(m)#
#   loglik[1] = sum(log(m.rowsum))#
#   priordens[1] = sum((prior-1)*log(pi)) #
#   classprob = m/m.rowsum #an n by k matrix#
#   i=1#
#   if(maxiter >= 2){#
#     for(i in 2:maxiter){  #
#       pi = colSums(classprob) + prior-1#
#       pi = ifelse(pi<1e-5,1e-5,pi) #set any estimates that are less than zero, which can happen with prior<1, to 0#
#       pi = normalize(pi)#
#         #
#       #Now re-estimate pi#
#       m  = t(pi * t(matrix_lik)) #
#       m.rowsum = rowSums(m)#
#       loglik[i] = sum(log(m.rowsum))#
#       priordens[i] = sum((prior-1)*log(pi)) #
#       classprob = m/m.rowsum#
#     #
#     #
#       if(abs(loglik[i]+priordens[i]-loglik[i-1]-priordens[i-1])<tol) break;#
#     }#
#   }#
#   converged=(abs(loglik[i]+priordens[i]-loglik[i-1]-priordens[i-1])<tol)#
#   if(!converged){#
#       warning("EM algorithm in function mixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
#   }#
#   return(list(pihat = pi, B=loglik[1:i], #
#               niter = i, converged=converged))#
# }#
#estimate mixture proportions of sigmaa by EM algorithm#
#prior gives the parameter of a Dirichlet prior on pi#
#(prior is used to encourage results towards smallest value of sigma when#
#likelihood is flat)#
#nullcheck indicates whether to check whether the loglike exceeds the null#
#(may not want to use if prior is used)#
#VB provides an approach to estimate the approximate posterior distribution#
#of mixture proportions of sigmaa by variational Bayes method#
#(use Dirichlet prior and approximate Dirichlet posterior)#
#if cxx TRUE use cpp version of R function mixEM#
EMest = function(betahat,sebetahat,g,prior,null.comp=1,nullcheck=TRUE,VB=FALSE, maxiter=5000, cxx=TRUE, df=NULL){ #
  pi.init = g$pi#
  k=ncomp(g)#
  n = length(betahat)#
  tol = min(0.1/n,1e-5) # set convergence criteria to be more stringent for larger samples#
  if(is.null(df)){#
    matrix_lik = t(compdens_conv(g,betahat,sebetahat))#
  }#
  else{#
    matrix_lik = t(compdens_conv_t(g,betahat,sebetahat,df))#
  }#
  #checks whether the gradient at pi0=1 is positive (suggesting that this is a fixed point)#
  #if(nullcheck){#
  #  if(all(gradient(matrix_lik)>=0)){#
  #    pi.init=rep(0,k)#
  #    pi.init[null.comp]=1 #this will make pi.init=(1,0,0...,0) which is a fixed point of the EM#
  #  }#
  #}#
  if(VB==TRUE){#
    EMfit=mixVBEM(matrix_lik,prior,maxiter=maxiter)}#
  else{#
    if (cxx==TRUE){#
      EMfit = cxxMixEM(matrix_lik,prior,pi.init,1e-5, maxiter) #currently use different convergence criteria for cxx version #
      if(!EMfit$converged){#
        warning("EM algorithm in function cxxMixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
      }#
    }#
    else{#
      EMfit = mixEM(matrix_lik,prior,pi.init,tol, maxiter)#
      if(!EMfit$converged & !(maxiter==1)){#
        warning("EM algorithm in function mixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
      }#
    }#
  }#
  pi = EMfit$pihat     #
  penloglik = EMfit$B #
  converged = EMfit$converged#
  niter = EMfit$niter#
  loglik.final =  penloglik(pi,matrix_lik,1) #compute penloglik without penalty#
  null.loglik = sum(log(matrix_lik[,null.comp]))  #
  if(nullcheck==TRUE & VB==FALSE){ #null check doesn't work with VB yet#
    pinull = rep(0,k)#
    pinull[null.comp]=1#
    null.penloglik = penloglik(pinull,matrix_lik,prior)#
    final.penloglik = penloglik(pi,matrix_lik,prior)#
    if(null.penloglik > final.penloglik){ #check whether exceeded "null" likelihood where everything is null#
      pi=pinull#
      loglik.final=penloglik(pi,matrix_lik,1)#
    }#
  }#
  g$pi=pi#
  return(list(loglik=loglik.final,null.loglik=null.loglik,#
              matrix_lik=matrix_lik,converged=converged,g=g))#
}#
#' @title Compute Posterior#
#'#
#' @description Return the posterior on beta given a prior (g) that is a mixture of normals (class normalmix) #
#' and observation betahat \sim N(beta,sebetahat)#
#'#
#' @details This can be used to obt#
#'#
#' @param g: a normalmix with components indicating the prior; works only if g has means 0#
#' @param betahat (n vector of observations) #
#' @param sebetahat (n vector of standard errors/deviations of observations)#
#' #
#' @return A list, (pi1,mu1,sigma1) whose components are each k by n matrices#
#' where k is number of mixture components in g, n is number of observations in betahat#
#' #
#' @export#
#' #
#' #
posterior_dist = function(g,betahat,sebetahat){#
  if(class(g)!="normalmix"){#
    stop("Error: posterior_dist implemented only for g of class normalmix")#
  }#
  pi0 = g$pi#
  mu0 = g$mean#
  sigma0 = g$sd  #
  k= length(pi0)#
  n= length(betahat)#
  if(!all.equal(g$mean,rep(0,k))) stop("Error: posterior_dist currently only implemented for zero-centered priors")#
  pi1 = pi0 * t(matrix_dens(betahat,sebetahat,sigma0))#
  pi1 = apply(pi1, 2, normalize) #pi1 is now an k by n matrix#
  #make k by n matrix versions of sigma0^2 and sebetahat^2#
  # and mu0 and betahat#
  s0m2 = matrix(sigma0^2,nrow=k,ncol=n,byrow=FALSE)#
  sebm2 = matrix(sebetahat^2,nrow=k,ncol=n, byrow=TRUE)#
  mu0m = matrix(mu0,nrow=k,ncol=n,byrow=FALSE)#
  bhatm = matrix(betahat,nrow=k,ncol=n,byrow=TRUE)#
  sigma1 = (s0m2*sebm2/(s0m2 + sebm2))^(0.5)  #
  w = sebm2/(s0m2 + sebm2)#
  mu1 = w*mu0m + (1-w)*bhatm#
  #WHERE DATA ARE MISSING, SET POSTERIOR = PRIOR#
  ismiss = (is.na(betahat) | is.na(sebetahat)) #
  pi1[,ismiss] = pi0#
  mu1[,ismiss] = mu0#
  sigma1[,ismiss] = sigma0#
  return(list(pi=pi1,mu=mu1,sigma=sigma1))#
}#
#
#return matrix of densities of observations (betahat) #
# assuming betahat_j \sim N(0, sebetahat_j^2 + sigmaavec_k^2)#
#normalized by maximum of each column#
#INPUT#
#betahat is n vector, #
#sebetahat is n vector, #
#sigmaavec is k vector#
#return is n by k matrix of the normal likelihoods, #
# with (j,k)th element the density of N(betahat_j; mean=0, var = sebetahat_j^2 + sigmaavec_k^2)#
#normalized to have maximum 1 in each column#
matrix_dens = function(betahat, sebetahat, sigmaavec){#
  k = length(sigmaavec)#
  n = length(betahat)#
  ldens = dnorm(betahat,0,sqrt(outer(sebetahat^2,sigmaavec^2,FUN="+")),log=TRUE)#
  maxldens = apply(ldens, 1, max)#
  ldens = ldens - maxldens#
  return(exp(ldens))#
}#
#
#return the "effective" estimate#
#that is the effect size betanew whose z score betanew/se#
#would give the same p value as betahat/se compared to a t with df#
effective.effect=function(betahat,se,df){#
  p = pt(betahat/se,df)#
  qnorm(p,sd=se)#
}#
#' @title Function to compute q values from local false discovery rates#
#'#
#' @description Computes q values from a vector of local fdr estimates#
#'#
#' @details The q value for a given lfdr is an estimate of the (tail) False Discovery Rate #
#' for all findings with a smaller lfdr, and is found by the average of the lfdr for#
#' all more significant findings. See Storey (2003), Annals of Statistics, for definition of q value.  #
#' #
#' #
#' @param lfdr, a vector of local fdr estimates#
#'#
#' @return vector of q values#
#' #
#' @export#
qval.from.lfdr = function(lfdr){#
  o = order(lfdr)#
  qvalue=rep(NA,length(lfdr))#
  qvalue[o] = (cumsum(sort(lfdr))/(1:sum(!is.na(lfdr))))#
  return(qvalue)#
}#
#
# try to select a default range for the sigmaa values#
# that should be used, based on the values of betahat and sebetahat#
# mult is the multiplier by which the sds differ across the grid#
autoselect.mixsd = function(betahat,sebetahat,mult){#
  sigmaamin = min(sebetahat)/10 #so that the minimum is small compared with measurement precision#
  if(all(betahat^2<sebetahat^2)){#
    sigmaamax = 8*sigmaamin #to deal with the occassional odd case where this could happen; 8 is arbitrary#
  } else {#
    sigmaamax = 2*sqrt(max(betahat^2-sebetahat^2)) #this computes a rough largest value you'd want to use, based on idea that sigmaamax^2 + sebetahat^2 should be at least betahat^2   #
  }#
  if(mult==0){#
    return(c(0,sigmaamax/2))#
  }else{#
    npoint = ceiling(log2(sigmaamax/sigmaamin)/log2(mult))#
    return(mult^((-npoint):0) * sigmaamax)#
  }#
}#
#' @title Summary method for ash object#
#'#
#' @description Print summary of fitted ash object#
#'#
#' @details See readme for more details#
#' #
#' @export#
#' #
summary.ash=function(a){#
  print(a$fitted.g)#
  print(tail(a$fit$loglik,1),digits=10)#
  print(a$fit$converged)#
}#
#
#' @title Print method for ash object#
#'#
#' @description Print the fitted distribution of beta values in the EB hierarchical model#
#'#
#' @details None#
#' #
#' @export#
#' #
print.ash =function(a){#
  print(a$fitted.g)#
}#
#
#' @title Plot method for ash object#
#'#
#' @description Plot the density of the underlying fitted distribution#
#'#
#' @details None#
#' #
#' @export#
#' #
plot.ash = function(a,xmin,xmax,...){#
  x = seq(xmin,xmax,length=1000)#
  y = density(a,x)#
  plot(y,type="l",...)#
}#
#
#compute the predictive density of an observation#
#given the fitted ash object a and the vector se of standard errors#
#not implemented yet#
predictive=function(a,se){#
}#
#' @title Get fitted loglikelihood for ash object#
#'#
#' @description Return the log-likelihood of the data under the fitted distribution#
#'#
#' @param a the fitted ash object#
#'#
#' @details None#
#' #
#' @export#
#' #
#'#
get_loglik = function(a){#
  return(tail(a$fit$loglik,1))#
}#
#
#' @title Get pi0 estimate for ash object#
#'#
#' @description Return estimate of the null proportion, pi0#
#'#
#' @param a the fitted ash object#
#'#
#' @details Extracts the estimate of the null proportion, pi0, from the object a#
#' #
#' @export#
#' #
get_pi0 = function(a){#
  null.comp = comp_sd(a$fitted.g)==0#
  return(sum(a$fitted.g$pi[null.comp]))#
}#
#
#' @title Compute loglikelihood for data from ash fit#
#'#
#' @description Return the log-likelihood of the data betahat, with standard errors betahatsd, #
#' under the fitted distribution in the ash object. #
#' #
#'#
#' @param a the fitted ash object#
#' @param betahat the data#
#' @param betahatsd the observed standard errors#
#' @param zscores indicates whether ash object was originally fit to z scores #
#' @details None#
#' #
#' @export#
#' #
#'#
loglik.ash = function(a,betahat,betahatsd,zscores=FALSE){#
  g=a$fitted.g#
  FUN="+"#
  if(zscores==TRUE){#
    g$sd = sqrt(g$sd^2+1) #
    FUN="*"#
  }#
  return(loglik_conv(g,betahat, betahatsd,FUN))#
}#
#
#' @title Density method for ash object#
#'#
#' @description Return the density of the underlying fitted distribution#
#'#
#' @param a the fitted ash object#
#' @param x the vector of locations at which density is to be computed#
#'#
#' @details None#
#' #
#' @export#
#' #
#'#
density.ash=function(a,x){list(x=x,y=dens(a$fitted.g,x))}#
#
#' @title cdf method for ash object#
#'#
#' @description Computed the cdf of the underlying fitted distribution#
#'#
#' @param a the fitted ash object#
#' @param x the vector of locations at which cdf is to be computed#
#' @param lower.tail (default=TRUE) whether to compute the lower or upper tail#
#'#
#' @details None#
#' #
#' @export#
#' #
#'#
cdf.ash=function(a,x,lower.tail=TRUE){#
  return(list(x=x,y=mixcdf(a$fitted.g,x,lower.tail)))#
}#
#return the KL-divergence between 2 dirichlet distributions#
#p,q are the vectors of dirichlet parameters of same lengths#
diriKL = function(p,q){#
  p.sum = sum(p)#
  q.sum = sum(q)#
  k = length(q)#
  KL = lgamma(q.sum)-lgamma(p.sum)+sum((q-p)*(digamma(q)-digamma(rep(q.sum,k))))+sum(lgamma(p)-lgamma(q))#
  return(KL)#
}#
#
#helper function for VBEM#
VB.update = function(matrix_lik, pipost){#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost * matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  B = sum(classprob*log(avgpipost*matrix_lik),na.rm=TRUE) - diriKL(prior,pipost) #negative free energy#
  return(list(classprob=classprob,B=B))#
}#
EMnonzeromean = function(x, sebetahat, mixsd, reltol=1e-8){#
}
' @useDynLib ashr#
#todo#
##
#' @title Main Adaptive SHrinkage function#
#'#
#' @description Takes vectors of estimates (betahat) and their standard errors (sebetahat), and applies#
#' shrinkage to them, using Empirical Bayes methods, to compute shrunk estimates for beta.#
#'#
#' @details See readme for more details#
#' #
#' @param betahat, a p vector of estimates #
#' @param sebetahat, a p vector of corresponding standard errors#
#' @param method: specifies how ash is to be run. Can be "shrinkage" (if main aim is shrinkage) or "fdr" (if main aim is to assess fdr or fsr)#
#' This is simply a convenient way to specify certain combinations of parameters: "shrinkage" sets pointmass=FALSE and prior="uniform";#
#' "fdr" sets pointmass=TRUE and prior="nullbiased".#
#' @param mixcompdist: distribution of components in mixture ("normal", "uniform" or "halfuniform")#
#'#
#' @param lambda1: multiplicative "inflation factor" for standard errors (like Genomic Control)#
#' @param lambda2: additive "inflation factor" for standard errors (like Genomic Control)#
#' @param nullcheck: whether to check that any fitted model exceeds the "null" likelihood#
#' in which all weight is on the first component#
#' @param df: appropriate degrees of freedom for (t) distribution of betahat/sebetahat#
#' @param randomstart: bool, indicating whether to initialize EM randomly. If FALSE, then initializes to prior mean (for EM algorithm) or prior (for VBEM)#
#' @param nullweight: scalar, the weight put on the prior of null under "fdr" method#
#' @param nonzeromean: bool, indicating whether to use a nonzero mean unimodal mixture(defaults to "FALSE")#
#' @param pointmass: bool, indicating whether to use a point mass at zero as one of components for a mixture distribution#
#' @param onlylogLR: bool, indicating whether to use this function to get logLR. Skip posterior prob, posterior mean, lfdr...#
#' @param prior: string, or numeric vector indicating Dirichlet prior on mixture proportions (defaults to "uniform", or 1,1...,1; also can be "nullbiased" 1,1/k-1,...,1/k-1 to put more weight on first component)#
#' @param mixsd: vector of sds for underlying mixture components #
#' @param VB: whether to use Variational Bayes to estimate mixture proportions (instead of EM to find MAP estimate)#
#' @param gridmult: the multiplier by which the default grid values for mixsd differ by one another. (Smaller values produce finer grids)#
#' @param minimal_output: if TRUE, just outputs the fitted g and the lfsr (useful for very big data sets where memory is an issue) #
#' @param g: the prior distribution for beta (usually estimated from the data; this is used primarily in simulated data to do computations with the "true" g)#
#' @param maxiter: maximum number of iterations of the EM algorithm#
#' @param cxx: flag to indicate whether to use the c++ (Rcpp) version#
#' #
#'#
#' @return a list with elements fitted.g is fitted mixture#
#' logLR : logP(D|mle(pi)) - logP(D|null)#
#' #
#' @export#
#' #
#' @examples #
#' beta = c(rep(0,100),rnorm(100))#
#' sebetahat = abs(rnorm(200,0,1))#
#' betahat = rnorm(200,beta,sebetahat)#
#' beta.ash = ash(betahat, sebetahat)#
#' summary(beta.ash)#
#' plot(betahat,beta.ash$PosteriorMean,xlim=c(-4,4),ylim=c(-4,4))#
#' #
#' #
#Things to do:#
# check sampling routine#
# check number of iterations#
ash = function(betahat,sebetahat,method = c("shrink","fdr"), #
               mixcompdist = c("normal","uniform","halfuniform"),#
               lambda1=1,lambda2=0,nullcheck=TRUE,df=NULL,randomstart=FALSE,#
               nullweight=10,nonzeromean=FALSE, #
               pointmass = FALSE, #
               onlylogLR = FALSE, #
               prior=c("uniform","nullbiased"), #
               mixsd=NULL, VB=FALSE,gridmult=sqrt(2),#
               minimaloutput=FALSE,#
               multiseqoutput=FALSE,#
               g=NULL,#
               maxiter = 5000,#
               cxx=FALSE){#
  #method provides a convenient interface to set a particular combinations of parameters for prior an#
  #If method is supplied, use it to set up specific values for these parameters; provide warning if values#
  #are also specified by user#
  #If method is not supplied use the user-supplied values (or defaults if user does not specify them)#
  if(!missing(method)){#
    method = match.arg(method) #
    if(method=="shrink"){#
      if(missing(prior)){#
        prior = "uniform"#
      } else {#
        warning("Specification of prior overrides default for method shrink")#
      }#
      if(missing(pointmass)){#
        pointmass=FALSE#
      } else {#
        warning("Specification of pointmass overrides default for method shrink")#
      }#
    }#
    if(method=="fdr"){#
      if(missing(prior)){#
        prior = "nullbiased"#
      } else {#
        warning("Specification of prior overrides default for method fdr")#
      }#
      if(missing(pointmass)){#
        pointmass=TRUE#
      } else {#
        warning("Specification of pointmass overrides default for method fdr")#
      }#
    }  #
  }#
  if(gridmult<=1&multiseqoutput!=TRUE)#
    stop("gridmult must be > 1")#
  mixcompdist = match.arg(mixcompdist)#
  # if(mixcompdist=="uniform" & pointmass==TRUE){#
  #    stop("point mass not yet implemented for uniform or half-uniform")#
  #  }#
  #  if(mixcompdist=="halfuniform" & pointmass==TRUE){#
  #    stop("point mass not yet implemented for uniform or half-uniform")#
  #  }#
  if(!is.numeric(prior)){#
    prior = match.arg(prior)#
  }  #
  if(length(sebetahat)==1){#
    sebetahat = rep(sebetahat,length(betahat))#
  }#
  if(length(sebetahat) != length(betahat)){#
    stop("Error: sebetahat must have length 1, or same length as betahat")#
  }#
  completeobs = (!is.na(betahat) & !is.na(sebetahat))#
  n=sum(completeobs)#
  if(n==0){#
    if(onlylogLR){#
      return(list(pi=NULL, logLR = 0))#
    }#
    else{#
      stop("Error: all input values are missing")#
    }#
  }  #
  if(!is.null(g)){#
    maxiter = 1 # if g is specified, don't iterate the EM#
    prior = rep(1,ncomp(g)) #prior is not actually used if g specified, but required to make sure EM doesn't produce warning#
    null.comp=1 #null.comp also not used, but required #
  } else {#
    if(is.null(mixsd)){#
      mixsd = autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)#
    }#
    if(pointmass){#
      mixsd = c(0,mixsd)#
    }#
    null.comp = which.min(mixsd) #which component is the "null"#
    k = length(mixsd)#
	if(nonzeromean & is.null(df)){#
		nonzeromean.fit=nonzeromeanEM(betahat[completeobs], sebetahat[completeobs], mixsd, maxiter=maxiter)#
		betahat[completeobs]= betahat[completeobs] - nonzeromean.fit$nonzeromean#
	}#
	elseif(nonzeromean & !is.null(df) ){#
		stop("Error: Nonzero mean only implemented for df=NULL")#
	}#
    if(!is.numeric(prior)){#
      if(prior=="nullbiased"){ # set up prior to favour "null"#
        prior = rep(1,k)#
        prior[null.comp] = nullweight #prior 10-1 in favour of null by default#
      }else if(prior=="uniform"){#
        prior = rep(1,k)#
      }#
    }#
    if(length(prior)!=k | !is.numeric(prior)){#
      stop("invalid prior specification")#
    }#
    if(randomstart){#
      pi = rgamma(k,1,1)#
    } else {#
      if(k<n){#
        pi=rep(1,k)/n #default initialization strongly favours null; puts weight 1/n on everything except null#
        pi[null.comp] = (n-k+1)/n #the motivation is data can quickly drive away from null, but tend to drive only slowly toward null.#
      } else {#
        pi=rep(1,k)/k#
      }#
    }#
    pi=normalize(pi)#
    if(!is.element(mixcompdist,c("normal","uniform","halfuniform"))) stop("Error: invalid type of mixcompdist")#
    if(mixcompdist=="normal") g=normalmix(pi,rep(0,k),mixsd)#
    if(mixcompdist=="uniform") g=unimix(pi,-mixsd,mixsd)#
    if(mixcompdist=="halfuniform"){#
      g = unimix(c(pi,pi)/2,c(-mixsd,rep(0,k)),c(rep(0,k),mixsd))#
      prior = rep(prior, 2)#
      pi = rep(pi, 2)#
    }#
  }#
  pi.fit=EMest(betahat[completeobs],lambda1*sebetahat[completeobs]+lambda2,g,prior,null.comp=null.comp,nullcheck=nullcheck,VB=VB,maxiter = maxiter, cxx=cxx, df=df)  #
  if (!onlylogLR){#
      n=length(betahat)#
      if (!multiseqoutput){#
          ZeroProb = rep(0,length=n)#
          NegativeProb = rep(0,length=n)#
      }#
      if (!minimaloutput){#
          PosteriorMean = rep(0,length=n)#
          PosteriorSD = rep(0,length=n)#
      }#
      if (!multiseqoutput){#
            ZeroProb[completeobs] = colSums(comppostprob(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)[comp_sd(pi.fit$g)==0,,drop=FALSE])     #
              NegativeProb[completeobs] = cdf_post(pi.fit$g, 0, betahat[completeobs],sebetahat[completeobs],df) - ZeroProb[completeobs]#
          }#
          if (!minimaloutput){#
              PosteriorMean[completeobs] = postmean(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
              PosteriorSD[completeobs] = postsd(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
      }#
                                        #FOR MISSING OBSERVATIONS, USE THE PRIOR INSTEAD OF THE POSTERIOR#
      if (!multiseqoutput){#
          ZeroProb[!completeobs] = sum(mixprop(pi.fit$g)[comp_sd(pi.fit$g)==0])#
          NegativeProb[!completeobs] = mixcdf(pi.fit$g,0)#
          lfsr = compute_lfsr(NegativeProb,ZeroProb)#
      }#
      if (!minimaloutput){#
          PosteriorMean[!completeobs] = mixmean(pi.fit$g)#
          PosteriorSD[!completeobs] = mixsd(pi.fit$g)#
      }#
      if (!minimaloutput & !multiseqoutput){#
          PositiveProb = 1- NegativeProb-ZeroProb#
          lfsra = compute_lfsra(PositiveProb,NegativeProb,ZeroProb) #
          lfdr = ZeroProb#
          qvalue = qval.from.lfdr(lfdr)#
      }#
  }#
  if (!minimaloutput)#
      logLR = tail(pi.fit$loglik,1) - pi.fit$null.loglik#
#
  if(nonzeromean & is.null(df)){#
      #Adding back the nonzero mean#
      betahat[completeobs]= betahat[completeobs]+nonzeromean.fit$nonzeromean#
      pi.fit$g$mean =nonzeromean.fit$nonzeromean#
      PosteriorMean= PosteriorMean + nonzeromean.fit$nonzeromean      #
  }	    #
  if (onlylogLR){#
      return(list(fitted.g=pi.fit$g, logLR = logLR, df=df))}#
  else if (minimaloutput){#
      return(list(fitted.g = pi.fit$g, lfsr = lfsr, fit = pi.fit, df=df))}#
  else if (multiseqoutput){#
      return(list(fitted.g = pi.fit$g, logLR = logLR, PosteriorMean = PosteriorMean, PosteriorSD = PosteriorSD, call= match.call(), df=df))}#
  else{#
      result = list(fitted.g = pi.fit$g, logLR = logLR, PosteriorMean = PosteriorMean, PosteriorSD = PosteriorSD, PositiveProb = PositiveProb, NegativeProb = NegativeProb, ZeroProb = ZeroProb, lfsr = lfsr,lfsra = lfsra, lfdr = lfdr, qvalue = qvalue, fit = pi.fit, lambda1 = lambda1, lambda2 = lambda2, call = match.call(), data = list(betahat = betahat, sebetahat=sebetahat), df=df)#
      class(result) = "ash"#
      return(result)#
  }#
}#
  #if(nsamp>0){#
  #  sample = posterior_sample(post,nsamp)#
  #}#
#
# #' @title Faster version of function ash#
# #'#
# #' @description This function has similar functionality as ash, but only returns some of the outputs.#
# #'#
# #' @param betahat, a p vector of estimates#
# #' @param sebetahat, a p vector of corresponding standard errors#
# #' @param nullcheck: whether to check that any fitted model exceeds the "null" likelihood in which all weight is on the first component#
# #' @param randomstart: bool, indicating whether to initialize EM randomly. If FALSE, then initializes to prior mean (for EM algorithm) or prior (for VBEM)#
# #' @param pointmass: bool, indicating whether to use a point mass at zero as one of components for a mixture distribution#
# #' @param onlylogLR: bool, indicating whether to use this function to get logLR. Skip posterior prob, posterior mean, lfdr...#
# #' @param prior: string, or numeric vector indicating Dirichlet prior on mixture proportions (defaults to "uniform", or 1,1...,1; also can be "nullbiased" 1,1/k-1,...,1/k-1 to put more weight on first component)#
# #' @param mixsd: vector of sds for underlying mixture components#
# #' @param VB: whether to use Variational Bayes to estimate mixture proportions (instead of EM to find MAP estimate)#
# #' @param gridmult: the multiplier by which the default grid values for mixsd differ by one another. (Smaller values produce finer grids)#
# #' @param g: the prior distribution for beta (usually estimated from the data; this is used primarily in simulated data to do computations with the "true" g)#
# #' @param cxx: flag to indicate whether to use the c++ (Rcpp) version#
# #'#
# #' @return a list with elements fitted.g is fitted mixture#
# #' logLR : logP(D|mle(pi)) - logP(D|null)#
# #'#
# #' @export#
# fast.ash = function(betahat,sebetahat, #
#                     nullcheck=TRUE,randomstart=FALSE, #
#                     pointmass = TRUE,    #
#                     prior=c("nullbiased","uniform"), #
#                     mixsd=NULL, VB=FALSE,gridmult=4,#
#                     g=NULL, cxx=TRUE,#
#                     onlylogLR = FALSE,df=NULL){#
#   #
#   if(onlylogLR){#
#     pointmass <- TRUE  #
#   }#
#   #
#   #If method is supplied, use it to set up defaults; provide warning if these default values#
#   #are also specified by user#
#   if(!is.numeric(prior)){#
#     prior = match.arg(prior)#
#   }#
#   #
#   if(length(sebetahat)==1){#
#     sebetahat = rep(sebetahat,length(betahat))#
#   }#
#   if(length(sebetahat) != length(betahat)){#
#     stop("Error: sebetahat must have length 1, or same length as betahat")#
#   }#
#   #
#   completeobs = (!is.na(betahat) & !is.na(sebetahat))#
#   if(sum(completeobs)==0){#
#     if(onlylogLR){#
#       return(list(pi=NULL, logLR = 0))#
#     }else{#
#       stop("Error: all input values are missing")#
#     }#
#   }  #
#   #
#   if(is.null(mixsd)){#
#     mixsd= autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)#
#   }#
#   if(pointmass){#
#     mixsd = c(0,mixsd)#
#   }#
#   #
#   k=length(mixsd)  #
#   null.comp = which.min(mixsd) #which component is the "null"#
#   #
#   if(!is.numeric(prior)){#
#     if(prior=="nullbiased"){ # set up prior to favour "null"#
#       prior = rep(1,k)#
#       prior[null.comp] = 10 #prior 10-1 in favour of null#
#     }else if(prior=="uniform"){#
#       prior = rep(1,k)#
#     }#
#   }#
#   #
#   if(length(prior)!=k | !is.numeric(prior)){#
#     stop("invalid prior specification")#
#   }#
#   #
#   if(missing(g)){#
#     pi = prior^2 #default is to initialize pi at prior (mean)#
#     if(randomstart){pi=rgamma(k,1,1)}#
#     pi=normalize(pi)#
#     g=normalmix(pi,rep(0,k),mixsd)#
#     maxiter = 5000#
#   } else {#
#     maxiter = 1; # if g is specified, don't iterate the EM #
#   }#
#   #
#   pi.fit=EMest(betahat[completeobs],sebetahat[completeobs],g,prior,null.comp=null.comp,nullcheck=nullcheck,VB=VB,maxiter = maxiter, cxx=cxx, df=df)  #
#   #
#   if(onlylogLR){#
#     logLR = tail(pi.fit$loglik,1) - pi.fit$null.loglik#
#     return(list(pi=pi.fit$pi, logLR = logLR))#
#   }else{#
#     #
#     n=length(betahat)#
#     PosteriorMean = rep(0,length=n)#
#     PosteriorSD=rep(0,length=n)#
#     #
#     if(is.null(df)){#
#       PosteriorMean[completeobs] = postmean(pi.fit$g,betahat[completeobs],sebetahat[completeobs])#
#       PosteriorSD[completeobs] =postsd(pi.fit$g,betahat[completeobs],sebetahat[completeobs]) #
#     }#
#     else{#
#       PosteriorMean[completeobs] = postmean_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
#       PosteriorSD[completeobs] =postsd_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
#     }#
#     #FOR MISSING OBSERVATIONS, USE THE PRIOR INSTEAD OF THE POSTERIOR#
#     PosteriorMean[!completeobs] = mixmean(pi.fit$g)#
#     PosteriorSD[!completeobs] =mixsd(pi.fit$g)  #
#     #
#     result = list(fitted.g=pi.fit$g,PosteriorMean = PosteriorMean,PosteriorSD=PosteriorSD,call=match.call(),data=list(betahat = betahat, sebetahat=sebetahat))#
#     return(result)#
#   }#
#   #if(nsamp>0){#
#   #  sample = posterior_sample(post,nsamp)#
#   #}#
# }#
compute_lfsr = function(NegativeProb,ZeroProb){#
  ifelse(NegativeProb> 0.5*(1-ZeroProb),1-NegativeProb,NegativeProb+ZeroProb)#
}#
#
compute_lfsra = function(PositiveProb, NegativeProb,ZeroProb){#
  ifelse(PositiveProb<NegativeProb,2*PositiveProb+ZeroProb,2*NegativeProb+ZeroProb)  #
}  #
#
#' @title Estimate unimodal nonzero mean of a mixture model by EM algorithm#
#'#
#' @description Given the data, standard error of the data and standard deviations of the Gaussian mixture model, estimate the mean of a unimodal Gaussian mixture by an EM algorithm.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates unimodal mean \eqn{\mu} by EM algorithm. Uses the SQUAREM package to accelerate convergence of EM. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' #
#' @param betahat, a p vector of estimates #
#' @param sebetahat, a p vector of corresponding standard errors#
#' @param mixsd: vector of sds for underlying mixture components #
#' @param pi.init, the initial value of \eqn{\pi} to use. If not specified defaults to (1/k,...,1/k).#
#' @param tol, the tolerance for convergence of log-likelihood.#
#' @param maxiter the maximum number of iterations performed#
#' #
#' @return A list, including the estimates (\eqn{\mu}) and (\eqn{\pi}), the log likelihood for each iteration (NQ)#
#' and a flag to indicate convergence#
#'  #
#' @export#
#' #
#' #
nonzeromeanEM = function(betahat, sebetahat, mixsd, pi.init=NULL,tol=1e-7,maxiter=5000){#
  if(is.null(pi.init)){#
    pi.init = rep(1/length(mixsd),length(mixsd))# Use as starting point for pi#
  }#
  mupi=c(mean(betahat),pi.init)#
  res=squarem(par=mupi,fixptfn=nonzeromeanEMfixpoint,objfn=nonzeromeanEMobj,betahat=betahat,sebetahat=sebetahat,mixsd=mixsd,control=list(maxiter=maxiter,tol=tol))#
  return(list(nonzeromean=res$par[1],pi=res$par[-1],NQ=-res$value.objfn,niter = res$iter, converged=res$convergence,post=res$par))#
}#
#
nonzeromeanEMfixpoint = function(mupi,betahat,sebetahat,mixsd){#
	#omegamatrix=matrix(NA,nrow=length(betahat),ncol=length(mixsd))#
	mu=mupi[1]#
	pimean=mupi[-1]#
	sdmat = sqrt(outer(sebetahat ^2,mixsd^2,"+")) #
	xmat=matrix(rep(betahat,length(mixsd)),ncol=length(mixsd))#
	omegamatrix=t(t(dnorm(xmat,mean=mu,sd=sdmat))*pimean)#
	omegamatrix=omegamatrix /rowSums(omegamatrix)#
	pinew=normalize(colSums(omegamatrix))#
	munew=sum(omegamatrix*xmat/(sdmat^2))/sum(omegamatrix/(sdmat^2))#
	mupi=c(munew,pinew)#
	return(mupi)#
}#
#
nonzeromeanEMobj = function(mupi,betahat,sebetahat,mixsd){#
	mu=mupi[1]#
	pimean=mupi[-1]#
	sdmat = sqrt(outer(sebetahat ^2,mixsd^2,"+")) #
	xmat=matrix(rep(betahat,length(mixsd)),ncol=length(mixsd))#
	omegamatrix=t(t(dnorm(xmat,mean=mu,sd=sdmat))*pimean)#
	omegamatrix=omegamatrix /rowSums(omegamatrix)#
	NegativeQ=-sum(omegamatrix*dnorm(xmat,mean=mu,sd=sdmat,log=TRUE))#
	return(NegativeQ)#
}#
#' @title Estimate posterior distribution on mixture proportions of a mixture model by a Variational Bayes EM algorithm#
#'#
#' @description Given the individual component likelihoods for a mixture model, estimates the posterior on #
#' the mixture proportions by an VBEM algorithm. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates posterior on mixture proportions \eqn{\pi} by Variational Bayes, #
#' with a Dirichlet prior on \eqn{\pi}. #
#' Algorithm adapted from Bishop (2009), Pattern Recognition and Machine Learning, Chapter 10.#
#' #
#' @param matrix_lik: a n by k matrix with (j,k)th element equal to \eqn{f_k(x_j)}.#
#' @param prior: a k vector of the parameters of the Dirichlet prior on \eqn{\pi}. Recommended to be rep(1,k)#
#' @param post.init: the initial value of the posterior parameters. If not specified defaults to the prior parameters.#
#' @param tol: the tolerance for convergence of log-likelihood bound.#
#' @param maxiter: the maximum number of iterations performed#
#' #
#' @return A list, whose components include point estimates (pihat), #
#' the parameters of the fitted posterior on \eqn{\pi} (pipost),#
#' the bound on the log likelihood for each iteration (B)#
#' and a flag to indicate convergence (converged).#
#'  #
#' @export#
#' #
mixVBEM = function(matrix_lik, prior, pi.init = NULL,tol=1e-7, maxiter=5000){#
  k=ncol(matrix_lik)#
  if(is.null(pi.init)){#
    pi.init = rep(1,k)# Use as starting point for pi#
  } #
  res = squarem(par=pi.init,fixptfn=VBfixpoint, objfn=VBnegpenloglik,matrix_lik=matrix_lik, prior=prior, control=list(maxiter=maxiter,tol=tol))#
  return(list(pihat = res$par/sum(res$par), B=res$value.objfn, niter = res$iter, converged=res$convergence,post=res$par))#
}#
VBfixpoint = function(pipost, matrix_lik, prior){  #
  n=nrow(matrix_lik)#
  k=ncol(matrix_lik)#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost*matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  pipostnew = colSums(classprob) + prior#
  return(pipostnew)#
}#
#
VBnegpenloglik=function(pipost,matrix_lik,prior){#
  return(-VBpenloglik(pipost,matrix_lik,prior))#
}#
#
VBpenloglik = function(pipost, matrix_lik, prior){#
  n=nrow(matrix_lik)#
  k=ncol(matrix_lik)#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost*matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  B= sum(classprob*log(avgpipost*matrix_lik),na.rm=TRUE) - diriKL(prior,pipost) - sum(classprob*log(classprob)) #
  return(B)#
}#
#' @title Estimate mixture proportions of a mixture model by EM algorithm#
#'#
#' @description Given the individual component likelihoods for a mixture model, estimates the mixture proportions by an EM algorithm.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates mixture proportions \eqn{\pi} by maximum likelihood, or by maximum a posteriori (MAP) estimation for a Dirichlet prior on $\pi$ #
#' (if a prior is specified).  Uses the SQUAREM package to accelerate convergence of EM. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' #
#' @param matrix_lik, a n by k matrix with (j,k)th element equal to \eqn{f_k(x_j)}.#
#' @param prior, a k vector of the parameters of the Dirichlet prior on \eqn{\pi}. Recommended to be rep(1,k)#
#' @param pi.init, the initial value of \eqn{\pi} to use. If not specified defaults to (1/k,...,1/k).#
#' @param tol, the tolerance for convergence of log-likelihood.#
#' @param maxiter the maximum number of iterations performed#
#' #
#' @return A list, including the estimates (pihat), the log likelihood for each interation (B)#
#' and a flag to indicate convergence#
#'  #
#' @export#
#' #
#' #
mixEM = function(matrix_lik, prior, pi.init = NULL,tol=1e-7, maxiter=5000){#
  if(is.null(pi.init)){#
    pi.init = rep(1/k,k)# Use as starting point for pi#
  } #
  res = squarem(par=pi.init,fixptfn=fixpoint, objfn=negpenloglik,matrix_lik=matrix_lik, prior=prior, control=list(maxiter=maxiter,tol=tol))#
  return(list(pihat = normalize(pmax(0,res$par)), B=res$value.objfn, #
              niter = res$iter, converged=res$convergence))#
}#
#
# helper functions used by mixEM#
normalize = function(x){return(x/sum(x))}#
#
fixpoint = function(pi, matrix_lik, prior){  #
  pi = normalize(pmax(0,pi)) #avoid occasional problems with negative pis due to rounding#
  m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
  m.rowsum = rowSums(m)#
  classprob = m/m.rowsum #an n by k matrix#
  pinew = normalize(colSums(classprob) + prior - 1)#
  return(pinew)#
}#
#
negpenloglik = function(pi,matrix_lik,prior){return(-penloglik(pi,matrix_lik,prior))}#
#
penloglik = function(pi, matrix_lik, prior){#
  pi = normalize(pmax(0,pi))#
  m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
  m.rowsum = rowSums(m)#
  loglik = sum(log(m.rowsum))#
  subset = (prior != 1.0)#
  priordens = sum((prior-1)[subset]*log(pi[subset]))#
  return(loglik+priordens)#
}#
#
#The kth element of this vector is the derivative #
#of the loglik for $\pi=(\pi_0,...,1-\pi_0,...)$ with respect to $\pi_0$ at $\pi_0=1$.#
gradient = function(matrix_lik){#
  n = nrow(matrix_lik)#
  grad = n - colSums(matrix_lik/matrix_lik[,1]) #
  return(grad)#
}#
#
# mixEM = function(matrix_lik, prior, pi.init = NULL,tol=0.0001, maxiter=5000){#
#   n=nrow(matrix_lik)#
#   k=ncol(matrix_lik)#
#   B = rep(0,maxiter)#
#   pi = pi.init#
#   if(is.null(pi.init)){#
#     pi = rep(1/k,k)# Use as starting point for pi#
#   } #
#   pi = ifelse(pi<1e-5,1e-5,pi) #set any estimates that are too small to be just very small#
#   pi = normalize(pi)#
#   #
#   loglik = rep(0,maxiter)#
#   priordens= rep(0,maxiter)#
#   m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
#   m.rowsum = rowSums(m)#
#   loglik[1] = sum(log(m.rowsum))#
#   priordens[1] = sum((prior-1)*log(pi)) #
#   classprob = m/m.rowsum #an n by k matrix#
#   i=1#
#   if(maxiter >= 2){#
#     for(i in 2:maxiter){  #
#       pi = colSums(classprob) + prior-1#
#       pi = ifelse(pi<1e-5,1e-5,pi) #set any estimates that are less than zero, which can happen with prior<1, to 0#
#       pi = normalize(pi)#
#         #
#       #Now re-estimate pi#
#       m  = t(pi * t(matrix_lik)) #
#       m.rowsum = rowSums(m)#
#       loglik[i] = sum(log(m.rowsum))#
#       priordens[i] = sum((prior-1)*log(pi)) #
#       classprob = m/m.rowsum#
#     #
#     #
#       if(abs(loglik[i]+priordens[i]-loglik[i-1]-priordens[i-1])<tol) break;#
#     }#
#   }#
#   converged=(abs(loglik[i]+priordens[i]-loglik[i-1]-priordens[i-1])<tol)#
#   if(!converged){#
#       warning("EM algorithm in function mixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
#   }#
#   return(list(pihat = pi, B=loglik[1:i], #
#               niter = i, converged=converged))#
# }#
#estimate mixture proportions of sigmaa by EM algorithm#
#prior gives the parameter of a Dirichlet prior on pi#
#(prior is used to encourage results towards smallest value of sigma when#
#likelihood is flat)#
#nullcheck indicates whether to check whether the loglike exceeds the null#
#(may not want to use if prior is used)#
#VB provides an approach to estimate the approximate posterior distribution#
#of mixture proportions of sigmaa by variational Bayes method#
#(use Dirichlet prior and approximate Dirichlet posterior)#
#if cxx TRUE use cpp version of R function mixEM#
EMest = function(betahat,sebetahat,g,prior,null.comp=1,nullcheck=TRUE,VB=FALSE, maxiter=5000, cxx=TRUE, df=NULL){ #
  pi.init = g$pi#
  k=ncomp(g)#
  n = length(betahat)#
  tol = min(0.1/n,1e-5) # set convergence criteria to be more stringent for larger samples#
  if(is.null(df)){#
    matrix_lik = t(compdens_conv(g,betahat,sebetahat))#
  }#
  else{#
    matrix_lik = t(compdens_conv_t(g,betahat,sebetahat,df))#
  }#
  #checks whether the gradient at pi0=1 is positive (suggesting that this is a fixed point)#
  #if(nullcheck){#
  #  if(all(gradient(matrix_lik)>=0)){#
  #    pi.init=rep(0,k)#
  #    pi.init[null.comp]=1 #this will make pi.init=(1,0,0...,0) which is a fixed point of the EM#
  #  }#
  #}#
  if(VB==TRUE){#
    EMfit=mixVBEM(matrix_lik,prior,maxiter=maxiter)}#
  else{#
    if (cxx==TRUE){#
      EMfit = cxxMixEM(matrix_lik,prior,pi.init,1e-5, maxiter) #currently use different convergence criteria for cxx version #
      if(!EMfit$converged){#
        warning("EM algorithm in function cxxMixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
      }#
    }#
    else{#
      EMfit = mixEM(matrix_lik,prior,pi.init,tol, maxiter)#
      if(!EMfit$converged & !(maxiter==1)){#
        warning("EM algorithm in function mixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
      }#
    }#
  }#
  pi = EMfit$pihat     #
  penloglik = EMfit$B #
  converged = EMfit$converged#
  niter = EMfit$niter#
  loglik.final =  penloglik(pi,matrix_lik,1) #compute penloglik without penalty#
  null.loglik = sum(log(matrix_lik[,null.comp]))  #
  if(nullcheck==TRUE & VB==FALSE){ #null check doesn't work with VB yet#
    pinull = rep(0,k)#
    pinull[null.comp]=1#
    null.penloglik = penloglik(pinull,matrix_lik,prior)#
    final.penloglik = penloglik(pi,matrix_lik,prior)#
    if(null.penloglik > final.penloglik){ #check whether exceeded "null" likelihood where everything is null#
      pi=pinull#
      loglik.final=penloglik(pi,matrix_lik,1)#
    }#
  }#
  g$pi=pi#
  return(list(loglik=loglik.final,null.loglik=null.loglik,#
              matrix_lik=matrix_lik,converged=converged,g=g))#
}#
#' @title Compute Posterior#
#'#
#' @description Return the posterior on beta given a prior (g) that is a mixture of normals (class normalmix) #
#' and observation betahat \sim N(beta,sebetahat)#
#'#
#' @details This can be used to obt#
#'#
#' @param g: a normalmix with components indicating the prior; works only if g has means 0#
#' @param betahat (n vector of observations) #
#' @param sebetahat (n vector of standard errors/deviations of observations)#
#' #
#' @return A list, (pi1,mu1,sigma1) whose components are each k by n matrices#
#' where k is number of mixture components in g, n is number of observations in betahat#
#' #
#' @export#
#' #
#' #
posterior_dist = function(g,betahat,sebetahat){#
  if(class(g)!="normalmix"){#
    stop("Error: posterior_dist implemented only for g of class normalmix")#
  }#
  pi0 = g$pi#
  mu0 = g$mean#
  sigma0 = g$sd  #
  k= length(pi0)#
  n= length(betahat)#
  if(!all.equal(g$mean,rep(0,k))) stop("Error: posterior_dist currently only implemented for zero-centered priors")#
  pi1 = pi0 * t(matrix_dens(betahat,sebetahat,sigma0))#
  pi1 = apply(pi1, 2, normalize) #pi1 is now an k by n matrix#
  #make k by n matrix versions of sigma0^2 and sebetahat^2#
  # and mu0 and betahat#
  s0m2 = matrix(sigma0^2,nrow=k,ncol=n,byrow=FALSE)#
  sebm2 = matrix(sebetahat^2,nrow=k,ncol=n, byrow=TRUE)#
  mu0m = matrix(mu0,nrow=k,ncol=n,byrow=FALSE)#
  bhatm = matrix(betahat,nrow=k,ncol=n,byrow=TRUE)#
  sigma1 = (s0m2*sebm2/(s0m2 + sebm2))^(0.5)  #
  w = sebm2/(s0m2 + sebm2)#
  mu1 = w*mu0m + (1-w)*bhatm#
  #WHERE DATA ARE MISSING, SET POSTERIOR = PRIOR#
  ismiss = (is.na(betahat) | is.na(sebetahat)) #
  pi1[,ismiss] = pi0#
  mu1[,ismiss] = mu0#
  sigma1[,ismiss] = sigma0#
  return(list(pi=pi1,mu=mu1,sigma=sigma1))#
}#
#
#return matrix of densities of observations (betahat) #
# assuming betahat_j \sim N(0, sebetahat_j^2 + sigmaavec_k^2)#
#normalized by maximum of each column#
#INPUT#
#betahat is n vector, #
#sebetahat is n vector, #
#sigmaavec is k vector#
#return is n by k matrix of the normal likelihoods, #
# with (j,k)th element the density of N(betahat_j; mean=0, var = sebetahat_j^2 + sigmaavec_k^2)#
#normalized to have maximum 1 in each column#
matrix_dens = function(betahat, sebetahat, sigmaavec){#
  k = length(sigmaavec)#
  n = length(betahat)#
  ldens = dnorm(betahat,0,sqrt(outer(sebetahat^2,sigmaavec^2,FUN="+")),log=TRUE)#
  maxldens = apply(ldens, 1, max)#
  ldens = ldens - maxldens#
  return(exp(ldens))#
}#
#
#return the "effective" estimate#
#that is the effect size betanew whose z score betanew/se#
#would give the same p value as betahat/se compared to a t with df#
effective.effect=function(betahat,se,df){#
  p = pt(betahat/se,df)#
  qnorm(p,sd=se)#
}#
#' @title Function to compute q values from local false discovery rates#
#'#
#' @description Computes q values from a vector of local fdr estimates#
#'#
#' @details The q value for a given lfdr is an estimate of the (tail) False Discovery Rate #
#' for all findings with a smaller lfdr, and is found by the average of the lfdr for#
#' all more significant findings. See Storey (2003), Annals of Statistics, for definition of q value.  #
#' #
#' #
#' @param lfdr, a vector of local fdr estimates#
#'#
#' @return vector of q values#
#' #
#' @export#
qval.from.lfdr = function(lfdr){#
  o = order(lfdr)#
  qvalue=rep(NA,length(lfdr))#
  qvalue[o] = (cumsum(sort(lfdr))/(1:sum(!is.na(lfdr))))#
  return(qvalue)#
}#
#
# try to select a default range for the sigmaa values#
# that should be used, based on the values of betahat and sebetahat#
# mult is the multiplier by which the sds differ across the grid#
autoselect.mixsd = function(betahat,sebetahat,mult){#
  sigmaamin = min(sebetahat)/10 #so that the minimum is small compared with measurement precision#
  if(all(betahat^2<sebetahat^2)){#
    sigmaamax = 8*sigmaamin #to deal with the occassional odd case where this could happen; 8 is arbitrary#
  } else {#
    sigmaamax = 2*sqrt(max(betahat^2-sebetahat^2)) #this computes a rough largest value you'd want to use, based on idea that sigmaamax^2 + sebetahat^2 should be at least betahat^2   #
  }#
  if(mult==0){#
    return(c(0,sigmaamax/2))#
  }else{#
    npoint = ceiling(log2(sigmaamax/sigmaamin)/log2(mult))#
    return(mult^((-npoint):0) * sigmaamax)#
  }#
}#
#return the KL-divergence between 2 dirichlet distributions#
#p,q are the vectors of dirichlet parameters of same lengths#
diriKL = function(p,q){#
  p.sum = sum(p)#
  q.sum = sum(q)#
  k = length(q)#
  KL = lgamma(q.sum)-lgamma(p.sum)+sum((q-p)*(digamma(q)-digamma(rep(q.sum,k))))+sum(lgamma(p)-lgamma(q))#
  return(KL)#
}#
#
#helper function for VBEM#
VB.update = function(matrix_lik, pipost){#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost * matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  B = sum(classprob*log(avgpipost*matrix_lik),na.rm=TRUE) - diriKL(prior,pipost) #negative free energy#
  return(list(classprob=classprob,B=B))#
}
onlylogLR=FALSE
' @useDynLib ashr#
#todo#
##
#' @title Main Adaptive SHrinkage function#
#'#
#' @description Takes vectors of estimates (betahat) and their standard errors (sebetahat), and applies#
#' shrinkage to them, using Empirical Bayes methods, to compute shrunk estimates for beta.#
#'#
#' @details See readme for more details#
#' #
#' @param betahat, a p vector of estimates #
#' @param sebetahat, a p vector of corresponding standard errors#
#' @param method: specifies how ash is to be run. Can be "shrinkage" (if main aim is shrinkage) or "fdr" (if main aim is to assess fdr or fsr)#
#' This is simply a convenient way to specify certain combinations of parameters: "shrinkage" sets pointmass=FALSE and prior="uniform";#
#' "fdr" sets pointmass=TRUE and prior="nullbiased".#
#' @param mixcompdist: distribution of components in mixture ("normal", "uniform" or "halfuniform")#
#'#
#' @param lambda1: multiplicative "inflation factor" for standard errors (like Genomic Control)#
#' @param lambda2: additive "inflation factor" for standard errors (like Genomic Control)#
#' @param nullcheck: whether to check that any fitted model exceeds the "null" likelihood#
#' in which all weight is on the first component#
#' @param df: appropriate degrees of freedom for (t) distribution of betahat/sebetahat#
#' @param randomstart: bool, indicating whether to initialize EM randomly. If FALSE, then initializes to prior mean (for EM algorithm) or prior (for VBEM)#
#' @param nullweight: scalar, the weight put on the prior of null under "fdr" method#
#' @param nonzeromean: bool, indicating whether to use a nonzero mean unimodal mixture(defaults to "FALSE")#
#' @param pointmass: bool, indicating whether to use a point mass at zero as one of components for a mixture distribution#
#' @param onlylogLR: bool, indicating whether to use this function to get logLR. Skip posterior prob, posterior mean, lfdr...#
#' @param prior: string, or numeric vector indicating Dirichlet prior on mixture proportions (defaults to "uniform", or 1,1...,1; also can be "nullbiased" 1,1/k-1,...,1/k-1 to put more weight on first component)#
#' @param mixsd: vector of sds for underlying mixture components #
#' @param VB: whether to use Variational Bayes to estimate mixture proportions (instead of EM to find MAP estimate)#
#' @param gridmult: the multiplier by which the default grid values for mixsd differ by one another. (Smaller values produce finer grids)#
#' @param minimal_output: if TRUE, just outputs the fitted g and the lfsr (useful for very big data sets where memory is an issue) #
#' @param g: the prior distribution for beta (usually estimated from the data; this is used primarily in simulated data to do computations with the "true" g)#
#' @param maxiter: maximum number of iterations of the EM algorithm#
#' @param cxx: flag to indicate whether to use the c++ (Rcpp) version#
#' #
#'#
#' @return a list with elements fitted.g is fitted mixture#
#' logLR : logP(D|mle(pi)) - logP(D|null)#
#' #
#' @export#
#' #
#' @examples #
#' beta = c(rep(0,100),rnorm(100))#
#' sebetahat = abs(rnorm(200,0,1))#
#' betahat = rnorm(200,beta,sebetahat)#
#' beta.ash = ash(betahat, sebetahat)#
#' summary(beta.ash)#
#' plot(betahat,beta.ash$PosteriorMean,xlim=c(-4,4),ylim=c(-4,4))#
#' #
#' #
#Things to do:#
# check sampling routine#
# check number of iterations#
ash = function(betahat,sebetahat,method = c("shrink","fdr"), #
               mixcompdist = c("normal","uniform","halfuniform"),#
               lambda1=1,lambda2=0,nullcheck=TRUE,df=NULL,randomstart=FALSE,#
               nullweight=10,nonzeromean=FALSE, #
               pointmass = FALSE, #
               onlylogLR = FALSE, #
               prior=c("uniform","nullbiased"), #
               mixsd=NULL, VB=FALSE,gridmult=sqrt(2),#
               minimaloutput=FALSE,#
               multiseqoutput=FALSE,#
               g=NULL,#
               maxiter = 5000,#
               cxx=FALSE){#
  #method provides a convenient interface to set a particular combinations of parameters for prior an#
  #If method is supplied, use it to set up specific values for these parameters; provide warning if values#
  #are also specified by user#
  #If method is not supplied use the user-supplied values (or defaults if user does not specify them)#
  if(!missing(method)){#
    method = match.arg(method) #
    if(method=="shrink"){#
      if(missing(prior)){#
        prior = "uniform"#
      } else {#
        warning("Specification of prior overrides default for method shrink")#
      }#
      if(missing(pointmass)){#
        pointmass=FALSE#
      } else {#
        warning("Specification of pointmass overrides default for method shrink")#
      }#
    }#
    if(method=="fdr"){#
      if(missing(prior)){#
        prior = "nullbiased"#
      } else {#
        warning("Specification of prior overrides default for method fdr")#
      }#
      if(missing(pointmass)){#
        pointmass=TRUE#
      } else {#
        warning("Specification of pointmass overrides default for method fdr")#
      }#
    }  #
  }#
  if(gridmult<=1&multiseqoutput!=TRUE)#
    stop("gridmult must be > 1")#
  mixcompdist = match.arg(mixcompdist)#
  # if(mixcompdist=="uniform" & pointmass==TRUE){#
  #    stop("point mass not yet implemented for uniform or half-uniform")#
  #  }#
  #  if(mixcompdist=="halfuniform" & pointmass==TRUE){#
  #    stop("point mass not yet implemented for uniform or half-uniform")#
  #  }#
  if(!is.numeric(prior)){#
    prior = match.arg(prior)#
  }  #
  if(length(sebetahat)==1){#
    sebetahat = rep(sebetahat,length(betahat))#
  }#
  if(length(sebetahat) != length(betahat)){#
    stop("Error: sebetahat must have length 1, or same length as betahat")#
  }#
  completeobs = (!is.na(betahat) & !is.na(sebetahat))#
  n=sum(completeobs)#
  if(n==0){#
    if(onlylogLR){#
      return(list(pi=NULL, logLR = 0))#
    }#
    else{#
      stop("Error: all input values are missing")#
    }#
  }  #
  if(!is.null(g)){#
    maxiter = 1 # if g is specified, don't iterate the EM#
    prior = rep(1,ncomp(g)) #prior is not actually used if g specified, but required to make sure EM doesn't produce warning#
    null.comp=1 #null.comp also not used, but required #
  } else {#
    if(is.null(mixsd)){#
      mixsd = autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)#
    }#
    if(pointmass){#
      mixsd = c(0,mixsd)#
    }#
    null.comp = which.min(mixsd) #which component is the "null"#
    k = length(mixsd)#
	if(nonzeromean & is.null(df)){#
		nonzeromean.fit=nonzeromeanEM(betahat[completeobs], sebetahat[completeobs], mixsd, maxiter=maxiter)#
		betahat[completeobs]= betahat[completeobs] - nonzeromean.fit$nonzeromean#
	}#
	elseif(nonzeromean & !is.null(df) ){#
		stop("Error: Nonzero mean only implemented for df=NULL")#
	}#
    if(!is.numeric(prior)){#
      if(prior=="nullbiased"){ # set up prior to favour "null"#
        prior = rep(1,k)#
        prior[null.comp] = nullweight #prior 10-1 in favour of null by default#
      }else if(prior=="uniform"){#
        prior = rep(1,k)#
      }#
    }#
    if(length(prior)!=k | !is.numeric(prior)){#
      stop("invalid prior specification")#
    }#
    if(randomstart){#
      pi = rgamma(k,1,1)#
    } else {#
      if(k<n){#
        pi=rep(1,k)/n #default initialization strongly favours null; puts weight 1/n on everything except null#
        pi[null.comp] = (n-k+1)/n #the motivation is data can quickly drive away from null, but tend to drive only slowly toward null.#
      } else {#
        pi=rep(1,k)/k#
      }#
    }#
    pi=normalize(pi)#
    if(!is.element(mixcompdist,c("normal","uniform","halfuniform"))) stop("Error: invalid type of mixcompdist")#
    if(mixcompdist=="normal") g=normalmix(pi,rep(0,k),mixsd)#
    if(mixcompdist=="uniform") g=unimix(pi,-mixsd,mixsd)#
    if(mixcompdist=="halfuniform"){#
      g = unimix(c(pi,pi)/2,c(-mixsd,rep(0,k)),c(rep(0,k),mixsd))#
      prior = rep(prior, 2)#
      pi = rep(pi, 2)#
    }#
  }#
  pi.fit=EMest(betahat[completeobs],lambda1*sebetahat[completeobs]+lambda2,g,prior,null.comp=null.comp,nullcheck=nullcheck,VB=VB,maxiter = maxiter, cxx=cxx, df=df)  #
  if (!onlylogLR){#
      n=length(betahat)#
      if (!multiseqoutput){#
          ZeroProb = rep(0,length=n)#
          NegativeProb = rep(0,length=n)#
      }#
      if (!minimaloutput){#
          PosteriorMean = rep(0,length=n)#
          PosteriorSD = rep(0,length=n)#
      }#
      if (!multiseqoutput){#
            ZeroProb[completeobs] = colSums(comppostprob(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)[comp_sd(pi.fit$g)==0,,drop=FALSE])     #
              NegativeProb[completeobs] = cdf_post(pi.fit$g, 0, betahat[completeobs],sebetahat[completeobs],df) - ZeroProb[completeobs]#
          }#
          if (!minimaloutput){#
              PosteriorMean[completeobs] = postmean(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
              PosteriorSD[completeobs] = postsd(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
      }#
                                        #FOR MISSING OBSERVATIONS, USE THE PRIOR INSTEAD OF THE POSTERIOR#
      if (!multiseqoutput){#
          ZeroProb[!completeobs] = sum(mixprop(pi.fit$g)[comp_sd(pi.fit$g)==0])#
          NegativeProb[!completeobs] = mixcdf(pi.fit$g,0)#
          lfsr = compute_lfsr(NegativeProb,ZeroProb)#
      }#
      if (!minimaloutput){#
          PosteriorMean[!completeobs] = mixmean(pi.fit$g)#
          PosteriorSD[!completeobs] = mixsd(pi.fit$g)#
      }#
      if (!minimaloutput & !multiseqoutput){#
          PositiveProb = 1- NegativeProb-ZeroProb#
          lfsra = compute_lfsra(PositiveProb,NegativeProb,ZeroProb) #
          lfdr = ZeroProb#
          qvalue = qval.from.lfdr(lfdr)#
      }#
  }#
  if (!minimaloutput)#
      logLR = tail(pi.fit$loglik,1) - pi.fit$null.loglik#
#
  if(nonzeromean & is.null(df)){#
      #Adding back the nonzero mean#
      betahat[completeobs]= betahat[completeobs]+nonzeromean.fit$nonzeromean#
      pi.fit$g$mean =nonzeromean.fit$nonzeromean#
      PosteriorMean= PosteriorMean + nonzeromean.fit$nonzeromean      #
  }	    #
  if (onlylogLR){#
      return(list(fitted.g=pi.fit$g, logLR = logLR, df=df))}#
  else if (minimaloutput){#
      return(list(fitted.g = pi.fit$g, lfsr = lfsr, fit = pi.fit, df=df))}#
  else if (multiseqoutput){#
      return(list(fitted.g = pi.fit$g, logLR = logLR, PosteriorMean = PosteriorMean, PosteriorSD = PosteriorSD, call= match.call(), df=df))}#
  else{#
      result = list(fitted.g = pi.fit$g, logLR = logLR, PosteriorMean = PosteriorMean, PosteriorSD = PosteriorSD, PositiveProb = PositiveProb, NegativeProb = NegativeProb, ZeroProb = ZeroProb, lfsr = lfsr,lfsra = lfsra, lfdr = lfdr, qvalue = qvalue, fit = pi.fit, lambda1 = lambda1, lambda2 = lambda2, call = match.call(), data = list(betahat = betahat, sebetahat=sebetahat), df=df)#
      class(result) = "ash"#
      return(result)#
  }#
}#
  #if(nsamp>0){#
  #  sample = posterior_sample(post,nsamp)#
  #}#
#
# #' @title Faster version of function ash#
# #'#
# #' @description This function has similar functionality as ash, but only returns some of the outputs.#
# #'#
# #' @param betahat, a p vector of estimates#
# #' @param sebetahat, a p vector of corresponding standard errors#
# #' @param nullcheck: whether to check that any fitted model exceeds the "null" likelihood in which all weight is on the first component#
# #' @param randomstart: bool, indicating whether to initialize EM randomly. If FALSE, then initializes to prior mean (for EM algorithm) or prior (for VBEM)#
# #' @param pointmass: bool, indicating whether to use a point mass at zero as one of components for a mixture distribution#
# #' @param onlylogLR: bool, indicating whether to use this function to get logLR. Skip posterior prob, posterior mean, lfdr...#
# #' @param prior: string, or numeric vector indicating Dirichlet prior on mixture proportions (defaults to "uniform", or 1,1...,1; also can be "nullbiased" 1,1/k-1,...,1/k-1 to put more weight on first component)#
# #' @param mixsd: vector of sds for underlying mixture components#
# #' @param VB: whether to use Variational Bayes to estimate mixture proportions (instead of EM to find MAP estimate)#
# #' @param gridmult: the multiplier by which the default grid values for mixsd differ by one another. (Smaller values produce finer grids)#
# #' @param g: the prior distribution for beta (usually estimated from the data; this is used primarily in simulated data to do computations with the "true" g)#
# #' @param cxx: flag to indicate whether to use the c++ (Rcpp) version#
# #'#
# #' @return a list with elements fitted.g is fitted mixture#
# #' logLR : logP(D|mle(pi)) - logP(D|null)#
# #'#
# #' @export#
# fast.ash = function(betahat,sebetahat, #
#                     nullcheck=TRUE,randomstart=FALSE, #
#                     pointmass = TRUE,    #
#                     prior=c("nullbiased","uniform"), #
#                     mixsd=NULL, VB=FALSE,gridmult=4,#
#                     g=NULL, cxx=TRUE,#
#                     onlylogLR = FALSE,df=NULL){#
#   #
#   if(onlylogLR){#
#     pointmass <- TRUE  #
#   }#
#   #
#   #If method is supplied, use it to set up defaults; provide warning if these default values#
#   #are also specified by user#
#   if(!is.numeric(prior)){#
#     prior = match.arg(prior)#
#   }#
#   #
#   if(length(sebetahat)==1){#
#     sebetahat = rep(sebetahat,length(betahat))#
#   }#
#   if(length(sebetahat) != length(betahat)){#
#     stop("Error: sebetahat must have length 1, or same length as betahat")#
#   }#
#   #
#   completeobs = (!is.na(betahat) & !is.na(sebetahat))#
#   if(sum(completeobs)==0){#
#     if(onlylogLR){#
#       return(list(pi=NULL, logLR = 0))#
#     }else{#
#       stop("Error: all input values are missing")#
#     }#
#   }  #
#   #
#   if(is.null(mixsd)){#
#     mixsd= autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)#
#   }#
#   if(pointmass){#
#     mixsd = c(0,mixsd)#
#   }#
#   #
#   k=length(mixsd)  #
#   null.comp = which.min(mixsd) #which component is the "null"#
#   #
#   if(!is.numeric(prior)){#
#     if(prior=="nullbiased"){ # set up prior to favour "null"#
#       prior = rep(1,k)#
#       prior[null.comp] = 10 #prior 10-1 in favour of null#
#     }else if(prior=="uniform"){#
#       prior = rep(1,k)#
#     }#
#   }#
#   #
#   if(length(prior)!=k | !is.numeric(prior)){#
#     stop("invalid prior specification")#
#   }#
#   #
#   if(missing(g)){#
#     pi = prior^2 #default is to initialize pi at prior (mean)#
#     if(randomstart){pi=rgamma(k,1,1)}#
#     pi=normalize(pi)#
#     g=normalmix(pi,rep(0,k),mixsd)#
#     maxiter = 5000#
#   } else {#
#     maxiter = 1; # if g is specified, don't iterate the EM #
#   }#
#   #
#   pi.fit=EMest(betahat[completeobs],sebetahat[completeobs],g,prior,null.comp=null.comp,nullcheck=nullcheck,VB=VB,maxiter = maxiter, cxx=cxx, df=df)  #
#   #
#   if(onlylogLR){#
#     logLR = tail(pi.fit$loglik,1) - pi.fit$null.loglik#
#     return(list(pi=pi.fit$pi, logLR = logLR))#
#   }else{#
#     #
#     n=length(betahat)#
#     PosteriorMean = rep(0,length=n)#
#     PosteriorSD=rep(0,length=n)#
#     #
#     if(is.null(df)){#
#       PosteriorMean[completeobs] = postmean(pi.fit$g,betahat[completeobs],sebetahat[completeobs])#
#       PosteriorSD[completeobs] =postsd(pi.fit$g,betahat[completeobs],sebetahat[completeobs]) #
#     }#
#     else{#
#       PosteriorMean[completeobs] = postmean_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
#       PosteriorSD[completeobs] =postsd_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
#     }#
#     #FOR MISSING OBSERVATIONS, USE THE PRIOR INSTEAD OF THE POSTERIOR#
#     PosteriorMean[!completeobs] = mixmean(pi.fit$g)#
#     PosteriorSD[!completeobs] =mixsd(pi.fit$g)  #
#     #
#     result = list(fitted.g=pi.fit$g,PosteriorMean = PosteriorMean,PosteriorSD=PosteriorSD,call=match.call(),data=list(betahat = betahat, sebetahat=sebetahat))#
#     return(result)#
#   }#
#   #if(nsamp>0){#
#   #  sample = posterior_sample(post,nsamp)#
#   #}#
# }#
compute_lfsr = function(NegativeProb,ZeroProb){#
  ifelse(NegativeProb> 0.5*(1-ZeroProb),1-NegativeProb,NegativeProb+ZeroProb)#
}#
#
compute_lfsra = function(PositiveProb, NegativeProb,ZeroProb){#
  ifelse(PositiveProb<NegativeProb,2*PositiveProb+ZeroProb,2*NegativeProb+ZeroProb)  #
}  #
#
#' @title Estimate unimodal nonzero mean of a mixture model by EM algorithm#
#'#
#' @description Given the data, standard error of the data and standard deviations of the Gaussian mixture model, estimate the mean of a unimodal Gaussian mixture by an EM algorithm.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates unimodal mean \eqn{\mu} by EM algorithm. Uses the SQUAREM package to accelerate convergence of EM. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' #
#' @param betahat, a p vector of estimates #
#' @param sebetahat, a p vector of corresponding standard errors#
#' @param mixsd: vector of sds for underlying mixture components #
#' @param pi.init, the initial value of \eqn{\pi} to use. If not specified defaults to (1/k,...,1/k).#
#' @param tol, the tolerance for convergence of log-likelihood.#
#' @param maxiter the maximum number of iterations performed#
#' #
#' @return A list, including the estimates (\eqn{\mu}) and (\eqn{\pi}), the log likelihood for each iteration (NQ)#
#' and a flag to indicate convergence#
#'  #
#' @export#
#' #
#' #
nonzeromeanEM = function(betahat, sebetahat, mixsd, pi.init=NULL,tol=1e-7,maxiter=5000){#
  if(is.null(pi.init)){#
    pi.init = rep(1/length(mixsd),length(mixsd))# Use as starting point for pi#
  }#
  mupi=c(mean(betahat),pi.init)#
  res=squarem(par=mupi,fixptfn=nonzeromeanEMfixpoint,objfn=nonzeromeanEMobj,betahat=betahat,sebetahat=sebetahat,mixsd=mixsd,control=list(maxiter=maxiter,tol=tol))#
  return(list(nonzeromean=res$par[1],pi=res$par[-1],NQ=-res$value.objfn,niter = res$iter, converged=res$convergence,post=res$par))#
}#
#
nonzeromeanEMfixpoint = function(mupi,betahat,sebetahat,mixsd){#
	#omegamatrix=matrix(NA,nrow=length(betahat),ncol=length(mixsd))#
	mu=mupi[1]#
	pimean=mupi[-1]#
	sdmat = sqrt(outer(sebetahat ^2,mixsd^2,"+")) #
	xmat=matrix(rep(betahat,length(mixsd)),ncol=length(mixsd))#
	omegamatrix=t(t(dnorm(xmat,mean=mu,sd=sdmat))*pimean)#
	omegamatrix=omegamatrix /rowSums(omegamatrix)#
	pinew=normalize(colSums(omegamatrix))#
	munew=sum(omegamatrix*xmat/(sdmat^2))/sum(omegamatrix/(sdmat^2))#
	mupi=c(munew,pinew)#
	return(mupi)#
}#
#
nonzeromeanEMobj = function(mupi,betahat,sebetahat,mixsd){#
	mu=mupi[1]#
	pimean=mupi[-1]#
	sdmat = sqrt(outer(sebetahat ^2,mixsd^2,"+")) #
	xmat=matrix(rep(betahat,length(mixsd)),ncol=length(mixsd))#
	omegamatrix=t(t(dnorm(xmat,mean=mu,sd=sdmat))*pimean)#
	omegamatrix=omegamatrix /rowSums(omegamatrix)#
	NegativeQ=-sum(omegamatrix*dnorm(xmat,mean=mu,sd=sdmat,log=TRUE))#
	return(NegativeQ)#
}#
#' @title Estimate posterior distribution on mixture proportions of a mixture model by a Variational Bayes EM algorithm#
#'#
#' @description Given the individual component likelihoods for a mixture model, estimates the posterior on #
#' the mixture proportions by an VBEM algorithm. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates posterior on mixture proportions \eqn{\pi} by Variational Bayes, #
#' with a Dirichlet prior on \eqn{\pi}. #
#' Algorithm adapted from Bishop (2009), Pattern Recognition and Machine Learning, Chapter 10.#
#' #
#' @param matrix_lik: a n by k matrix with (j,k)th element equal to \eqn{f_k(x_j)}.#
#' @param prior: a k vector of the parameters of the Dirichlet prior on \eqn{\pi}. Recommended to be rep(1,k)#
#' @param post.init: the initial value of the posterior parameters. If not specified defaults to the prior parameters.#
#' @param tol: the tolerance for convergence of log-likelihood bound.#
#' @param maxiter: the maximum number of iterations performed#
#' #
#' @return A list, whose components include point estimates (pihat), #
#' the parameters of the fitted posterior on \eqn{\pi} (pipost),#
#' the bound on the log likelihood for each iteration (B)#
#' and a flag to indicate convergence (converged).#
#'  #
#' @export#
#' #
mixVBEM = function(matrix_lik, prior, pi.init = NULL,tol=1e-7, maxiter=5000){#
  k=ncol(matrix_lik)#
  if(is.null(pi.init)){#
    pi.init = rep(1,k)# Use as starting point for pi#
  } #
  res = squarem(par=pi.init,fixptfn=VBfixpoint, objfn=VBnegpenloglik,matrix_lik=matrix_lik, prior=prior, control=list(maxiter=maxiter,tol=tol))#
  return(list(pihat = res$par/sum(res$par), B=res$value.objfn, niter = res$iter, converged=res$convergence,post=res$par))#
}#
VBfixpoint = function(pipost, matrix_lik, prior){  #
  n=nrow(matrix_lik)#
  k=ncol(matrix_lik)#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost*matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  pipostnew = colSums(classprob) + prior#
  return(pipostnew)#
}#
#
VBnegpenloglik=function(pipost,matrix_lik,prior){#
  return(-VBpenloglik(pipost,matrix_lik,prior))#
}#
#
VBpenloglik = function(pipost, matrix_lik, prior){#
  n=nrow(matrix_lik)#
  k=ncol(matrix_lik)#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost*matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  B= sum(classprob*log(avgpipost*matrix_lik),na.rm=TRUE) - diriKL(prior,pipost) - sum(classprob*log(classprob)) #
  return(B)#
}#
#' @title Estimate mixture proportions of a mixture model by EM algorithm#
#'#
#' @description Given the individual component likelihoods for a mixture model, estimates the mixture proportions by an EM algorithm.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates mixture proportions \eqn{\pi} by maximum likelihood, or by maximum a posteriori (MAP) estimation for a Dirichlet prior on $\pi$ #
#' (if a prior is specified).  Uses the SQUAREM package to accelerate convergence of EM. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' #
#' @param matrix_lik, a n by k matrix with (j,k)th element equal to \eqn{f_k(x_j)}.#
#' @param prior, a k vector of the parameters of the Dirichlet prior on \eqn{\pi}. Recommended to be rep(1,k)#
#' @param pi.init, the initial value of \eqn{\pi} to use. If not specified defaults to (1/k,...,1/k).#
#' @param tol, the tolerance for convergence of log-likelihood.#
#' @param maxiter the maximum number of iterations performed#
#' #
#' @return A list, including the estimates (pihat), the log likelihood for each interation (B)#
#' and a flag to indicate convergence#
#'  #
#' @export#
#' #
#' #
mixEM = function(matrix_lik, prior, pi.init = NULL,tol=1e-7, maxiter=5000){#
  if(is.null(pi.init)){#
    pi.init = rep(1/k,k)# Use as starting point for pi#
  } #
  res = squarem(par=pi.init,fixptfn=fixpoint, objfn=negpenloglik,matrix_lik=matrix_lik, prior=prior, control=list(maxiter=maxiter,tol=tol))#
  return(list(pihat = normalize(pmax(0,res$par)), B=res$value.objfn, #
              niter = res$iter, converged=res$convergence))#
}#
#
# helper functions used by mixEM#
normalize = function(x){return(x/sum(x))}#
#
fixpoint = function(pi, matrix_lik, prior){  #
  pi = normalize(pmax(0,pi)) #avoid occasional problems with negative pis due to rounding#
  m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
  m.rowsum = rowSums(m)#
  classprob = m/m.rowsum #an n by k matrix#
  pinew = normalize(colSums(classprob) + prior - 1)#
  return(pinew)#
}#
#
negpenloglik = function(pi,matrix_lik,prior){return(-penloglik(pi,matrix_lik,prior))}#
#
penloglik = function(pi, matrix_lik, prior){#
  pi = normalize(pmax(0,pi))#
  m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
  m.rowsum = rowSums(m)#
  loglik = sum(log(m.rowsum))#
  subset = (prior != 1.0)#
  priordens = sum((prior-1)[subset]*log(pi[subset]))#
  return(loglik+priordens)#
}#
#
#The kth element of this vector is the derivative #
#of the loglik for $\pi=(\pi_0,...,1-\pi_0,...)$ with respect to $\pi_0$ at $\pi_0=1$.#
gradient = function(matrix_lik){#
  n = nrow(matrix_lik)#
  grad = n - colSums(matrix_lik/matrix_lik[,1]) #
  return(grad)#
}#
#
# mixEM = function(matrix_lik, prior, pi.init = NULL,tol=0.0001, maxiter=5000){#
#   n=nrow(matrix_lik)#
#   k=ncol(matrix_lik)#
#   B = rep(0,maxiter)#
#   pi = pi.init#
#   if(is.null(pi.init)){#
#     pi = rep(1/k,k)# Use as starting point for pi#
#   } #
#   pi = ifelse(pi<1e-5,1e-5,pi) #set any estimates that are too small to be just very small#
#   pi = normalize(pi)#
#   #
#   loglik = rep(0,maxiter)#
#   priordens= rep(0,maxiter)#
#   m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
#   m.rowsum = rowSums(m)#
#   loglik[1] = sum(log(m.rowsum))#
#   priordens[1] = sum((prior-1)*log(pi)) #
#   classprob = m/m.rowsum #an n by k matrix#
#   i=1#
#   if(maxiter >= 2){#
#     for(i in 2:maxiter){  #
#       pi = colSums(classprob) + prior-1#
#       pi = ifelse(pi<1e-5,1e-5,pi) #set any estimates that are less than zero, which can happen with prior<1, to 0#
#       pi = normalize(pi)#
#         #
#       #Now re-estimate pi#
#       m  = t(pi * t(matrix_lik)) #
#       m.rowsum = rowSums(m)#
#       loglik[i] = sum(log(m.rowsum))#
#       priordens[i] = sum((prior-1)*log(pi)) #
#       classprob = m/m.rowsum#
#     #
#     #
#       if(abs(loglik[i]+priordens[i]-loglik[i-1]-priordens[i-1])<tol) break;#
#     }#
#   }#
#   converged=(abs(loglik[i]+priordens[i]-loglik[i-1]-priordens[i-1])<tol)#
#   if(!converged){#
#       warning("EM algorithm in function mixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
#   }#
#   return(list(pihat = pi, B=loglik[1:i], #
#               niter = i, converged=converged))#
# }#
#estimate mixture proportions of sigmaa by EM algorithm#
#prior gives the parameter of a Dirichlet prior on pi#
#(prior is used to encourage results towards smallest value of sigma when#
#likelihood is flat)#
#nullcheck indicates whether to check whether the loglike exceeds the null#
#(may not want to use if prior is used)#
#VB provides an approach to estimate the approximate posterior distribution#
#of mixture proportions of sigmaa by variational Bayes method#
#(use Dirichlet prior and approximate Dirichlet posterior)#
#if cxx TRUE use cpp version of R function mixEM#
EMest = function(betahat,sebetahat,g,prior,null.comp=1,nullcheck=TRUE,VB=FALSE, maxiter=5000, cxx=TRUE, df=NULL){ #
  pi.init = g$pi#
  k=ncomp(g)#
  n = length(betahat)#
  tol = min(0.1/n,1e-5) # set convergence criteria to be more stringent for larger samples#
  if(is.null(df)){#
    matrix_lik = t(compdens_conv(g,betahat,sebetahat))#
  }#
  else{#
    matrix_lik = t(compdens_conv_t(g,betahat,sebetahat,df))#
  }#
  #checks whether the gradient at pi0=1 is positive (suggesting that this is a fixed point)#
  #if(nullcheck){#
  #  if(all(gradient(matrix_lik)>=0)){#
  #    pi.init=rep(0,k)#
  #    pi.init[null.comp]=1 #this will make pi.init=(1,0,0...,0) which is a fixed point of the EM#
  #  }#
  #}#
  if(VB==TRUE){#
    EMfit=mixVBEM(matrix_lik,prior,maxiter=maxiter)}#
  else{#
    if (cxx==TRUE){#
      EMfit = cxxMixEM(matrix_lik,prior,pi.init,1e-5, maxiter) #currently use different convergence criteria for cxx version #
      if(!EMfit$converged){#
        warning("EM algorithm in function cxxMixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
      }#
    }#
    else{#
      EMfit = mixEM(matrix_lik,prior,pi.init,tol, maxiter)#
      if(!EMfit$converged & !(maxiter==1)){#
        warning("EM algorithm in function mixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
      }#
    }#
  }#
  pi = EMfit$pihat     #
  penloglik = EMfit$B #
  converged = EMfit$converged#
  niter = EMfit$niter#
  loglik.final =  penloglik(pi,matrix_lik,1) #compute penloglik without penalty#
  null.loglik = sum(log(matrix_lik[,null.comp]))  #
  if(nullcheck==TRUE & VB==FALSE){ #null check doesn't work with VB yet#
    pinull = rep(0,k)#
    pinull[null.comp]=1#
    null.penloglik = penloglik(pinull,matrix_lik,prior)#
    final.penloglik = penloglik(pi,matrix_lik,prior)#
    if(null.penloglik > final.penloglik){ #check whether exceeded "null" likelihood where everything is null#
      pi=pinull#
      loglik.final=penloglik(pi,matrix_lik,1)#
    }#
  }#
  g$pi=pi#
  return(list(loglik=loglik.final,null.loglik=null.loglik,#
              matrix_lik=matrix_lik,converged=converged,g=g))#
}#
#' @title Compute Posterior#
#'#
#' @description Return the posterior on beta given a prior (g) that is a mixture of normals (class normalmix) #
#' and observation betahat \sim N(beta,sebetahat)#
#'#
#' @details This can be used to obt#
#'#
#' @param g: a normalmix with components indicating the prior; works only if g has means 0#
#' @param betahat (n vector of observations) #
#' @param sebetahat (n vector of standard errors/deviations of observations)#
#' #
#' @return A list, (pi1,mu1,sigma1) whose components are each k by n matrices#
#' where k is number of mixture components in g, n is number of observations in betahat#
#' #
#' @export#
#' #
#' #
posterior_dist = function(g,betahat,sebetahat){#
  if(class(g)!="normalmix"){#
    stop("Error: posterior_dist implemented only for g of class normalmix")#
  }#
  pi0 = g$pi#
  mu0 = g$mean#
  sigma0 = g$sd  #
  k= length(pi0)#
  n= length(betahat)#
  if(!all.equal(g$mean,rep(0,k))) stop("Error: posterior_dist currently only implemented for zero-centered priors")#
  pi1 = pi0 * t(matrix_dens(betahat,sebetahat,sigma0))#
  pi1 = apply(pi1, 2, normalize) #pi1 is now an k by n matrix#
  #make k by n matrix versions of sigma0^2 and sebetahat^2#
  # and mu0 and betahat#
  s0m2 = matrix(sigma0^2,nrow=k,ncol=n,byrow=FALSE)#
  sebm2 = matrix(sebetahat^2,nrow=k,ncol=n, byrow=TRUE)#
  mu0m = matrix(mu0,nrow=k,ncol=n,byrow=FALSE)#
  bhatm = matrix(betahat,nrow=k,ncol=n,byrow=TRUE)#
  sigma1 = (s0m2*sebm2/(s0m2 + sebm2))^(0.5)  #
  w = sebm2/(s0m2 + sebm2)#
  mu1 = w*mu0m + (1-w)*bhatm#
  #WHERE DATA ARE MISSING, SET POSTERIOR = PRIOR#
  ismiss = (is.na(betahat) | is.na(sebetahat)) #
  pi1[,ismiss] = pi0#
  mu1[,ismiss] = mu0#
  sigma1[,ismiss] = sigma0#
  return(list(pi=pi1,mu=mu1,sigma=sigma1))#
}#
#
#return matrix of densities of observations (betahat) #
# assuming betahat_j \sim N(0, sebetahat_j^2 + sigmaavec_k^2)#
#normalized by maximum of each column#
#INPUT#
#betahat is n vector, #
#sebetahat is n vector, #
#sigmaavec is k vector#
#return is n by k matrix of the normal likelihoods, #
# with (j,k)th element the density of N(betahat_j; mean=0, var = sebetahat_j^2 + sigmaavec_k^2)#
#normalized to have maximum 1 in each column#
matrix_dens = function(betahat, sebetahat, sigmaavec){#
  k = length(sigmaavec)#
  n = length(betahat)#
  ldens = dnorm(betahat,0,sqrt(outer(sebetahat^2,sigmaavec^2,FUN="+")),log=TRUE)#
  maxldens = apply(ldens, 1, max)#
  ldens = ldens - maxldens#
  return(exp(ldens))#
}#
#
#return the "effective" estimate#
#that is the effect size betanew whose z score betanew/se#
#would give the same p value as betahat/se compared to a t with df#
effective.effect=function(betahat,se,df){#
  p = pt(betahat/se,df)#
  qnorm(p,sd=se)#
}#
#' @title Function to compute q values from local false discovery rates#
#'#
#' @description Computes q values from a vector of local fdr estimates#
#'#
#' @details The q value for a given lfdr is an estimate of the (tail) False Discovery Rate #
#' for all findings with a smaller lfdr, and is found by the average of the lfdr for#
#' all more significant findings. See Storey (2003), Annals of Statistics, for definition of q value.  #
#' #
#' #
#' @param lfdr, a vector of local fdr estimates#
#'#
#' @return vector of q values#
#' #
#' @export#
qval.from.lfdr = function(lfdr){#
  o = order(lfdr)#
  qvalue=rep(NA,length(lfdr))#
  qvalue[o] = (cumsum(sort(lfdr))/(1:sum(!is.na(lfdr))))#
  return(qvalue)#
}#
#
# try to select a default range for the sigmaa values#
# that should be used, based on the values of betahat and sebetahat#
# mult is the multiplier by which the sds differ across the grid#
autoselect.mixsd = function(betahat,sebetahat,mult){#
  sigmaamin = min(sebetahat)/10 #so that the minimum is small compared with measurement precision#
  if(all(betahat^2<sebetahat^2)){#
    sigmaamax = 8*sigmaamin #to deal with the occassional odd case where this could happen; 8 is arbitrary#
  } else {#
    sigmaamax = 2*sqrt(max(betahat^2-sebetahat^2)) #this computes a rough largest value you'd want to use, based on idea that sigmaamax^2 + sebetahat^2 should be at least betahat^2   #
  }#
  if(mult==0){#
    return(c(0,sigmaamax/2))#
  }else{#
    npoint = ceiling(log2(sigmaamax/sigmaamin)/log2(mult))#
    return(mult^((-npoint):0) * sigmaamax)#
  }#
}#
#return the KL-divergence between 2 dirichlet distributions#
#p,q are the vectors of dirichlet parameters of same lengths#
diriKL = function(p,q){#
  p.sum = sum(p)#
  q.sum = sum(q)#
  k = length(q)#
  KL = lgamma(q.sum)-lgamma(p.sum)+sum((q-p)*(digamma(q)-digamma(rep(q.sum,k))))+sum(lgamma(p)-lgamma(q))#
  return(KL)#
}#
#
#helper function for VBEM#
VB.update = function(matrix_lik, pipost){#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost * matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  B = sum(classprob*log(avgpipost*matrix_lik),na.rm=TRUE) - diriKL(prior,pipost) #negative free energy#
  return(list(classprob=classprob,B=B))#
}
' @useDynLib ashr#
#todo#
##
#' @title Main Adaptive SHrinkage function#
#'#
#' @description Takes vectors of estimates (betahat) and their standard errors (sebetahat), and applies#
#' shrinkage to them, using Empirical Bayes methods, to compute shrunk estimates for beta.#
#'#
#' @details See readme for more details#
#' #
#' @param betahat, a p vector of estimates #
#' @param sebetahat, a p vector of corresponding standard errors#
#' @param method: specifies how ash is to be run. Can be "shrinkage" (if main aim is shrinkage) or "fdr" (if main aim is to assess fdr or fsr)#
#' This is simply a convenient way to specify certain combinations of parameters: "shrinkage" sets pointmass=FALSE and prior="uniform";#
#' "fdr" sets pointmass=TRUE and prior="nullbiased".#
#' @param mixcompdist: distribution of components in mixture ("normal", "uniform" or "halfuniform")#
#'#
#' @param lambda1: multiplicative "inflation factor" for standard errors (like Genomic Control)#
#' @param lambda2: additive "inflation factor" for standard errors (like Genomic Control)#
#' @param nullcheck: whether to check that any fitted model exceeds the "null" likelihood#
#' in which all weight is on the first component#
#' @param df: appropriate degrees of freedom for (t) distribution of betahat/sebetahat#
#' @param randomstart: bool, indicating whether to initialize EM randomly. If FALSE, then initializes to prior mean (for EM algorithm) or prior (for VBEM)#
#' @param nullweight: scalar, the weight put on the prior of null under "fdr" method#
#' @param nonzeromean: bool, indicating whether to use a nonzero mean unimodal mixture(defaults to "FALSE")#
#' @param pointmass: bool, indicating whether to use a point mass at zero as one of components for a mixture distribution#
#' @param onlylogLR: bool, indicating whether to use this function to get logLR. Skip posterior prob, posterior mean, lfdr...#
#' @param prior: string, or numeric vector indicating Dirichlet prior on mixture proportions (defaults to "uniform", or 1,1...,1; also can be "nullbiased" 1,1/k-1,...,1/k-1 to put more weight on first component)#
#' @param mixsd: vector of sds for underlying mixture components #
#' @param VB: whether to use Variational Bayes to estimate mixture proportions (instead of EM to find MAP estimate)#
#' @param gridmult: the multiplier by which the default grid values for mixsd differ by one another. (Smaller values produce finer grids)#
#' @param minimal_output: if TRUE, just outputs the fitted g and the lfsr (useful for very big data sets where memory is an issue) #
#' @param g: the prior distribution for beta (usually estimated from the data; this is used primarily in simulated data to do computations with the "true" g)#
#' @param maxiter: maximum number of iterations of the EM algorithm#
#' @param cxx: flag to indicate whether to use the c++ (Rcpp) version#
#' #
#'#
#' @return a list with elements fitted.g is fitted mixture#
#' logLR : logP(D|mle(pi)) - logP(D|null)#
#' #
#' @export#
#' #
#' @examples #
#' beta = c(rep(0,100),rnorm(100))#
#' sebetahat = abs(rnorm(200,0,1))#
#' betahat = rnorm(200,beta,sebetahat)#
#' beta.ash = ash(betahat, sebetahat)#
#' summary(beta.ash)#
#' plot(betahat,beta.ash$PosteriorMean,xlim=c(-4,4),ylim=c(-4,4))#
#' #
#' #
#Things to do:#
# check sampling routine#
# check number of iterations
ash = function(betahat,sebetahat,method = c("shrink","fdr"), #
               mixcompdist = c("normal","uniform","halfuniform"),#
               lambda1=1,lambda2=0,nullcheck=TRUE,df=NULL,randomstart=FALSE,#
               nullweight=10,nonzeromean=FALSE, #
               pointmass = FALSE, #
               onlylogLR = FALSE, #
               prior=c("uniform","nullbiased"), #
               mixsd=NULL, VB=FALSE,gridmult=sqrt(2),#
               minimaloutput=FALSE,#
               multiseqoutput=FALSE,#
               g=NULL,#
               maxiter = 5000,#
               cxx=FALSE){#
  #method provides a convenient interface to set a particular combinations of parameters for prior an#
  #If method is supplied, use it to set up specific values for these parameters; provide warning if values#
  #are also specified by user#
  #If method is not supplied use the user-supplied values (or defaults if user does not specify them)#
  if(!missing(method)){#
    method = match.arg(method) #
    if(method=="shrink"){#
      if(missing(prior)){#
        prior = "uniform"#
      } else {#
        warning("Specification of prior overrides default for method shrink")#
      }#
      if(missing(pointmass)){#
        pointmass=FALSE#
      } else {#
        warning("Specification of pointmass overrides default for method shrink")#
      }#
    }#
    if(method=="fdr"){#
      if(missing(prior)){#
        prior = "nullbiased"#
      } else {#
        warning("Specification of prior overrides default for method fdr")#
      }#
      if(missing(pointmass)){#
        pointmass=TRUE#
      } else {#
        warning("Specification of pointmass overrides default for method fdr")#
      }#
    }  #
  }#
  if(gridmult<=1&multiseqoutput!=TRUE)#
    stop("gridmult must be > 1")#
  mixcompdist = match.arg(mixcompdist)#
  # if(mixcompdist=="uniform" & pointmass==TRUE){#
  #    stop("point mass not yet implemented for uniform or half-uniform")#
  #  }#
  #  if(mixcompdist=="halfuniform" & pointmass==TRUE){#
  #    stop("point mass not yet implemented for uniform or half-uniform")#
  #  }#
  if(!is.numeric(prior)){#
    prior = match.arg(prior)#
  }  #
  if(length(sebetahat)==1){#
    sebetahat = rep(sebetahat,length(betahat))#
  }#
  if(length(sebetahat) != length(betahat)){#
    stop("Error: sebetahat must have length 1, or same length as betahat")#
  }#
  completeobs = (!is.na(betahat) & !is.na(sebetahat))#
  n=sum(completeobs)#
  if(n==0){#
    if(onlylogLR){#
      return(list(pi=NULL, logLR = 0))#
    }#
    else{#
      stop("Error: all input values are missing")#
    }#
  }  #
  if(!is.null(g)){#
    maxiter = 1 # if g is specified, don't iterate the EM#
    prior = rep(1,ncomp(g)) #prior is not actually used if g specified, but required to make sure EM doesn't produce warning#
    null.comp=1 #null.comp also not used, but required #
  } else {#
    if(is.null(mixsd)){#
      mixsd = autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)#
    }#
    if(pointmass){#
      mixsd = c(0,mixsd)#
    }#
    null.comp = which.min(mixsd) #which component is the "null"#
    k = length(mixsd)#
	if(nonzeromean & is.null(df)){#
		nonzeromean.fit=nonzeromeanEM(betahat[completeobs], sebetahat[completeobs], mixsd, maxiter=maxiter)#
		betahat[completeobs]= betahat[completeobs] - nonzeromean.fit$nonzeromean#
	}#
	elseif(nonzeromean & !is.null(df) ){#
		stop("Error: Nonzero mean only implemented for df=NULL")#
	}#
    if(!is.numeric(prior)){#
      if(prior=="nullbiased"){ # set up prior to favour "null"#
        prior = rep(1,k)#
        prior[null.comp] = nullweight #prior 10-1 in favour of null by default#
      }else if(prior=="uniform"){#
        prior = rep(1,k)#
      }#
    }#
    if(length(prior)!=k | !is.numeric(prior)){#
      stop("invalid prior specification")#
    }#
    if(randomstart){#
      pi = rgamma(k,1,1)#
    } else {#
      if(k<n){#
        pi=rep(1,k)/n #default initialization strongly favours null; puts weight 1/n on everything except null#
        pi[null.comp] = (n-k+1)/n #the motivation is data can quickly drive away from null, but tend to drive only slowly toward null.#
      } else {#
        pi=rep(1,k)/k#
      }#
    }#
    pi=normalize(pi)#
    if(!is.element(mixcompdist,c("normal","uniform","halfuniform"))) stop("Error: invalid type of mixcompdist")#
    if(mixcompdist=="normal") g=normalmix(pi,rep(0,k),mixsd)#
    if(mixcompdist=="uniform") g=unimix(pi,-mixsd,mixsd)#
    if(mixcompdist=="halfuniform"){#
      g = unimix(c(pi,pi)/2,c(-mixsd,rep(0,k)),c(rep(0,k),mixsd))#
      prior = rep(prior, 2)#
      pi = rep(pi, 2)#
    }#
  }#
  pi.fit=EMest(betahat[completeobs],lambda1*sebetahat[completeobs]+lambda2,g,prior,null.comp=null.comp,nullcheck=nullcheck,VB=VB,maxiter = maxiter, cxx=cxx, df=df)  #
  if (!onlylogLR){#
      n=length(betahat)#
      if (!multiseqoutput){#
          ZeroProb = rep(0,length=n)#
          NegativeProb = rep(0,length=n)#
      }#
      if (!minimaloutput){#
          PosteriorMean = rep(0,length=n)#
          PosteriorSD = rep(0,length=n)#
      }#
      if (!multiseqoutput){#
            ZeroProb[completeobs] = colSums(comppostprob(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)[comp_sd(pi.fit$g)==0,,drop=FALSE])     #
              NegativeProb[completeobs] = cdf_post(pi.fit$g, 0, betahat[completeobs],sebetahat[completeobs],df) - ZeroProb[completeobs]#
          }#
          if (!minimaloutput){#
              PosteriorMean[completeobs] = postmean(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
              PosteriorSD[completeobs] = postsd(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
      }#
                                        #FOR MISSING OBSERVATIONS, USE THE PRIOR INSTEAD OF THE POSTERIOR#
      if (!multiseqoutput){#
          ZeroProb[!completeobs] = sum(mixprop(pi.fit$g)[comp_sd(pi.fit$g)==0])#
          NegativeProb[!completeobs] = mixcdf(pi.fit$g,0)#
          lfsr = compute_lfsr(NegativeProb,ZeroProb)#
      }#
      if (!minimaloutput){#
          PosteriorMean[!completeobs] = mixmean(pi.fit$g)#
          PosteriorSD[!completeobs] = mixsd(pi.fit$g)#
      }#
      if (!minimaloutput & !multiseqoutput){#
          PositiveProb = 1- NegativeProb-ZeroProb#
          lfsra = compute_lfsra(PositiveProb,NegativeProb,ZeroProb) #
          lfdr = ZeroProb#
          qvalue = qval.from.lfdr(lfdr)#
      }#
  }#
  if (!minimaloutput)#
      logLR = tail(pi.fit$loglik,1) - pi.fit$null.loglik#
#
  if(nonzeromean & is.null(df)){#
      #Adding back the nonzero mean#
      betahat[completeobs]= betahat[completeobs]+nonzeromean.fit$nonzeromean#
      pi.fit$g$mean =nonzeromean.fit$nonzeromean#
      PosteriorMean= PosteriorMean + nonzeromean.fit$nonzeromean      #
  }	    #
  if (onlylogLR){#
      return(list(fitted.g=pi.fit$g, logLR = logLR, df=df))}#
  else if (minimaloutput){#
      return(list(fitted.g = pi.fit$g, lfsr = lfsr, fit = pi.fit, df=df))}#
  else if (multiseqoutput){#
      return(list(fitted.g = pi.fit$g, logLR = logLR, PosteriorMean = PosteriorMean, PosteriorSD = PosteriorSD, call= match.call(), df=df))}#
  else{#
      result = list(fitted.g = pi.fit$g, logLR = logLR, PosteriorMean = PosteriorMean, PosteriorSD = PosteriorSD, PositiveProb = PositiveProb, NegativeProb = NegativeProb, ZeroProb = ZeroProb, lfsr = lfsr,lfsra = lfsra, lfdr = lfdr, qvalue = qvalue, fit = pi.fit, lambda1 = lambda1, lambda2 = lambda2, call = match.call(), data = list(betahat = betahat, sebetahat=sebetahat), df=df)#
      class(result) = "ash"#
      return(result)#
  }#
}
ash = function(betahat,sebetahat,method = c("shrink","fdr"), #
               mixcompdist = c("normal","uniform","halfuniform"),#
               lambda1=1,lambda2=0,nullcheck=TRUE,df=NULL,randomstart=FALSE,#
               nullweight=10,nonzeromean=FALSE, #
               pointmass = FALSE, #
               onlylogLR = FALSE, #
               prior=c("uniform","nullbiased"), #
               mixsd=NULL, VB=FALSE,gridmult=sqrt(2),#
               minimaloutput=FALSE,#
               multiseqoutput=FALSE,#
               g=NULL,#
               maxiter = 5000,#
               cxx=FALSE){#
  #method provides a convenient interface to set a particular combinations of parameters for prior an#
  #If method is supplied, use it to set up specific values for these parameters; provide warning if values#
  #are also specified by user#
  #If method is not supplied use the user-supplied values (or defaults if user does not specify them)#
  if(!missing(method)){#
    method = match.arg(method) #
    if(method=="shrink"){#
      if(missing(prior)){#
        prior = "uniform"#
      } else {#
        warning("Specification of prior overrides default for method shrink")#
      }#
      if(missing(pointmass)){#
        pointmass=FALSE#
      } else {#
        warning("Specification of pointmass overrides default for method shrink")#
      }#
    }#
    if(method=="fdr"){#
      if(missing(prior)){#
        prior = "nullbiased"#
      } else {#
        warning("Specification of prior overrides default for method fdr")#
      }#
      if(missing(pointmass)){#
        pointmass=TRUE#
      } else {#
        warning("Specification of pointmass overrides default for method fdr")#
      }#
    }  #
  }#
  if(gridmult<=1&multiseqoutput!=TRUE)#
    stop("gridmult must be > 1")#
  mixcompdist = match.arg(mixcompdist)#
  # if(mixcompdist=="uniform" & pointmass==TRUE){#
  #    stop("point mass not yet implemented for uniform or half-uniform")#
  #  }#
  #  if(mixcompdist=="halfuniform" & pointmass==TRUE){#
  #    stop("point mass not yet implemented for uniform or half-uniform")#
  #  }#
  if(!is.numeric(prior)){#
    prior = match.arg(prior)#
  }  #
  if(length(sebetahat)==1){#
    sebetahat = rep(sebetahat,length(betahat))#
  }#
  if(length(sebetahat) != length(betahat)){#
    stop("Error: sebetahat must have length 1, or same length as betahat")#
  }#
  completeobs = (!is.na(betahat) & !is.na(sebetahat))#
  n=sum(completeobs)#
  if(n==0){#
    if(onlylogLR){#
      return(list(pi=NULL, logLR = 0))#
    }#
    else{#
      stop("Error: all input values are missing")#
    }#
  }  #
  if(!is.null(g)){#
    maxiter = 1 # if g is specified, don't iterate the EM#
    prior = rep(1,ncomp(g)) #prior is not actually used if g specified, but required to make sure EM doesn't produce warning#
    null.comp=1 #null.comp also not used, but required #
  } else {#
    if(is.null(mixsd)){#
      mixsd = autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)#
    }#
    if(pointmass){#
      mixsd = c(0,mixsd)#
    }#
    null.comp = which.min(mixsd) #which component is the "null"#
    k = length(mixsd)#
	if(nonzeromean & is.null(df)){#
		nonzeromean.fit=nonzeromeanEM(betahat[completeobs], sebetahat[completeobs], mixsd, maxiter=maxiter)#
		betahat[completeobs]= betahat[completeobs] - nonzeromean.fit$nonzeromean#
	}#
	elseif(nonzeromean & !is.null(df) ){#
		stop("Error: Nonzero mean only implemented for df=NULL")#
	}#
    if(!is.numeric(prior)){#
      if(prior=="nullbiased"){ # set up prior to favour "null"#
        prior = rep(1,k)#
        prior[null.comp] = nullweight #prior 10-1 in favour of null by default#
      }else if(prior=="uniform"){#
        prior = rep(1,k)#
      }#
    }#
    if(length(prior)!=k | !is.numeric(prior)){#
      stop("invalid prior specification")#
    }#
    if(randomstart){#
      pi = rgamma(k,1,1)#
    } else {#
      if(k<n){#
        pi=rep(1,k)/n #default initialization strongly favours null; puts weight 1/n on everything except null#
        pi[null.comp] = (n-k+1)/n #the motivation is data can quickly drive away from null, but tend to drive only slowly toward null.#
      } else {#
        pi=rep(1,k)/k#
      }#
    }#
    pi=normalize(pi)#
    if(!is.element(mixcompdist,c("normal","uniform","halfuniform"))) stop("Error: invalid type of mixcompdist")#
    if(mixcompdist=="normal") g=normalmix(pi,rep(0,k),mixsd)#
    if(mixcompdist=="uniform") g=unimix(pi,-mixsd,mixsd)#
    if(mixcompdist=="halfuniform"){#
      g = unimix(c(pi,pi)/2,c(-mixsd,rep(0,k)),c(rep(0,k),mixsd))#
      prior = rep(prior, 2)#
      pi = rep(pi, 2)#
    }#
  }#
  pi.fit=EMest(betahat[completeobs],lambda1*sebetahat[completeobs]+lambda2,g,prior,null.comp=null.comp,nullcheck=nullcheck,VB=VB,maxiter = maxiter, cxx=cxx, df=df)  #
  if (!onlylogLR){#
      n=length(betahat)#
      if (!multiseqoutput){#
          ZeroProb = rep(0,length=n)#
          NegativeProb = rep(0,length=n)#
      }#
      if (!minimaloutput){#
          PosteriorMean = rep(0,length=n)#
          PosteriorSD = rep(0,length=n)#
      }#
#      if (!multiseqoutput){#
#          ZeroProb[completeobs] = colSums(comppostprob(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)[comp_sd(pi.fit$g)==0,,drop=FALSE])     #
#          NegativeProb[completeobs] = cdf_post(pi.fit$g, 0, betahat[completeobs],sebetahat[completeobs],df) - ZeroProb[completeobs]#
#      }#
#      if (!minimaloutput){#
#          PosteriorMean[completeobs] = postmean(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
#          PosteriorSD[completeobs] = postsd(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
#      }#
                                        #FOR MISSING OBSERVATIONS, USE THE PRIOR INSTEAD OF THE POSTERIOR#
      if (!multiseqoutput){#
          ZeroProb[!completeobs] = sum(mixprop(pi.fit$g)[comp_sd(pi.fit$g)==0])#
          NegativeProb[!completeobs] = mixcdf(pi.fit$g,0)#
          lfsr = compute_lfsr(NegativeProb,ZeroProb)#
      }#
      if (!minimaloutput){#
          PosteriorMean[!completeobs] = mixmean(pi.fit$g)#
          PosteriorSD[!completeobs] = mixsd(pi.fit$g)#
      }#
      if (!minimaloutput & !multiseqoutput){#
          PositiveProb = 1- NegativeProb-ZeroProb#
          lfsra = compute_lfsra(PositiveProb,NegativeProb,ZeroProb) #
          lfdr = ZeroProb#
          qvalue = qval.from.lfdr(lfdr)#
      }#
  }#
  if (!minimaloutput)#
      logLR = tail(pi.fit$loglik,1) - pi.fit$null.loglik#
#
  if(nonzeromean & is.null(df)){#
      #Adding back the nonzero mean#
      betahat[completeobs]= betahat[completeobs]+nonzeromean.fit$nonzeromean#
      pi.fit$g$mean =nonzeromean.fit$nonzeromean#
      PosteriorMean= PosteriorMean + nonzeromean.fit$nonzeromean      #
  }	    #
  if (onlylogLR){#
      return(list(fitted.g=pi.fit$g, logLR = logLR, df=df))}#
  else if (minimaloutput){#
      return(list(fitted.g = pi.fit$g, lfsr = lfsr, fit = pi.fit, df=df))}#
  else if (multiseqoutput){#
      return(list(fitted.g = pi.fit$g, logLR = logLR, PosteriorMean = PosteriorMean, PosteriorSD = PosteriorSD, call= match.call(), df=df))}#
  else{#
      result = list(fitted.g = pi.fit$g, logLR = logLR, PosteriorMean = PosteriorMean, PosteriorSD = PosteriorSD, PositiveProb = PositiveProb, NegativeProb = NegativeProb, ZeroProb = ZeroProb, lfsr = lfsr,lfsra = lfsra, lfdr = lfdr, qvalue = qvalue, fit = pi.fit, lambda1 = lambda1, lambda2 = lambda2, call = match.call(), data = list(betahat = betahat, sebetahat=sebetahat), df=df)#
      class(result) = "ash"#
      return(result)#
  }#
}
ash = function(betahat,sebetahat,method = c("shrink","fdr"), #
               mixcompdist = c("normal","uniform","halfuniform"),#
               lambda1=1,lambda2=0,nullcheck=TRUE,df=NULL,randomstart=FALSE,#
               nullweight=10,nonzeromean=FALSE, #
               pointmass = FALSE, #
               onlylogLR = FALSE, #
               prior=c("uniform","nullbiased"), #
               mixsd=NULL, VB=FALSE,gridmult=sqrt(2),#
               minimaloutput=FALSE,#
               multiseqoutput=FALSE,#
               g=NULL,#
               maxiter = 5000,#
               cxx=FALSE){#
  #method provides a convenient interface to set a particular combinations of parameters for prior an#
  #If method is supplied, use it to set up specific values for these parameters; provide warning if values#
  #are also specified by user#
  #If method is not supplied use the user-supplied values (or defaults if user does not specify them)#
  if(!missing(method)){#
    method = match.arg(method) #
    if(method=="shrink"){#
      if(missing(prior)){#
        prior = "uniform"#
      } else {#
        warning("Specification of prior overrides default for method shrink")#
      }#
      if(missing(pointmass)){#
        pointmass=FALSE#
      } else {#
        warning("Specification of pointmass overrides default for method shrink")#
      }#
    }#
    if(method=="fdr"){#
      if(missing(prior)){#
        prior = "nullbiased"#
      } else {#
        warning("Specification of prior overrides default for method fdr")#
      }#
      if(missing(pointmass)){#
        pointmass=TRUE#
      } else {#
        warning("Specification of pointmass overrides default for method fdr")#
      }#
    }  #
  }#
  if(gridmult<=1&multiseqoutput!=TRUE)#
    stop("gridmult must be > 1")#
  mixcompdist = match.arg(mixcompdist)#
  # if(mixcompdist=="uniform" & pointmass==TRUE){#
  #    stop("point mass not yet implemented for uniform or half-uniform")#
  #  }#
  #  if(mixcompdist=="halfuniform" & pointmass==TRUE){#
  #    stop("point mass not yet implemented for uniform or half-uniform")#
  #  }#
  if(!is.numeric(prior)){#
    prior = match.arg(prior)#
  }  #
  if(length(sebetahat)==1){#
    sebetahat = rep(sebetahat,length(betahat))#
  }#
  if(length(sebetahat) != length(betahat)){#
    stop("Error: sebetahat must have length 1, or same length as betahat")#
  }#
  completeobs = (!is.na(betahat) & !is.na(sebetahat))#
  n=sum(completeobs)#
  if(n==0){#
    if(onlylogLR){#
      return(list(pi=NULL, logLR = 0))#
    }#
    else{#
      stop("Error: all input values are missing")#
    }#
  }  #
  if(!is.null(g)){#
    maxiter = 1 # if g is specified, don't iterate the EM#
    prior = rep(1,ncomp(g)) #prior is not actually used if g specified, but required to make sure EM doesn't produce warning#
    null.comp=1 #null.comp also not used, but required #
  } else {#
    if(is.null(mixsd)){#
      mixsd = autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)#
    }#
    if(pointmass){#
      mixsd = c(0,mixsd)#
    }#
    null.comp = which.min(mixsd) #which component is the "null"#
    k = length(mixsd)#
	if(nonzeromean & is.null(df)){#
		nonzeromean.fit=nonzeromeanEM(betahat[completeobs], sebetahat[completeobs], mixsd, maxiter=maxiter)#
		betahat[completeobs]= betahat[completeobs] - nonzeromean.fit$nonzeromean#
	}#
	elseif(nonzeromean & !is.null(df) ){#
		stop("Error: Nonzero mean only implemented for df=NULL")#
	}#
    if(!is.numeric(prior)){#
      if(prior=="nullbiased"){ # set up prior to favour "null"#
        prior = rep(1,k)#
        prior[null.comp] = nullweight #prior 10-1 in favour of null by default#
      }else if(prior=="uniform"){#
        prior = rep(1,k)#
      }#
    }#
    if(length(prior)!=k | !is.numeric(prior)){#
      stop("invalid prior specification")#
    }#
    if(randomstart){#
      pi = rgamma(k,1,1)#
    } else {#
      if(k<n){#
        pi=rep(1,k)/n #default initialization strongly favours null; puts weight 1/n on everything except null#
        pi[null.comp] = (n-k+1)/n #the motivation is data can quickly drive away from null, but tend to drive only slowly toward null.#
      } else {#
        pi=rep(1,k)/k#
      }#
    }#
    pi=normalize(pi)#
    if(!is.element(mixcompdist,c("normal","uniform","halfuniform"))) stop("Error: invalid type of mixcompdist")#
    if(mixcompdist=="normal") g=normalmix(pi,rep(0,k),mixsd)#
    if(mixcompdist=="uniform") g=unimix(pi,-mixsd,mixsd)#
    if(mixcompdist=="halfuniform"){#
      g = unimix(c(pi,pi)/2,c(-mixsd,rep(0,k)),c(rep(0,k),mixsd))#
      prior = rep(prior, 2)#
      pi = rep(pi, 2)#
    }#
  }#
  pi.fit=EMest(betahat[completeobs],lambda1*sebetahat[completeobs]+lambda2,g,prior,null.comp=null.comp,nullcheck=nullcheck,VB=VB,maxiter = maxiter, cxx=cxx, df=df)  #
  if (!onlylogLR){#
      n=length(betahat)#
      if (!multiseqoutput){#
          ZeroProb = rep(0,length=n)#
          NegativeProb = rep(0,length=n)#
      }#
      if (!minimaloutput){#
          PosteriorMean = rep(0,length=n)#
          PosteriorSD = rep(0,length=n)#
      }#
      if (!multiseqoutput){#
          ZeroProb[completeobs] = colSums(comppostprob(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)[comp_sd(pi.fit$g)==0,,drop=FALSE])     #
          NegativeProb[completeobs] = cdf_post(pi.fit$g, 0, betahat[completeobs],sebetahat[completeobs],df) - ZeroProb[completeobs]#
      }#
      if (!minimaloutput){#
          PosteriorMean[completeobs] = postmean(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
          PosteriorSD[completeobs] = postsd(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
      }#
                                        #FOR MISSING OBSERVATIONS, USE THE PRIOR INSTEAD OF THE POSTERIOR#
      if (!multiseqoutput){#
          ZeroProb[!completeobs] = sum(mixprop(pi.fit$g)[comp_sd(pi.fit$g)==0])#
          NegativeProb[!completeobs] = mixcdf(pi.fit$g,0)#
          lfsr = compute_lfsr(NegativeProb,ZeroProb)#
      }#
      if (!minimaloutput){#
          PosteriorMean[!completeobs] = mixmean(pi.fit$g)#
          PosteriorSD[!completeobs] = mixsd(pi.fit$g)#
      }#
      if (!minimaloutput & !multiseqoutput){#
          PositiveProb = 1- NegativeProb-ZeroProb#
          lfsra = compute_lfsra(PositiveProb,NegativeProb,ZeroProb) #
          lfdr = ZeroProb#
          qvalue = qval.from.lfdr(lfdr)#
      }#
  }#
  if (!minimaloutput)#
      logLR = tail(pi.fit$loglik,1) - pi.fit$null.loglik#
#
  if(nonzeromean & is.null(df)){#
      #Adding back the nonzero mean#
      betahat[completeobs]= betahat[completeobs]+nonzeromean.fit$nonzeromean#
      pi.fit$g$mean =nonzeromean.fit$nonzeromean#
      PosteriorMean= PosteriorMean + nonzeromean.fit$nonzeromean      #
  }	    #
  if (onlylogLR){#
      return(list(fitted.g=pi.fit$g, logLR = logLR, df=df))#
  }#
  else if (minimaloutput){#
      return(list(fitted.g = pi.fit$g, lfsr = lfsr, fit = pi.fit, df=df))#
  }#
  else if (multiseqoutput){#
      return(list(fitted.g = pi.fit$g, logLR = logLR, PosteriorMean = PosteriorMean, PosteriorSD = PosteriorSD, call= match.call(), df=df))#
  }#
  else{#
      result = list(fitted.g = pi.fit$g, logLR = logLR, PosteriorMean = PosteriorMean, PosteriorSD = PosteriorSD, PositiveProb = PositiveProb, NegativeProb = NegativeProb, ZeroProb = ZeroProb, lfsr = lfsr,lfsra = lfsra, lfdr = lfdr, qvalue = qvalue, fit = pi.fit, lambda1 = lambda1, lambda2 = lambda2, call = match.call(), data = list(betahat = betahat, sebetahat=sebetahat), df=df)#
      class(result) = "ash"#
      return(result)#
  }#
}
ASH UTILITY FUNCTIONS #############################
#
#' @title Summary method for ash object#
#'#
#' @description Print summary of fitted ash object#
#'#
#' @details See readme for more details#
#' #
#' @export#
#' #
summary.ash=function(a){#
  print(a$fitted.g)#
  print(tail(a$fit$loglik,1),digits=10)#
  print(a$fit$converged)#
}#
#
#' @title Print method for ash object#
#'#
#' @description Print the fitted distribution of beta values in the EB hierarchical model#
#'#
#' @details None#
#' #
#' @export#
#' #
print.ash =function(a){#
  print(a$fitted.g)#
}#
#
#' @title Plot method for ash object#
#'#
#' @description Plot the density of the underlying fitted distribution#
#'#
#' @details None#
#' #
#' @export#
#' #
plot.ash = function(a,xmin,xmax,...){#
  x = seq(xmin,xmax,length=1000)#
  y = density(a,x)#
  plot(y,type="l",...)#
}#
#
#compute the predictive density of an observation#
#given the fitted ash object a and the vector se of standard errors#
#not implemented yet#
predictive=function(a,se){#
}#
#' @title Get fitted loglikelihood for ash object#
#'#
#' @description Return the log-likelihood of the data under the fitted distribution#
#'#
#' @param a the fitted ash object#
#'#
#' @details None#
#' #
#' @export#
#' #
#'#
get_loglik = function(a){#
  return(tail(a$fit$loglik,1))#
}#
#
#' @title Get pi0 estimate for ash object#
#'#
#' @description Return estimate of the null proportion, pi0#
#'#
#' @param a the fitted ash object#
#'#
#' @details Extracts the estimate of the null proportion, pi0, from the object a#
#' #
#' @export#
#' #
get_pi0 = function(a){#
  null.comp = comp_sd(a$fitted.g)==0#
  return(sum(a$fitted.g$pi[null.comp]))#
}#
#
#' @title Compute loglikelihood for data from ash fit#
#'#
#' @description Return the log-likelihood of the data betahat, with standard errors betahatsd, #
#' under the fitted distribution in the ash object. #
#' #
#'#
#' @param a the fitted ash object#
#' @param betahat the data#
#' @param betahatsd the observed standard errors#
#' @param zscores indicates whether ash object was originally fit to z scores #
#' @details None#
#' #
#' @export#
#' #
#'#
loglik.ash = function(a,betahat,betahatsd,zscores=FALSE){#
  g=a$fitted.g#
  FUN="+"#
  if(zscores==TRUE){#
    g$sd = sqrt(g$sd^2+1) #
    FUN="*"#
  }#
  return(loglik_conv(g,betahat, betahatsd,FUN))#
}#
#
#' @title Density method for ash object#
#'#
#' @description Return the density of the underlying fitted distribution#
#'#
#' @param a the fitted ash object#
#' @param x the vector of locations at which density is to be computed#
#'#
#' @details None#
#' #
#' @export#
#' #
#'#
density.ash=function(a,x){list(x=x,y=dens(a$fitted.g,x))}#
#
#' @title cdf method for ash object#
#'#
#' @description Computed the cdf of the underlying fitted distribution#
#'#
#' @param a the fitted ash object#
#' @param x the vector of locations at which cdf is to be computed#
#' @param lower.tail (default=TRUE) whether to compute the lower or upper tail#
#'#
#' @details None#
#' #
#' @export#
#' #
#'#
cdf.ash=function(a,x,lower.tail=TRUE){#
  return(list(x=x,y=mixcdf(a$fitted.g,x,lower.tail)))#
}#
#' @title Credible Interval Computation for the ash object#
#'#
#' @description Given the ash object return by the main function ash, this function computes the corresponding credible interval of the mixture model.#
#'#
#' @details Uses default optimization function and perform component-wise credible interval computation. The computation cost is linear of the length of betahat.#
#'#
#' @param a the fitted ash object #
#' @param levels, the level for the credible interval, (default=0.95)#
#' @param tol, the tolerance for convergence of log-likelihood.#
#' #
#' @return A matrix, with first column being the posterior mean, second and third column being the lower bound and upper bound for the credible interval. #
#'  #
#' @export#
#' #
#' #
ashci = function (a,level=0.95){#
  x=a$data$betahat#
  s=a$data$sebetahat#
  m=a$fitted.g#
  lower=min(x)-qnorm(level)*(max(m$sd)+max(s))#
  upper=max(x)+qnorm(level)*(max(m$sd)+max(s))#
  CImatrix=matrix(NA,nrow=length(x),ncol=3)	#
  colnames(CImatrix)=c("Posterior Mean",(1-level)/2,(1+level)/2)#
  CImatrix[,1]=a$PosteriorMean#
  if( class(a$fitted.g) == "normalmix" | class(a$fitted.g) == "unimix" ){#
    for(i in 1:length(x)){#
	  CImatrix[i,2]=optim(par=a$PosteriorMean[i],f=ci.lower,m=m,x=x[i],s=s[i],level=level,df=df,method="Brent",lower=lower,upper=upper)$par#
	  CImatrix[i,3]=optim(par=a$PosteriorMean[i],f=ci.upper,m=m,x=x[i],s=s[i],level=level,df=df,method="Brent",lower=lower,upper=upper)$par#
	}#
  }#
  else{stop(paste("Invalid class",class(m)))}#
  return(CImatrix)#
}#
#
ci.lower=function(z,m,x,s,level,df){#
	tailprob=cdf_post(m,z,x,s,df)#
	return(abs(tailprob-(1-level)/2))#
}#
#
ci.upper=function(z,m,x,s,level,df){#
	tailprob=1-cdf_post(m,z,x,s,df)#
	return(abs(tailprob-(1-level)/2))#
}
GENERIC FUNCTIONS #############################
# find matrix of densities at y, for each component of the mixture#
# INPUT y is an n-vector#
# OUTPUT k by n matrix of densities#
compdens = function(x,y,log=FALSE){#
  UseMethod("compdens")#
}#
compdens.default = function(x,y,log=FALSE){#
  stop(paste("Invalid class", class(m), "for first argument in",  match.call()))  #
}#
#
#standard deviations#
comp_sd = function(m){#
  UseMethod("comp_sd")#
}#
comp_sd.default = function(m){#
  stop("method comp_sd not written for this class")#
}#
#
#second moments#
comp_mean2 = function(m){#
  UseMethod("comp_mean2")#
}#
comp_mean2.default = function(m){#
  comp_sd(m)^2 + comp_mean(m)^2#
}#
#return the overall mean of the mixture#
mixmean = function(m){#
  UseMethod("mixmean")#
}#
mixmean.default = function(m){#
  sum(m$pi * comp_mean(m))#
}#
#
#return the overall second moment of the mixture#
mixmean2 = function(m){#
  UseMethod("mixmean2")#
}#
mixmean2.default = function(m){#
  sum(m$pi * comp_mean2(m))#
}#
#
#return the overall sd of the mixture#
mixsd = function(m){#
  UseMethod("mixsd")#
}#
mixsd.default = function(m){#
  sqrt(mixmean2(m)-mixmean(m)^2)#
}#
#
#means#
comp_mean = function(m){#
  UseMethod("comp_mean")#
}#
comp_mean.default = function(m){#
  stop("method comp_mean not written for this class")#
}#
#
#number of components#
ncomp = function(m){#
  UseMethod("ncomp")#
}#
ncomp.default = function(m){#
  return(length(m$pi))#
}#
#
#return mixture proportions, a generic function#
mixprop = function(m){#
  UseMethod("mixprop")#
}#
mixprop.default = function(m){#
  m$pi#
}#
#
#' @title mixcdf#
#'#
#' @description Returns cdf for a mixture (generic function)#
#' #
#' @details None#
#' #
#' @param x a mixture (eg of type normalmix or unimix)#
#' @param y locations at which cdf to be computed#
#' @param lower.tail: boolean indicating whether to report lower tail#
#' #
#' @return an object of class normalmix#
#' #
#' @export#
#' #
#' @examples mixcdf(normalmix(c(0.5,0.5),c(0,0),c(1,2)),seq(-4,4,length=100))#
#' #
mixcdf = function(x,y,lower.tail=TRUE){#
  UseMethod("mixcdf")#
}#
#' @title mixcdf.default#
#' @export#
#' #
mixcdf.default = function(x,y,lower.tail=TRUE){#
  x$pi %*% comp_cdf(x,y,lower.tail)#
}#
#
#find cdf for each component, a generic function#
comp_cdf = function(x,y,lower.tail=TRUE){#
  UseMethod("comp_cdf")#
}#
comp_cdf.default = function(x,y,lower.tail=TRUE){#
  stop("comp_cdf not implemented for this class")#
}#
#find density at y, a generic function#
dens = function(x,y){#
  UseMethod("dens")#
}#
dens.default = function(x,y){#
  return (x$pi %*% compdens(x, y))#
}#
#
#find log likelihood of data in x (a vector) for mixture in m#
loglik = function(m,x){#
  UseMethod("loglik")#
}#
loglik.default = function(m,x){#
  sum(log(dens(m,x)))#
}#
#
#find log likelihood of data in betahat, when #
#the mixture m is convolved with a normal with sd betahatsd#
#betahatsd is an n vector#
#betahat is an n vector#
#v is the degree of freedom#
#' @title loglik_conv#
#' #
#' @export#
#' #
loglik_conv = function(m,betahat,betahatsd,v,FUN="+"){#
  UseMethod("loglik_conv")#
}#
#' @title loglik_conv.default#
#' #
#' @export#
#' #
loglik_conv.default = function(m,betahat,betahatsd,v,FUN="+"){#
  sum(log(dens_conv(m,betahat,betahatsd,v,FUN)))#
}#
#
#compute the density of the components of the mixture m#
#when convoluted with a normal with standard deviation s#
#or a scaled (se) student.t with df v#
#the density is evaluated at x#
#x and s are n-vectors#
#m is a mixture with k components#
#output is a k by n matrix of densities#
compdens_conv = function(m,x,s,v,FUN="+"){#
  UseMethod("compdens_conv")#
}#
compdens_conv.default = function(m,x,s,v,FUN="+"){#
  stop(paste("Invalid class", class(m), "for first argument in",  match.call()))  #
}#
#
#compute density of mixture m convoluted with normal of sd (s) or student t with df v#
#at locations x#
#m is a mixture#
#x is an n vector#
#s is an n vector or integer#
dens_conv = function(m,x,s,v,FUN="+"){#
  UseMethod("dens_conv")#
}#
dens_conv.default = function(m,x,s,v,FUN="+"){#
  colSums(m$pi * compdens_conv(m,x,s,v,FUN))#
}#
#
#compute the posterior prob that each observation#
#came from each component of the mixture m#
#output a k by n vector of probabilities#
#computed by weighting the component densities by pi#
#and then normalizing#
comppostprob=function(m,x,s,v){#
 UseMethod("comppostprob") #
}#
comppostprob.default = function(m,x,s,v){#
  tmp= (t(m$pi * compdens_conv(m,x,s,v))/dens_conv(m,x,s,v))#
  ismissing = (is.na(x) | is.na(s))#
  tmp[ismissing,]=m$pi#
  t(tmp)#
}#
# evaluate cdf of posterior distribution of beta at c#
# m is the prior on beta, a mixture#
# c is location of evaluation#
# assumption is betahat | beta \sim N(beta,sebetahat)#
# m is a mixture with k components#
# c a scalar#
# betahat, sebetahat are n vectors #
# output is a k by n matrix#
compcdf_post=function(m,c,betahat,sebetahat,v){#
  UseMethod("compcdf_post")#
}#
compcdf_post.default=function(m,c,betahat,sebetahat,v){#
  stop("method compcdf_post not written for this class")#
}#
cdf_post = function(m,c,betahat,sebetahat,v){#
  UseMethod("cdf_post")#
}#
cdf_post.default=function(m,c,betahat,sebetahat,v){#
  colSums(comppostprob(m,betahat,sebetahat,v)*compcdf_post(m,c,betahat,sebetahat,v))#
}#
#
#output posterior mean for beta for prior mixture m,#
#given observations betahat, sebetahat, df v#
postmean = function(m, betahat,sebetahat,v){#
  UseMethod("postmean")#
}#
postmean.default = function(m,betahat,sebetahat,v){#
  colSums(comppostprob(m,betahat,sebetahat,v) * comp_postmean(m,betahat,sebetahat,v))#
}#
#output posterior mean-squared value for beta for prior mixture m,#
#given observations betahat, sebetahat, df v#
postmean2 = function(m, betahat,sebetahat,v){#
  UseMethod("postmean2")#
}#
postmean2.default = function(m,betahat,sebetahat,v){#
  colSums(comppostprob(m,betahat,sebetahat,v) * comp_postmean2(m,betahat,sebetahat,v))#
}#
#
#output posterior sd for beta for prior mixture m,#
#given observations betahat, sebetahat, df v#
postsd = function(m,betahat,sebetahat,v){#
  UseMethod("postsd")#
}#
postsd.default = function(m,betahat,sebetahat,v){#
  sqrt(postmean2(m,betahat,sebetahat,v)-postmean(m,betahat,sebetahat,v)^2)#
}#
#
#output posterior mean-squared value for beta for prior mixture m,#
#given observations betahat, sebetahat, df v#
comp_postmean2 = function(m,betahat,sebetahat,v){#
  UseMethod("comp_postmean2")#
}#
comp_postmean2.default = function(m,betahat,sebetahat,v){#
  comp_postsd(m,betahat,sebetahat,v)^2 + comp_postmean(m,betahat,sebetahat,v)^2#
}#
#output posterior mean for beta for each component of prior mixture m,#
#given observations betahat, sebetahat, df v#
comp_postmean = function(m, betahat,sebetahat,v){#
  UseMethod("comp_postmean")#
}#
comp_postmean.default = function(m,betahat,sebetahat,v){#
  stop("method comp_postmean not written for this class")#
}#
#output posterior sd for beta for each component of prior mixture m,#
#given observations betahat, sebetahat, df v#
comp_postsd = function(m, betahat,sebetahat,v){#
  UseMethod("comp_postsd")#
}#
comp_postsd.default = function(m,betahat,sebetahat,v){#
  stop("method comp_postsd not written for this class")#
}#
#
#find nice limits of mixture m for plotting#
min_lim = function(m){#
  UseMethod("min_lim")#
}#
min_lim.default=function(m){#
  -5#
}#
#
max_lim = function(m){#
  UseMethod("max_lim")#
}#
max_lim.default=function(m){#
  5#
}#
#plot density of mixture#
plot_dens = function(m,npoints=100,...){#
  UseMethod("plot_dens")#
}#
plot_dens.default = function(m,npoints=100,...){#
  x = seq(min_lim(m),max_lim(m),length=npoints)#
  plot(x,dens(m,x),type="l",xlab="density",ylab="x",...)#
}#
#
plot_post_cdf = function(m,betahat,sebetahat,v,npoints=100,...){#
  UseMethod("plot_post_cdf")#
}#
plot_post_cdf.default = function(m,betahat,sebetahat,v,npoints=100,...){#
  x = seq(min_lim(m),max_lim(m),length=npoints)#
  x_cdf = vapply(x,cdf_post,FUN.VALUE=betahat,m=m,betahat=betahat,sebetahat=sebetahat,v=v)#
  plot(x,x_cdf,type="l",xlab="x",ylab="cdf",...)#
 # for(i in 2:nrow(x_cdf)){#
 #   lines(x,x_cdf[i,],col=i)#
 # }#
}#
#
############################### METHODS FOR normalmix class ############################
#
#' @title Constructor for normalmix class#
#'#
#' @description Creates an object of class normalmix (finite mixture of univariate normals)#
#' #
#' @details None#
#' #
#' @param pi vector of mixture proportions#
#' @param mean vector of means#
#' @param sd: vector of standard deviations#
#' #
#' @return an object of class normalmix#
#' #
#' @export#
#' #
#' @examples normalmix(c(0.5,0.5),c(0,0),c(1,2))#
#' #
normalmix = function(pi,mean,sd){#
  structure(data.frame(pi,mean,sd),class="normalmix")#
}#
#
comp_sd.normalmix = function(m){#
  m$sd#
}#
#
comp_mean.normalmix = function(m){#
  m$mean#
}#
#
compdens.normalmix = function(x,y,log=FALSE){#
  k=ncomp(x)#
  n=length(y)#
  d = matrix(rep(y,rep(k,n)),nrow=k)#
  return(matrix(dnorm(d, x$mean, x$sd, log),nrow=k))  #
}#
#
#density of convolution of each component of a normal mixture with N(0,s^2) or s*t(v) at x#
# x an n-vector at which density is to be evaluated#
#return a k by n matrix#
#Note that convolution of two normals is normal, so it works that way#
compdens_conv.normalmix = function(m,x,s,v,FUN="+"){#
  if(!is.null(v)){#
  	stop("method comp_postsd of normal mixture not written for df!null")#
  }#
  if(length(s)==1){s=rep(s,length(x))}#
  sdmat = sqrt(outer(s^2,m$sd^2,FUN)) #n by k matrix of standard deviations of convolutions#
  return(t(dnorm(outer(x,m$mean,FUN="-")/sdmat)/sdmat))#
}#
comp_cdf.normalmix = function(x,y,lower.tail=TRUE){#
  vapply(y,pnorm,x$mean,x$mean,x$sd,lower.tail)#
}#
#
#c is a scalar#
#m a mixture with k components#
#betahat a vector of n observations#
#sebetahat an n vector of standard errors#
#return a k by n matrix of the posterior cdf#
compcdf_post.normalmix=function(m,c,betahat,sebetahat,v){#
  if(!is.null(v)){#
  	stop("Error: normal mixture for student-t likelihood is not yet implemented")#
  }  #
  k = length(m$pi)#
  n=length(betahat)#
  #compute posterior standard deviation (s1) and posterior mean (m1)#
  s1 = sqrt(outer(sebetahat^2,m$sd^2,FUN="*")/outer(sebetahat^2,m$sd^2,FUN="+"))#
  ismissing = (is.na(betahat) | is.na(sebetahat))#
  s1[ismissing,]=m$sd#
  m1 = t(comp_postmean(m,betahat,sebetahat,v))#
  t(pnorm(c,mean=m1,sd=s1))#
}#
#
#return posterior mean for each component of prior m, given observations betahat and sebetahat#
#input, m is a mixture with k components#
#betahat, sebetahat are n vectors#
#output is a k by n matrix#
comp_postmean.normalmix = function(m,betahat,sebetahat,v){#
  if(!isnull(v)){#
  	stop("method comp_postmean of normal mixture not written for df!=NULL")#
  }#
  tmp=(outer(sebetahat^2,m$mean, FUN="*") + outer(betahat,m$sd^2, FUN="*"))/outer(sebetahat^2,m$sd^2,FUN="+")#
  ismissing = (is.na(betahat) | is.na(sebetahat))#
  tmp[ismissing,]=m$mean #return prior mean when missing data#
  t(tmp)#
}#
#return posterior standard deviation for each component of prior m, given observations betahat and sebetahat#
#input, m is a mixture with k components#
#betahat, sebetahat are n vectors#
#output is a k by n matrix#
comp_postsd.normalmix = function(m,betahat,sebetahat,v){#
  if(!is.null(v)){#
  	stop("method comp_postsd of normal mixture not written for df!=NULL")#
  }#
  t(sqrt(outer(sebetahat^2,m$sd^2,FUN="*")/outer(sebetahat^2,m$sd^2,FUN="+")))#
}#
############################### METHODS FOR unimix class ############################
#
#constructor; pi, a and b are vectors; kth component is Uniform(a[k],b[k])#
unimix = function(pi,a,b){#
  structure(data.frame(pi,a,b),class="unimix")#
}#
#
comp_cdf.unimix = function(m,y,lower.tail=TRUE){#
  vapply(y,punif,m$a,min=m$a,max=m$b,lower.tail)#
}#
#
comp_sd.unimix = function(m){#
  (m$b-m$a)/sqrt(12)#
}#
#
comp_mean.unimix = function(m){#
  (m$a+m$b)/2#
}#
compdens.unimix = function(x,y,log=FALSE){#
  k=ncomp(x)#
  n=length(y)#
  d = matrix(rep(y,rep(k,n)),nrow=k)#
  return(matrix(dunif(d, x$a, x$b, log),nrow=k))  #
}#
#
#density of convolution of each component of a unif mixture with N(0,s) at x#
# x an n-vector#
#return a k by n matrix#
compdens_conv.unimix = function(m,x,s,v, FUN="+"){#
  if(FUN!="+") stop("Error; compdens_conv not implemented for uniform with FUN!=+")#
  if(is.null(v)){#
    compdens= t(pnorm(outer(x,m$a,FUN="-")/s)-pnorm(outer(x,m$b,FUN="-")/s))/(m$b-m$a)#
    compdens[m$a==m$b,]=t(dnorm(outer(x,m$a,FUN="-")/s)/s)[m$a==m$b,]#
  }#
  else{#
    compdens= t(pt(outer(x,m$a,FUN="-")/s,df=v)-pt(outer(x,m$b,FUN="-")/s,df=v))/(m$b-m$a)#
    compdens[m$a==m$b,]=t(dt(outer(x,m$a,FUN="-")/s,df=v)/s)[m$a==m$b,]#
  }#
  return(compdens)#
}#
#c is a scalar#
#m a mixture with k components#
#betahat a vector of n observations#
#sebetahat an n vector of standard errors#
#return a k by n matrix of the posterior cdf#
compcdf_post.unimix=function(m,c,betahat,sebetahat,v){#
  k = length(m$pi)#
  n=length(betahat)#
  tmp = matrix(1,nrow=k,ncol=n)#
  tmp[m$a > c,] = 0#
  subset = m$a<=c & m$b>c # subset of components (1..k) with nontrivial cdf#
  if(sum(subset)>0){#
  	if(is.null(v)){#
      pna = pnorm(outer(betahat,m$a[subset],FUN="-")/sebetahat)#
      pnc = pnorm(outer(betahat,rep(c,sum(subset)),FUN="-")/sebetahat)#
      pnb = pnorm(outer(betahat,m$b[subset],FUN="-")/sebetahat)#
    }#
    else{#
      pna = pt(outer(betahat,m$a[subset],FUN="-")/sebetahat, df=v)#
      pnc = pt(outer(betahat,rep(c,sum(subset)),FUN="-")/sebetahat, df=v)#
      pnb = pt(outer(betahat,m$b[subset],FUN="-")/sebetahat, df=v)#
    }#
    tmp[subset,] = t((pnc-pna)/(pnb-pna))#
  }#
  subset = (m$a == m$b) #subset of components with trivial cdf#
  tmp[subset,]= rep(m$a[subset] <= c,n)#
  tmp#
}#
#
my_etruncnorm= function(a,b,mean=0,sd=1){#
  alpha = (a-mean)/sd#
  beta =  (b-mean)/sd#
 #Flip the onese where both are positive, as the computations are more stable#
  #when both negative#
  flip = (alpha>0 & beta>0)#
  flip[is.na(flip)]=FALSE #deal with NAs#
  alpha[flip]= -alpha[flip]#
  beta[flip]=-beta[flip]#
  tmp= (-1)^flip * (mean+sd*etruncnorm(alpha,beta,0,1))#
  max_alphabeta = ifelse(alpha<beta, beta,alpha)#
  max_ab = ifelse(alpha<beta,b,a)#
  toobig = max_alphabeta<(-30)#
  toobig[is.na(toobig)]=FALSE #
  tmp[toobig] = max_ab[toobig]#
  tmp#
}#
#return posterior mean for each component of prior m, given observations betahat and sebetahat#
#input, m is a mixture with k components#
#betahat, sebetahat are n vectors#
#output is a k by n matrix#
#note that with uniform prior, posterior is truncated normal, so#
#this is computed using formula for mean of truncated normal #
comp_postmean.unimix = function(m,betahat,sebetahat,v){#
#   k= ncomp(m)#
#   n=length(betahat)#
#   a = matrix(m$a,nrow=n,ncol=k,byrow=TRUE)#
#   b = matrix(m$b,nrow=n,ncol=k,byrow=TRUE)#
#   matrix(etruncnorm(a,b,betahat,sebetahat),nrow=k,byrow=TRUE)#
  #note: etruncnorm is more stable for a and b negative than positive#
  #so maybe use this, and standardize to make the whole more stable.#
  alpha = outer(-betahat, m$a,FUN="+")/sebetahat#
  beta = outer(-betahat, m$b, FUN="+")/sebetahat#
  if(is.null(v)){#
    tmp = betahat + sebetahat*my_etruncnorm(alpha,beta,0,1)#
  }#
  else{#
  	tmp = betahat + sebetahat*my_etrunct(alpha,beta,v)#
  }#
  ismissing = is.na(betahat) | is.na(sebetahat)#
  tmp[ismissing,]= (m$a+m$b)/2#
  t(tmp)#
#   t(#
#     betahat + sebetahat* #
#       exp(dnorm(alpha,log=TRUE)- pnorm(alpha,log=TRUE))#
#    * #
#       (-expm1(dnorm(beta,log=TRUE)-dnorm(alpha,log=TRUE)))#
#     /#
#       (expm1(pnorm(beta,log=TRUE)-pnorm(alpha,log=TRUE)))#
#   )#
}#
#
#not yet implemented!#
#just returns 0s for now#
comp_postsd.unimix = function(m,betahat,sebetahat,v){#
  print("Warning: Posterior SDs not yet implemented for uniform components")#
  k= ncomp(m)#
  n=length(betahat)#
  return(matrix(NA,nrow=k,ncol=n)) #
}#
#
# the mean of a truncated student.t#
# the result is from the paper 'Moments of truncated Student-t distribution' by H.-J Kim #
#
my_etrunct= function(a,b,v){#
  A = v+a^2#
  B = v+b^2#
  F_a = pt(a,df=v)#
  F_b = pt(b,df=v)#
  G = gamma((v-1)/2)*v^(v/2)/(2*(F_b-F_a)*gamma(v/2)*gamma(1/2))#
  tmp = ifelse(a==b,a,G*(A^(-(v-1)/2)-B^(-(v-1)/2)))#
  tmp#
}
df
df=NULL
nonzeromean=TRUE
if(nonzeromean & is.null(df)){#
print('yes')}
df=3
if(nonzeromean & !is.null(df)){#
print('yes')}
' @useDynLib ashr#
#todo#
##
#' @title Main Adaptive SHrinkage function#
#'#
#' @description Takes vectors of estimates (betahat) and their standard errors (sebetahat), and applies#
#' shrinkage to them, using Empirical Bayes methods, to compute shrunk estimates for beta.#
#'#
#' @details See readme for more details#
#' #
#' @param betahat, a p vector of estimates #
#' @param sebetahat, a p vector of corresponding standard errors#
#' @param method: specifies how ash is to be run. Can be "shrinkage" (if main aim is shrinkage) or "fdr" (if main aim is to assess fdr or fsr)#
#' This is simply a convenient way to specify certain combinations of parameters: "shrinkage" sets pointmass=FALSE and prior="uniform";#
#' "fdr" sets pointmass=TRUE and prior="nullbiased".#
#' @param mixcompdist: distribution of components in mixture ("normal", "uniform" or "halfuniform")#
#'#
#' @param lambda1: multiplicative "inflation factor" for standard errors (like Genomic Control)#
#' @param lambda2: additive "inflation factor" for standard errors (like Genomic Control)#
#' @param nullcheck: whether to check that any fitted model exceeds the "null" likelihood#
#' in which all weight is on the first component#
#' @param df: appropriate degrees of freedom for (t) distribution of betahat/sebetahat#
#' @param randomstart: bool, indicating whether to initialize EM randomly. If FALSE, then initializes to prior mean (for EM algorithm) or prior (for VBEM)#
#' @param nullweight: scalar, the weight put on the prior of null under "fdr" method#
#' @param nonzeromean: bool, indicating whether to use a nonzero mean unimodal mixture(defaults to "FALSE")#
#' @param pointmass: bool, indicating whether to use a point mass at zero as one of components for a mixture distribution#
#' @param onlylogLR: bool, indicating whether to use this function to get logLR. Skip posterior prob, posterior mean, lfdr...#
#' @param prior: string, or numeric vector indicating Dirichlet prior on mixture proportions (defaults to "uniform", or 1,1...,1; also can be "nullbiased" 1,1/k-1,...,1/k-1 to put more weight on first component)#
#' @param mixsd: vector of sds for underlying mixture components #
#' @param VB: whether to use Variational Bayes to estimate mixture proportions (instead of EM to find MAP estimate)#
#' @param gridmult: the multiplier by which the default grid values for mixsd differ by one another. (Smaller values produce finer grids)#
#' @param minimal_output: if TRUE, just outputs the fitted g and the lfsr (useful for very big data sets where memory is an issue) #
#' @param g: the prior distribution for beta (usually estimated from the data; this is used primarily in simulated data to do computations with the "true" g)#
#' @param maxiter: maximum number of iterations of the EM algorithm#
#' @param cxx: flag to indicate whether to use the c++ (Rcpp) version#
#' #
#'#
#' @return a list with elements fitted.g is fitted mixture#
#' logLR : logP(D|mle(pi)) - logP(D|null)#
#' #
#' @export#
#' #
#' @examples #
#' beta = c(rep(0,100),rnorm(100))#
#' sebetahat = abs(rnorm(200,0,1))#
#' betahat = rnorm(200,beta,sebetahat)#
#' beta.ash = ash(betahat, sebetahat)#
#' summary(beta.ash)#
#' plot(betahat,beta.ash$PosteriorMean,xlim=c(-4,4),ylim=c(-4,4))#
#' #
#' #
#Things to do:#
# check sampling routine#
# check number of iterations#
ash = function(betahat,sebetahat,method = c("shrink","fdr"), #
               mixcompdist = c("normal","uniform","halfuniform"),#
               lambda1=1,lambda2=0,nullcheck=TRUE,df=NULL,randomstart=FALSE,#
               nullweight=10,nonzeromean=FALSE, #
               pointmass = FALSE, #
               onlylogLR = FALSE, #
               prior=c("uniform","nullbiased"), #
               mixsd=NULL, VB=FALSE,gridmult=sqrt(2),#
               minimaloutput=FALSE,#
               multiseqoutput=FALSE,#
               g=NULL,#
               maxiter = 5000,#
               cxx=FALSE){#
  #method provides a convenient interface to set a particular combinations of parameters for prior an#
  #If method is supplied, use it to set up specific values for these parameters; provide warning if values#
  #are also specified by user#
  #If method is not supplied use the user-supplied values (or defaults if user does not specify them)#
  if(!missing(method)){#
    method = match.arg(method) #
    if(method=="shrink"){#
      if(missing(prior)){#
        prior = "uniform"#
      } else {#
        warning("Specification of prior overrides default for method shrink")#
      }#
      if(missing(pointmass)){#
        pointmass=FALSE#
      } else {#
        warning("Specification of pointmass overrides default for method shrink")#
      }#
    }#
    if(method=="fdr"){#
      if(missing(prior)){#
        prior = "nullbiased"#
      } else {#
        warning("Specification of prior overrides default for method fdr")#
      }#
      if(missing(pointmass)){#
        pointmass=TRUE#
      } else {#
        warning("Specification of pointmass overrides default for method fdr")#
      }#
    }  #
  }#
  if(gridmult<=1&multiseqoutput!=TRUE)#
    stop("gridmult must be > 1")#
  mixcompdist = match.arg(mixcompdist)#
  # if(mixcompdist=="uniform" & pointmass==TRUE){#
  #    stop("point mass not yet implemented for uniform or half-uniform")#
  #  }#
  #  if(mixcompdist=="halfuniform" & pointmass==TRUE){#
  #    stop("point mass not yet implemented for uniform or half-uniform")#
  #  }#
  if(!is.numeric(prior)){#
    prior = match.arg(prior)#
  }  #
  if(length(sebetahat)==1){#
    sebetahat = rep(sebetahat,length(betahat))#
  }#
  if(length(sebetahat) != length(betahat)){#
    stop("Error: sebetahat must have length 1, or same length as betahat")#
  }#
  completeobs = (!is.na(betahat) & !is.na(sebetahat))#
  n=sum(completeobs)#
  if(n==0){#
    if(onlylogLR){#
      return(list(pi=NULL, logLR = 0))#
    }#
    else{#
      stop("Error: all input values are missing")#
    }#
  }  #
  if(!is.null(g)){#
    maxiter = 1 # if g is specified, don't iterate the EM#
    prior = rep(1,ncomp(g)) #prior is not actually used if g specified, but required to make sure EM doesn't produce warning#
    null.comp=1 #null.comp also not used, but required #
  } else {#
    if(is.null(mixsd)){#
      mixsd = autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)#
    }#
    if(pointmass){#
      mixsd = c(0,mixsd)#
    }#
    if(nonzeromean & is.null(df)){#
		nonzeromean.fit=nonzeromeanEM(betahat[completeobs], sebetahat[completeobs], mixsd, maxiter=maxiter)#
		betahat[completeobs]= betahat[completeobs] - nonzeromean.fit$nonzeromean#
	}#
	elseif(nonzeromean & !is.null(df) ){#
		stop("Error: Nonzero mean only implemented for df=NULL")#
	}#
    null.comp = which.min(mixsd) #which component is the "null"#
    k = length(mixsd)#
    if(!is.numeric(prior)){#
      if(prior=="nullbiased"){ # set up prior to favour "null"#
        prior = rep(1,k)#
        prior[null.comp] = nullweight #prior 10-1 in favour of null by default#
      }else if(prior=="uniform"){#
        prior = rep(1,k)#
      }#
    }#
    if(length(prior)!=k | !is.numeric(prior)){#
      stop("invalid prior specification")#
    }#
    if(randomstart){#
      pi = rgamma(k,1,1)#
    } else {#
      if(k<n){#
        pi=rep(1,k)/n #default initialization strongly favours null; puts weight 1/n on everything except null#
        pi[null.comp] = (n-k+1)/n #the motivation is data can quickly drive away from null, but tend to drive only slowly toward null.#
      } else {#
        pi=rep(1,k)/k#
      }#
    }#
    pi=normalize(pi)#
    if(!is.element(mixcompdist,c("normal","uniform","halfuniform"))) stop("Error: invalid type of mixcompdist")#
    if(mixcompdist=="normal") g=normalmix(pi,rep(0,k),mixsd)#
    if(mixcompdist=="uniform") g=unimix(pi,-mixsd,mixsd)#
    if(mixcompdist=="halfuniform"){#
      g = unimix(c(pi,pi)/2,c(-mixsd,rep(0,k)),c(rep(0,k),mixsd))#
      prior = rep(prior, 2)#
      pi = rep(pi, 2)#
    }#
  }#
  pi.fit=EMest(betahat[completeobs],lambda1*sebetahat[completeobs]+lambda2,g,prior,null.comp=null.comp,nullcheck=nullcheck,VB=VB,maxiter = maxiter, cxx=cxx, df=df)  #
  if (!onlylogLR){#
      n=length(betahat)#
      if (!multiseqoutput){#
          ZeroProb = rep(0,length=n)#
          NegativeProb = rep(0,length=n)#
      }#
      if (!minimaloutput){#
          PosteriorMean = rep(0,length=n)#
          PosteriorSD = rep(0,length=n)#
      }#
      if (!multiseqoutput){#
          ZeroProb[completeobs] = colSums(comppostprob(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)[comp_sd(pi.fit$g)==0,,drop=FALSE])#
          NegativeProb[completeobs] = cdf_post(pi.fit$g, 0, betahat[completeobs],sebetahat[completeobs],df) - ZeroProb[completeobs]#
      }#
      if (!minimaloutput){#
          PosteriorMean[completeobs] = postmean(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
          PosteriorSD[completeobs] = postsd(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
      }#
                                        #FOR MISSING OBSERVATIONS, USE THE PRIOR INSTEAD OF THE POSTERIOR#
      if (!multiseqoutput){#
          ZeroProb[!completeobs] = sum(mixprop(pi.fit$g)[comp_sd(pi.fit$g)==0])#
          NegativeProb[!completeobs] = mixcdf(pi.fit$g,0)#
          lfsr = compute_lfsr(NegativeProb,ZeroProb)#
      }#
      if (!minimaloutput){#
          PosteriorMean[!completeobs] = mixmean(pi.fit$g)#
          PosteriorSD[!completeobs] = mixsd(pi.fit$g)#
      }#
      if (!minimaloutput & !multiseqoutput){#
          PositiveProb = 1- NegativeProb-ZeroProb#
          lfsra = compute_lfsra(PositiveProb,NegativeProb,ZeroProb) #
          lfdr = ZeroProb#
          qvalue = qval.from.lfdr(lfdr)#
      }#
  }#
  if (!minimaloutput)#
      logLR = tail(pi.fit$loglik,1) - pi.fit$null.loglik#
  if (onlylogLR)#
      return(list(fitted.g=pi.fit$g, logLR = logLR, df=df))#
  else if (minimaloutput)#
      return(list(fitted.g = pi.fit$g, lfsr = lfsr, fit = pi.fit,df=df))#
  else if (multiseqoutput)#
      return(list(fitted.g = pi.fit$g, logLR = logLR, PosteriorMean = PosteriorMean, PosteriorSD = PosteriorSD, call= match.call(),df=df))#
  else{#
      result = list(fitted.g = pi.fit$g, logLR = logLR, PosteriorMean = PosteriorMean, PosteriorSD = PosteriorSD, PositiveProb = PositiveProb, NegativeProb = NegativeProb, ZeroProb = ZeroProb, lfsr = lfsr,lfsra = lfsra, lfdr = lfdr, qvalue = qvalue, fit = pi.fit, lambda1 = lambda1, lambda2 = lambda2, call = match.call(), data = list(betahat = betahat, sebetahat=sebetahat),df=df)#
      class(result) = "ash"#
      return(result)#
  }#
}#
  #if(nsamp>0){#
  #  sample = posterior_sample(post,nsamp)#
  #}#
#
# #' @title Faster version of function ash#
# #'#
# #' @description This function has similar functionality as ash, but only returns some of the outputs.#
# #'#
# #' @param betahat, a p vector of estimates#
# #' @param sebetahat, a p vector of corresponding standard errors#
# #' @param nullcheck: whether to check that any fitted model exceeds the "null" likelihood in which all weight is on the first component#
# #' @param randomstart: bool, indicating whether to initialize EM randomly. If FALSE, then initializes to prior mean (for EM algorithm) or prior (for VBEM)#
# #' @param pointmass: bool, indicating whether to use a point mass at zero as one of components for a mixture distribution#
# #' @param onlylogLR: bool, indicating whether to use this function to get logLR. Skip posterior prob, posterior mean, lfdr...#
# #' @param prior: string, or numeric vector indicating Dirichlet prior on mixture proportions (defaults to "uniform", or 1,1...,1; also can be "nullbiased" 1,1/k-1,...,1/k-1 to put more weight on first component)#
# #' @param mixsd: vector of sds for underlying mixture components#
# #' @param VB: whether to use Variational Bayes to estimate mixture proportions (instead of EM to find MAP estimate)#
# #' @param gridmult: the multiplier by which the default grid values for mixsd differ by one another. (Smaller values produce finer grids)#
# #' @param g: the prior distribution for beta (usually estimated from the data; this is used primarily in simulated data to do computations with the "true" g)#
# #' @param cxx: flag to indicate whether to use the c++ (Rcpp) version#
# #'#
# #' @return a list with elements fitted.g is fitted mixture#
# #' logLR : logP(D|mle(pi)) - logP(D|null)#
# #'#
# #' @export#
# fast.ash = function(betahat,sebetahat, #
#                     nullcheck=TRUE,randomstart=FALSE, #
#                     pointmass = TRUE,    #
#                     prior=c("nullbiased","uniform"), #
#                     mixsd=NULL, VB=FALSE,gridmult=4,#
#                     g=NULL, cxx=TRUE,#
#                     onlylogLR = FALSE,df=NULL){#
#   #
#   if(onlylogLR){#
#     pointmass <- TRUE  #
#   }#
#   #
#   #If method is supplied, use it to set up defaults; provide warning if these default values#
#   #are also specified by user#
#   if(!is.numeric(prior)){#
#     prior = match.arg(prior)#
#   }#
#   #
#   if(length(sebetahat)==1){#
#     sebetahat = rep(sebetahat,length(betahat))#
#   }#
#   if(length(sebetahat) != length(betahat)){#
#     stop("Error: sebetahat must have length 1, or same length as betahat")#
#   }#
#   #
#   completeobs = (!is.na(betahat) & !is.na(sebetahat))#
#   if(sum(completeobs)==0){#
#     if(onlylogLR){#
#       return(list(pi=NULL, logLR = 0))#
#     }else{#
#       stop("Error: all input values are missing")#
#     }#
#   }  #
#   #
#   if(is.null(mixsd)){#
#     mixsd= autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)#
#   }#
#   if(pointmass){#
#     mixsd = c(0,mixsd)#
#   }#
#   #
#   k=length(mixsd)  #
#   null.comp = which.min(mixsd) #which component is the "null"#
#   #
#   if(!is.numeric(prior)){#
#     if(prior=="nullbiased"){ # set up prior to favour "null"#
#       prior = rep(1,k)#
#       prior[null.comp] = 10 #prior 10-1 in favour of null#
#     }else if(prior=="uniform"){#
#       prior = rep(1,k)#
#     }#
#   }#
#   #
#   if(length(prior)!=k | !is.numeric(prior)){#
#     stop("invalid prior specification")#
#   }#
#   #
#   if(missing(g)){#
#     pi = prior^2 #default is to initialize pi at prior (mean)#
#     if(randomstart){pi=rgamma(k,1,1)}#
#     pi=normalize(pi)#
#     g=normalmix(pi,rep(0,k),mixsd)#
#     maxiter = 5000#
#   } else {#
#     maxiter = 1; # if g is specified, don't iterate the EM #
#   }#
#   #
#   pi.fit=EMest(betahat[completeobs],sebetahat[completeobs],g,prior,null.comp=null.comp,nullcheck=nullcheck,VB=VB,maxiter = maxiter, cxx=cxx, df=df)  #
#   #
#   if(onlylogLR){#
#     logLR = tail(pi.fit$loglik,1) - pi.fit$null.loglik#
#     return(list(pi=pi.fit$pi, logLR = logLR))#
#   }else{#
#     #
#     n=length(betahat)#
#     PosteriorMean = rep(0,length=n)#
#     PosteriorSD=rep(0,length=n)#
#     #
#     if(is.null(df)){#
#       PosteriorMean[completeobs] = postmean(pi.fit$g,betahat[completeobs],sebetahat[completeobs])#
#       PosteriorSD[completeobs] =postsd(pi.fit$g,betahat[completeobs],sebetahat[completeobs]) #
#     }#
#     else{#
#       PosteriorMean[completeobs] = postmean_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
#       PosteriorSD[completeobs] =postsd_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
#     }#
#     #FOR MISSING OBSERVATIONS, USE THE PRIOR INSTEAD OF THE POSTERIOR#
#     PosteriorMean[!completeobs] = mixmean(pi.fit$g)#
#     PosteriorSD[!completeobs] =mixsd(pi.fit$g)  #
#     #
#     result = list(fitted.g=pi.fit$g,PosteriorMean = PosteriorMean,PosteriorSD=PosteriorSD,call=match.call(),data=list(betahat = betahat, sebetahat=sebetahat))#
#     return(result)#
#   }#
#   #if(nsamp>0){#
#   #  sample = posterior_sample(post,nsamp)#
#   #}#
# }#
compute_lfsr = function(NegativeProb,ZeroProb){#
  ifelse(NegativeProb> 0.5*(1-ZeroProb),1-NegativeProb,NegativeProb+ZeroProb)#
}#
#
compute_lfsra = function(PositiveProb, NegativeProb,ZeroProb){#
  ifelse(PositiveProb<NegativeProb,2*PositiveProb+ZeroProb,2*NegativeProb+ZeroProb)  #
}  #
#
#' @title Estimate unimodal nonzero mean of a mixture model by EM algorithm#
#'#
#' @description Given the data, standard error of the data and standard deviations of the Gaussian mixture model, estimate the mean of a unimodal Gaussian mixture by an EM algorithm.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates unimodal mean \eqn{\mu} by EM algorithm. Uses the SQUAREM package to accelerate convergence of EM. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' #
#' @param betahat, a p vector of estimates #
#' @param sebetahat, a p vector of corresponding standard errors#
#' @param mixsd: vector of sds for underlying mixture components #
#' @param pi.init, the initial value of \eqn{\pi} to use. If not specified defaults to (1/k,...,1/k).#
#' @param tol, the tolerance for convergence of log-likelihood.#
#' @param maxiter the maximum number of iterations performed#
#' #
#' @return A list, including the estimates (\eqn{\mu}) and (\eqn{\pi}), the log likelihood for each iteration (NQ)#
#' and a flag to indicate convergence#
#'  #
#' @export#
#' #
#' #
nonzeromeanEM = function(betahat, sebetahat, mixsd, pi.init=NULL,tol=1e-7,maxiter=5000){#
  if(is.null(pi.init)){#
    pi.init = rep(1/length(mixsd),length(mixsd))# Use as starting point for pi#
  }#
  mupi=c(mean(betahat),pi.init)#
  res=squarem(par=mupi,fixptfn=nonzeromeanEMfixpoint,objfn=nonzeromeanEMobj,betahat=betahat,sebetahat=sebetahat,mixsd=mixsd,control=list(maxiter=maxiter,tol=tol))#
  return(list(nonzeromean=res$par[1],pi=res$par[-1],NQ=-res$value.objfn,niter = res$iter, converged=res$convergence,post=res$par))#
}#
#
nonzeromeanEMfixpoint = function(mupi,betahat,sebetahat,mixsd){#
	#omegamatrix=matrix(NA,nrow=length(betahat),ncol=length(mixsd))#
	mu=mupi[1]#
	pimean=mupi[-1]#
	sdmat = sqrt(outer(sebetahat ^2,mixsd^2,"+")) #
	xmat=matrix(rep(betahat,length(mixsd)),ncol=length(mixsd))#
	omegamatrix=t(t(dnorm(xmat,mean=mu,sd=sdmat))*pimean)#
	omegamatrix=omegamatrix /rowSums(omegamatrix)#
	pinew=normalize(colSums(omegamatrix))#
	munew=sum(omegamatrix*xmat/(sdmat^2))/sum(omegamatrix/(sdmat^2))#
	mupi=c(munew,pinew)#
	return(mupi)#
}#
#
nonzeromeanEMobj = function(mupi,betahat,sebetahat,mixsd){#
	mu=mupi[1]#
	pimean=mupi[-1]#
	sdmat = sqrt(outer(sebetahat ^2,mixsd^2,"+")) #
	xmat=matrix(rep(betahat,length(mixsd)),ncol=length(mixsd))#
	omegamatrix=t(t(dnorm(xmat,mean=mu,sd=sdmat))*pimean)#
	omegamatrix=omegamatrix /rowSums(omegamatrix)#
	NegativeQ=-sum(omegamatrix*dnorm(xmat,mean=mu,sd=sdmat,log=TRUE))#
	return(NegativeQ)#
}#
#' @title Estimate posterior distribution on mixture proportions of a mixture model by a Variational Bayes EM algorithm#
#'#
#' @description Given the individual component likelihoods for a mixture model, estimates the posterior on #
#' the mixture proportions by an VBEM algorithm. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates posterior on mixture proportions \eqn{\pi} by Variational Bayes, #
#' with a Dirichlet prior on \eqn{\pi}. #
#' Algorithm adapted from Bishop (2009), Pattern Recognition and Machine Learning, Chapter 10.#
#' #
#' @param matrix_lik: a n by k matrix with (j,k)th element equal to \eqn{f_k(x_j)}.#
#' @param prior: a k vector of the parameters of the Dirichlet prior on \eqn{\pi}. Recommended to be rep(1,k)#
#' @param post.init: the initial value of the posterior parameters. If not specified defaults to the prior parameters.#
#' @param tol: the tolerance for convergence of log-likelihood bound.#
#' @param maxiter: the maximum number of iterations performed#
#' #
#' @return A list, whose components include point estimates (pihat), #
#' the parameters of the fitted posterior on \eqn{\pi} (pipost),#
#' the bound on the log likelihood for each iteration (B)#
#' and a flag to indicate convergence (converged).#
#'  #
#' @export#
#' #
mixVBEM = function(matrix_lik, prior, pi.init = NULL,tol=1e-7, maxiter=5000){#
  k=ncol(matrix_lik)#
  if(is.null(pi.init)){#
    pi.init = rep(1,k)# Use as starting point for pi#
  } #
  res = squarem(par=pi.init,fixptfn=VBfixpoint, objfn=VBnegpenloglik,matrix_lik=matrix_lik, prior=prior, control=list(maxiter=maxiter,tol=tol))#
  return(list(pihat = res$par/sum(res$par), B=res$value.objfn, niter = res$iter, converged=res$convergence,post=res$par))#
}#
VBfixpoint = function(pipost, matrix_lik, prior){  #
  n=nrow(matrix_lik)#
  k=ncol(matrix_lik)#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost*matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  pipostnew = colSums(classprob) + prior#
  return(pipostnew)#
}#
#
VBnegpenloglik=function(pipost,matrix_lik,prior){#
  return(-VBpenloglik(pipost,matrix_lik,prior))#
}#
#
VBpenloglik = function(pipost, matrix_lik, prior){#
  n=nrow(matrix_lik)#
  k=ncol(matrix_lik)#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost*matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  B= sum(classprob*log(avgpipost*matrix_lik),na.rm=TRUE) - diriKL(prior,pipost) - sum(classprob*log(classprob)) #
  return(B)#
}#
#' @title Estimate mixture proportions of a mixture model by EM algorithm#
#'#
#' @description Given the individual component likelihoods for a mixture model, estimates the mixture proportions by an EM algorithm.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates mixture proportions \eqn{\pi} by maximum likelihood, or by maximum a posteriori (MAP) estimation for a Dirichlet prior on $\pi$ #
#' (if a prior is specified).  Uses the SQUAREM package to accelerate convergence of EM. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' #
#' @param matrix_lik, a n by k matrix with (j,k)th element equal to \eqn{f_k(x_j)}.#
#' @param prior, a k vector of the parameters of the Dirichlet prior on \eqn{\pi}. Recommended to be rep(1,k)#
#' @param pi.init, the initial value of \eqn{\pi} to use. If not specified defaults to (1/k,...,1/k).#
#' @param tol, the tolerance for convergence of log-likelihood.#
#' @param maxiter the maximum number of iterations performed#
#' #
#' @return A list, including the estimates (pihat), the log likelihood for each interation (B)#
#' and a flag to indicate convergence#
#'  #
#' @export#
#' #
#' #
mixEM = function(matrix_lik, prior, pi.init = NULL,tol=1e-7, maxiter=5000){#
  if(is.null(pi.init)){#
    pi.init = rep(1/k,k)# Use as starting point for pi#
  } #
  res = squarem(par=pi.init,fixptfn=fixpoint, objfn=negpenloglik,matrix_lik=matrix_lik, prior=prior, control=list(maxiter=maxiter,tol=tol))#
  return(list(pihat = normalize(pmax(0,res$par)), B=res$value.objfn, #
              niter = res$iter, converged=res$convergence))#
}#
#
# helper functions used by mixEM#
normalize = function(x){return(x/sum(x))}#
#
fixpoint = function(pi, matrix_lik, prior){  #
  pi = normalize(pmax(0,pi)) #avoid occasional problems with negative pis due to rounding#
  m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
  m.rowsum = rowSums(m)#
  classprob = m/m.rowsum #an n by k matrix#
  pinew = normalize(colSums(classprob) + prior - 1)#
  return(pinew)#
}#
#
negpenloglik = function(pi,matrix_lik,prior){return(-penloglik(pi,matrix_lik,prior))}#
#
penloglik = function(pi, matrix_lik, prior){#
  pi = normalize(pmax(0,pi))#
  m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
  m.rowsum = rowSums(m)#
  loglik = sum(log(m.rowsum))#
  subset = (prior != 1.0)#
  priordens = sum((prior-1)[subset]*log(pi[subset]))#
  return(loglik+priordens)#
}#
#
#The kth element of this vector is the derivative #
#of the loglik for $\pi=(\pi_0,...,1-\pi_0,...)$ with respect to $\pi_0$ at $\pi_0=1$.#
gradient = function(matrix_lik){#
  n = nrow(matrix_lik)#
  grad = n - colSums(matrix_lik/matrix_lik[,1]) #
  return(grad)#
}#
#
# mixEM = function(matrix_lik, prior, pi.init = NULL,tol=0.0001, maxiter=5000){#
#   n=nrow(matrix_lik)#
#   k=ncol(matrix_lik)#
#   B = rep(0,maxiter)#
#   pi = pi.init#
#   if(is.null(pi.init)){#
#     pi = rep(1/k,k)# Use as starting point for pi#
#   } #
#   pi = ifelse(pi<1e-5,1e-5,pi) #set any estimates that are too small to be just very small#
#   pi = normalize(pi)#
#   #
#   loglik = rep(0,maxiter)#
#   priordens= rep(0,maxiter)#
#   m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
#   m.rowsum = rowSums(m)#
#   loglik[1] = sum(log(m.rowsum))#
#   priordens[1] = sum((prior-1)*log(pi)) #
#   classprob = m/m.rowsum #an n by k matrix#
#   i=1#
#   if(maxiter >= 2){#
#     for(i in 2:maxiter){  #
#       pi = colSums(classprob) + prior-1#
#       pi = ifelse(pi<1e-5,1e-5,pi) #set any estimates that are less than zero, which can happen with prior<1, to 0#
#       pi = normalize(pi)#
#         #
#       #Now re-estimate pi#
#       m  = t(pi * t(matrix_lik)) #
#       m.rowsum = rowSums(m)#
#       loglik[i] = sum(log(m.rowsum))#
#       priordens[i] = sum((prior-1)*log(pi)) #
#       classprob = m/m.rowsum#
#     #
#     #
#       if(abs(loglik[i]+priordens[i]-loglik[i-1]-priordens[i-1])<tol) break;#
#     }#
#   }#
#   converged=(abs(loglik[i]+priordens[i]-loglik[i-1]-priordens[i-1])<tol)#
#   if(!converged){#
#       warning("EM algorithm in function mixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
#   }#
#   return(list(pihat = pi, B=loglik[1:i], #
#               niter = i, converged=converged))#
# }#
#estimate mixture proportions of sigmaa by EM algorithm#
#prior gives the parameter of a Dirichlet prior on pi#
#(prior is used to encourage results towards smallest value of sigma when#
#likelihood is flat)#
#nullcheck indicates whether to check whether the loglike exceeds the null#
#(may not want to use if prior is used)#
#VB provides an approach to estimate the approximate posterior distribution#
#of mixture proportions of sigmaa by variational Bayes method#
#(use Dirichlet prior and approximate Dirichlet posterior)#
#if cxx TRUE use cpp version of R function mixEM#
EMest = function(betahat,sebetahat,g,prior,null.comp=1,nullcheck=TRUE,VB=FALSE, maxiter=5000, cxx=TRUE, df=NULL){ #
  pi.init = g$pi#
  k=ncomp(g)#
  n = length(betahat)#
  tol = min(0.1/n,1e-5) # set convergence criteria to be more stringent for larger samples#
  if(is.null(df)){#
    matrix_lik = t(compdens_conv(g,betahat,sebetahat))#
  }#
  else{#
    matrix_lik = t(compdens_conv_t(g,betahat,sebetahat,df))#
  }#
  #checks whether the gradient at pi0=1 is positive (suggesting that this is a fixed point)#
  #if(nullcheck){#
  #  if(all(gradient(matrix_lik)>=0)){#
  #    pi.init=rep(0,k)#
  #    pi.init[null.comp]=1 #this will make pi.init=(1,0,0...,0) which is a fixed point of the EM#
  #  }#
  #}#
  if(VB==TRUE){#
    EMfit=mixVBEM(matrix_lik,prior,maxiter=maxiter)}#
  else{#
    if (cxx==TRUE){#
      EMfit = cxxMixEM(matrix_lik,prior,pi.init,1e-5, maxiter) #currently use different convergence criteria for cxx version #
      if(!EMfit$converged){#
        warning("EM algorithm in function cxxMixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
      }#
    }#
    else{#
      EMfit = mixEM(matrix_lik,prior,pi.init,tol, maxiter)#
      if(!EMfit$converged & !(maxiter==1)){#
        warning("EM algorithm in function mixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
      }#
    }#
  }#
  pi = EMfit$pihat     #
  penloglik = EMfit$B #
  converged = EMfit$converged#
  niter = EMfit$niter#
  loglik.final =  penloglik(pi,matrix_lik,1) #compute penloglik without penalty#
  null.loglik = sum(log(matrix_lik[,null.comp]))  #
  if(nullcheck==TRUE & VB==FALSE){ #null check doesn't work with VB yet#
    pinull = rep(0,k)#
    pinull[null.comp]=1#
    null.penloglik = penloglik(pinull,matrix_lik,prior)#
    final.penloglik = penloglik(pi,matrix_lik,prior)#
    if(null.penloglik > final.penloglik){ #check whether exceeded "null" likelihood where everything is null#
      pi=pinull#
      loglik.final=penloglik(pi,matrix_lik,1)#
    }#
  }#
  g$pi=pi#
  return(list(loglik=loglik.final,null.loglik=null.loglik,#
              matrix_lik=matrix_lik,converged=converged,g=g))#
}#
#' @title Compute Posterior#
#'#
#' @description Return the posterior on beta given a prior (g) that is a mixture of normals (class normalmix) #
#' and observation betahat \sim N(beta,sebetahat)#
#'#
#' @details This can be used to obt#
#'#
#' @param g: a normalmix with components indicating the prior; works only if g has means 0#
#' @param betahat (n vector of observations) #
#' @param sebetahat (n vector of standard errors/deviations of observations)#
#' #
#' @return A list, (pi1,mu1,sigma1) whose components are each k by n matrices#
#' where k is number of mixture components in g, n is number of observations in betahat#
#' #
#' @export#
#' #
#' #
posterior_dist = function(g,betahat,sebetahat){#
  if(class(g)!="normalmix"){#
    stop("Error: posterior_dist implemented only for g of class normalmix")#
  }#
  pi0 = g$pi#
  mu0 = g$mean#
  sigma0 = g$sd  #
  k= length(pi0)#
  n= length(betahat)#
  if(!all.equal(g$mean,rep(0,k))) stop("Error: posterior_dist currently only implemented for zero-centered priors")#
  pi1 = pi0 * t(matrix_dens(betahat,sebetahat,sigma0))#
  pi1 = apply(pi1, 2, normalize) #pi1 is now an k by n matrix#
  #make k by n matrix versions of sigma0^2 and sebetahat^2#
  # and mu0 and betahat#
  s0m2 = matrix(sigma0^2,nrow=k,ncol=n,byrow=FALSE)#
  sebm2 = matrix(sebetahat^2,nrow=k,ncol=n, byrow=TRUE)#
  mu0m = matrix(mu0,nrow=k,ncol=n,byrow=FALSE)#
  bhatm = matrix(betahat,nrow=k,ncol=n,byrow=TRUE)#
  sigma1 = (s0m2*sebm2/(s0m2 + sebm2))^(0.5)  #
  w = sebm2/(s0m2 + sebm2)#
  mu1 = w*mu0m + (1-w)*bhatm#
  #WHERE DATA ARE MISSING, SET POSTERIOR = PRIOR#
  ismiss = (is.na(betahat) | is.na(sebetahat)) #
  pi1[,ismiss] = pi0#
  mu1[,ismiss] = mu0#
  sigma1[,ismiss] = sigma0#
  return(list(pi=pi1,mu=mu1,sigma=sigma1))#
}#
#
#return matrix of densities of observations (betahat) #
# assuming betahat_j \sim N(0, sebetahat_j^2 + sigmaavec_k^2)#
#normalized by maximum of each column#
#INPUT#
#betahat is n vector, #
#sebetahat is n vector, #
#sigmaavec is k vector#
#return is n by k matrix of the normal likelihoods, #
# with (j,k)th element the density of N(betahat_j; mean=0, var = sebetahat_j^2 + sigmaavec_k^2)#
#normalized to have maximum 1 in each column#
matrix_dens = function(betahat, sebetahat, sigmaavec){#
  k = length(sigmaavec)#
  n = length(betahat)#
  ldens = dnorm(betahat,0,sqrt(outer(sebetahat^2,sigmaavec^2,FUN="+")),log=TRUE)#
  maxldens = apply(ldens, 1, max)#
  ldens = ldens - maxldens#
  return(exp(ldens))#
}#
#
#return the "effective" estimate#
#that is the effect size betanew whose z score betanew/se#
#would give the same p value as betahat/se compared to a t with df#
effective.effect=function(betahat,se,df){#
  p = pt(betahat/se,df)#
  qnorm(p,sd=se)#
}#
#' @title Function to compute q values from local false discovery rates#
#'#
#' @description Computes q values from a vector of local fdr estimates#
#'#
#' @details The q value for a given lfdr is an estimate of the (tail) False Discovery Rate #
#' for all findings with a smaller lfdr, and is found by the average of the lfdr for#
#' all more significant findings. See Storey (2003), Annals of Statistics, for definition of q value.  #
#' #
#' #
#' @param lfdr, a vector of local fdr estimates#
#'#
#' @return vector of q values#
#' #
#' @export#
qval.from.lfdr = function(lfdr){#
  o = order(lfdr)#
  qvalue=rep(NA,length(lfdr))#
  qvalue[o] = (cumsum(sort(lfdr))/(1:sum(!is.na(lfdr))))#
  return(qvalue)#
}#
#
# try to select a default range for the sigmaa values#
# that should be used, based on the values of betahat and sebetahat#
# mult is the multiplier by which the sds differ across the grid#
autoselect.mixsd = function(betahat,sebetahat,mult){#
  sigmaamin = min(sebetahat)/10 #so that the minimum is small compared with measurement precision#
  if(all(betahat^2<sebetahat^2)){#
    sigmaamax = 8*sigmaamin #to deal with the occassional odd case where this could happen; 8 is arbitrary#
  } else {#
    sigmaamax = 2*sqrt(max(betahat^2-sebetahat^2)) #this computes a rough largest value you'd want to use, based on idea that sigmaamax^2 + sebetahat^2 should be at least betahat^2   #
  }#
  if(mult==0){#
    return(c(0,sigmaamax/2))#
  }else{#
    npoint = ceiling(log2(sigmaamax/sigmaamin)/log2(mult))#
    return(mult^((-npoint):0) * sigmaamax)#
  }#
}#
#return the KL-divergence between 2 dirichlet distributions#
#p,q are the vectors of dirichlet parameters of same lengths#
diriKL = function(p,q){#
  p.sum = sum(p)#
  q.sum = sum(q)#
  k = length(q)#
  KL = lgamma(q.sum)-lgamma(p.sum)+sum((q-p)*(digamma(q)-digamma(rep(q.sum,k))))+sum(lgamma(p)-lgamma(q))#
  return(KL)#
}#
#
#helper function for VBEM#
VB.update = function(matrix_lik, pipost){#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost * matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  B = sum(classprob*log(avgpipost*matrix_lik),na.rm=TRUE) - diriKL(prior,pipost) #negative free energy#
  return(list(classprob=classprob,B=B))#
}
if(nonzeromean & is.null(df)){#
}
' @useDynLib ashr#
#todo#
##
#' @title Main Adaptive SHrinkage function#
#'#
#' @description Takes vectors of estimates (betahat) and their standard errors (sebetahat), and applies#
#' shrinkage to them, using Empirical Bayes methods, to compute shrunk estimates for beta.#
#'#
#' @details See readme for more details#
#' #
#' @param betahat, a p vector of estimates #
#' @param sebetahat, a p vector of corresponding standard errors#
#' @param method: specifies how ash is to be run. Can be "shrinkage" (if main aim is shrinkage) or "fdr" (if main aim is to assess fdr or fsr)#
#' This is simply a convenient way to specify certain combinations of parameters: "shrinkage" sets pointmass=FALSE and prior="uniform";#
#' "fdr" sets pointmass=TRUE and prior="nullbiased".#
#' @param mixcompdist: distribution of components in mixture ("normal", "uniform" or "halfuniform")#
#'#
#' @param lambda1: multiplicative "inflation factor" for standard errors (like Genomic Control)#
#' @param lambda2: additive "inflation factor" for standard errors (like Genomic Control)#
#' @param nullcheck: whether to check that any fitted model exceeds the "null" likelihood#
#' in which all weight is on the first component#
#' @param df: appropriate degrees of freedom for (t) distribution of betahat/sebetahat#
#' @param randomstart: bool, indicating whether to initialize EM randomly. If FALSE, then initializes to prior mean (for EM algorithm) or prior (for VBEM)#
#' @param nullweight: scalar, the weight put on the prior of null under "fdr" method#
#' @param nonzeromean: bool, indicating whether to use a nonzero mean unimodal mixture(defaults to "FALSE")#
#' @param pointmass: bool, indicating whether to use a point mass at zero as one of components for a mixture distribution#
#' @param onlylogLR: bool, indicating whether to use this function to get logLR. Skip posterior prob, posterior mean, lfdr...#
#' @param prior: string, or numeric vector indicating Dirichlet prior on mixture proportions (defaults to "uniform", or 1,1...,1; also can be "nullbiased" 1,1/k-1,...,1/k-1 to put more weight on first component)#
#' @param mixsd: vector of sds for underlying mixture components #
#' @param VB: whether to use Variational Bayes to estimate mixture proportions (instead of EM to find MAP estimate)#
#' @param gridmult: the multiplier by which the default grid values for mixsd differ by one another. (Smaller values produce finer grids)#
#' @param minimal_output: if TRUE, just outputs the fitted g and the lfsr (useful for very big data sets where memory is an issue) #
#' @param g: the prior distribution for beta (usually estimated from the data; this is used primarily in simulated data to do computations with the "true" g)#
#' @param maxiter: maximum number of iterations of the EM algorithm#
#' @param cxx: flag to indicate whether to use the c++ (Rcpp) version#
#' #
#'#
#' @return a list with elements fitted.g is fitted mixture#
#' logLR : logP(D|mle(pi)) - logP(D|null)#
#' #
#' @export#
#' #
#' @examples #
#' beta = c(rep(0,100),rnorm(100))#
#' sebetahat = abs(rnorm(200,0,1))#
#' betahat = rnorm(200,beta,sebetahat)#
#' beta.ash = ash(betahat, sebetahat)#
#' summary(beta.ash)#
#' plot(betahat,beta.ash$PosteriorMean,xlim=c(-4,4),ylim=c(-4,4))#
#' #
#' #
#Things to do:#
# check sampling routine#
# check number of iterations#
ash = function(betahat,sebetahat,method = c("shrink","fdr"), #
               mixcompdist = c("normal","uniform","halfuniform"),#
               lambda1=1,lambda2=0,nullcheck=TRUE,df=NULL,randomstart=FALSE,#
               nullweight=10,nonzeromean=FALSE, #
               pointmass = FALSE, #
               onlylogLR = FALSE, #
               prior=c("uniform","nullbiased"), #
               mixsd=NULL, VB=FALSE,gridmult=sqrt(2),#
               minimaloutput=FALSE,#
               multiseqoutput=FALSE,#
               g=NULL,#
               maxiter = 5000,#
               cxx=FALSE){#
  #method provides a convenient interface to set a particular combinations of parameters for prior an#
  #If method is supplied, use it to set up specific values for these parameters; provide warning if values#
  #are also specified by user#
  #If method is not supplied use the user-supplied values (or defaults if user does not specify them)#
  if(!missing(method)){#
    method = match.arg(method) #
    if(method=="shrink"){#
      if(missing(prior)){#
        prior = "uniform"#
      } else {#
        warning("Specification of prior overrides default for method shrink")#
      }#
      if(missing(pointmass)){#
        pointmass=FALSE#
      } else {#
        warning("Specification of pointmass overrides default for method shrink")#
      }#
    }#
    if(method=="fdr"){#
      if(missing(prior)){#
        prior = "nullbiased"#
      } else {#
        warning("Specification of prior overrides default for method fdr")#
      }#
      if(missing(pointmass)){#
        pointmass=TRUE#
      } else {#
        warning("Specification of pointmass overrides default for method fdr")#
      }#
    }  #
  }#
  if(gridmult<=1&multiseqoutput!=TRUE)#
    stop("gridmult must be > 1")#
  mixcompdist = match.arg(mixcompdist)#
  # if(mixcompdist=="uniform" & pointmass==TRUE){#
  #    stop("point mass not yet implemented for uniform or half-uniform")#
  #  }#
  #  if(mixcompdist=="halfuniform" & pointmass==TRUE){#
  #    stop("point mass not yet implemented for uniform or half-uniform")#
  #  }#
  if(!is.numeric(prior)){#
    prior = match.arg(prior)#
  }  #
  if(length(sebetahat)==1){#
    sebetahat = rep(sebetahat,length(betahat))#
  }#
  if(length(sebetahat) != length(betahat)){#
    stop("Error: sebetahat must have length 1, or same length as betahat")#
  }#
  completeobs = (!is.na(betahat) & !is.na(sebetahat))#
  n=sum(completeobs)#
  if(n==0){#
    if(onlylogLR){#
      return(list(pi=NULL, logLR = 0))#
    }#
    else{#
      stop("Error: all input values are missing")#
    }#
  }  #
  if(!is.null(g)){#
    maxiter = 1 # if g is specified, don't iterate the EM#
    prior = rep(1,ncomp(g)) #prior is not actually used if g specified, but required to make sure EM doesn't produce warning#
    null.comp=1 #null.comp also not used, but required #
  } else {#
    if(is.null(mixsd)){#
      mixsd = autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)#
    }#
    if(pointmass){#
      mixsd = c(0,mixsd)#
    }#
    if(nonzeromean & is.null(df)){#
		nonzeromean.fit=nonzeromeanEM(betahat[completeobs], sebetahat[completeobs], mixsd, maxiter=maxiter)#
		betahat[completeobs]= betahat[completeobs] - nonzeromean.fit$nonzeromean#
	}#
	else if(nonzeromean & !is.null(df)){#
		stop("Error: Nonzero mean only implemented for df=NULL")#
	}#
    null.comp = which.min(mixsd) #which component is the "null"#
    k = length(mixsd)#
    if(!is.numeric(prior)){#
      if(prior=="nullbiased"){ # set up prior to favour "null"#
        prior = rep(1,k)#
        prior[null.comp] = nullweight #prior 10-1 in favour of null by default#
      }else if(prior=="uniform"){#
        prior = rep(1,k)#
      }#
    }#
    if(length(prior)!=k | !is.numeric(prior)){#
      stop("invalid prior specification")#
    }#
    if(randomstart){#
      pi = rgamma(k,1,1)#
    } else {#
      if(k<n){#
        pi=rep(1,k)/n #default initialization strongly favours null; puts weight 1/n on everything except null#
        pi[null.comp] = (n-k+1)/n #the motivation is data can quickly drive away from null, but tend to drive only slowly toward null.#
      } else {#
        pi=rep(1,k)/k#
      }#
    }#
    pi=normalize(pi)#
    if(!is.element(mixcompdist,c("normal","uniform","halfuniform"))) stop("Error: invalid type of mixcompdist")#
    if(mixcompdist=="normal") g=normalmix(pi,rep(0,k),mixsd)#
    if(mixcompdist=="uniform") g=unimix(pi,-mixsd,mixsd)#
    if(mixcompdist=="halfuniform"){#
      g = unimix(c(pi,pi)/2,c(-mixsd,rep(0,k)),c(rep(0,k),mixsd))#
      prior = rep(prior, 2)#
      pi = rep(pi, 2)#
    }#
  }#
  pi.fit=EMest(betahat[completeobs],lambda1*sebetahat[completeobs]+lambda2,g,prior,null.comp=null.comp,nullcheck=nullcheck,VB=VB,maxiter = maxiter, cxx=cxx, df=df)  #
  if (!onlylogLR){#
      n=length(betahat)#
      if (!multiseqoutput){#
          ZeroProb = rep(0,length=n)#
          NegativeProb = rep(0,length=n)#
      }#
      if (!minimaloutput){#
          PosteriorMean = rep(0,length=n)#
          PosteriorSD = rep(0,length=n)#
      }#
      if (!multiseqoutput){#
          ZeroProb[completeobs] = colSums(comppostprob(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)[comp_sd(pi.fit$g)==0,,drop=FALSE])#
          NegativeProb[completeobs] = cdf_post(pi.fit$g, 0, betahat[completeobs],sebetahat[completeobs],df) - ZeroProb[completeobs]#
      }#
      if (!minimaloutput){#
          PosteriorMean[completeobs] = postmean(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
          PosteriorSD[completeobs] = postsd(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
      }#
                                        #FOR MISSING OBSERVATIONS, USE THE PRIOR INSTEAD OF THE POSTERIOR#
      if (!multiseqoutput){#
          ZeroProb[!completeobs] = sum(mixprop(pi.fit$g)[comp_sd(pi.fit$g)==0])#
          NegativeProb[!completeobs] = mixcdf(pi.fit$g,0)#
          lfsr = compute_lfsr(NegativeProb,ZeroProb)#
      }#
      if (!minimaloutput){#
          PosteriorMean[!completeobs] = mixmean(pi.fit$g)#
          PosteriorSD[!completeobs] = mixsd(pi.fit$g)#
      }#
      if (!minimaloutput & !multiseqoutput){#
          PositiveProb = 1- NegativeProb-ZeroProb#
          lfsra = compute_lfsra(PositiveProb,NegativeProb,ZeroProb) #
          lfdr = ZeroProb#
          qvalue = qval.from.lfdr(lfdr)#
      }#
  }#
  if (!minimaloutput)#
      logLR = tail(pi.fit$loglik,1) - pi.fit$null.loglik#
  if (onlylogLR)#
      return(list(fitted.g=pi.fit$g, logLR = logLR, df=df))#
  else if (minimaloutput)#
      return(list(fitted.g = pi.fit$g, lfsr = lfsr, fit = pi.fit,df=df))#
  else if (multiseqoutput)#
      return(list(fitted.g = pi.fit$g, logLR = logLR, PosteriorMean = PosteriorMean, PosteriorSD = PosteriorSD, call= match.call(),df=df))#
  else{#
      result = list(fitted.g = pi.fit$g, logLR = logLR, PosteriorMean = PosteriorMean, PosteriorSD = PosteriorSD, PositiveProb = PositiveProb, NegativeProb = NegativeProb, ZeroProb = ZeroProb, lfsr = lfsr,lfsra = lfsra, lfdr = lfdr, qvalue = qvalue, fit = pi.fit, lambda1 = lambda1, lambda2 = lambda2, call = match.call(), data = list(betahat = betahat, sebetahat=sebetahat),df=df)#
      class(result) = "ash"#
      return(result)#
  }#
}#
  #if(nsamp>0){#
  #  sample = posterior_sample(post,nsamp)#
  #}#
#
# #' @title Faster version of function ash#
# #'#
# #' @description This function has similar functionality as ash, but only returns some of the outputs.#
# #'#
# #' @param betahat, a p vector of estimates#
# #' @param sebetahat, a p vector of corresponding standard errors#
# #' @param nullcheck: whether to check that any fitted model exceeds the "null" likelihood in which all weight is on the first component#
# #' @param randomstart: bool, indicating whether to initialize EM randomly. If FALSE, then initializes to prior mean (for EM algorithm) or prior (for VBEM)#
# #' @param pointmass: bool, indicating whether to use a point mass at zero as one of components for a mixture distribution#
# #' @param onlylogLR: bool, indicating whether to use this function to get logLR. Skip posterior prob, posterior mean, lfdr...#
# #' @param prior: string, or numeric vector indicating Dirichlet prior on mixture proportions (defaults to "uniform", or 1,1...,1; also can be "nullbiased" 1,1/k-1,...,1/k-1 to put more weight on first component)#
# #' @param mixsd: vector of sds for underlying mixture components#
# #' @param VB: whether to use Variational Bayes to estimate mixture proportions (instead of EM to find MAP estimate)#
# #' @param gridmult: the multiplier by which the default grid values for mixsd differ by one another. (Smaller values produce finer grids)#
# #' @param g: the prior distribution for beta (usually estimated from the data; this is used primarily in simulated data to do computations with the "true" g)#
# #' @param cxx: flag to indicate whether to use the c++ (Rcpp) version#
# #'#
# #' @return a list with elements fitted.g is fitted mixture#
# #' logLR : logP(D|mle(pi)) - logP(D|null)#
# #'#
# #' @export#
# fast.ash = function(betahat,sebetahat, #
#                     nullcheck=TRUE,randomstart=FALSE, #
#                     pointmass = TRUE,    #
#                     prior=c("nullbiased","uniform"), #
#                     mixsd=NULL, VB=FALSE,gridmult=4,#
#                     g=NULL, cxx=TRUE,#
#                     onlylogLR = FALSE,df=NULL){#
#   #
#   if(onlylogLR){#
#     pointmass <- TRUE  #
#   }#
#   #
#   #If method is supplied, use it to set up defaults; provide warning if these default values#
#   #are also specified by user#
#   if(!is.numeric(prior)){#
#     prior = match.arg(prior)#
#   }#
#   #
#   if(length(sebetahat)==1){#
#     sebetahat = rep(sebetahat,length(betahat))#
#   }#
#   if(length(sebetahat) != length(betahat)){#
#     stop("Error: sebetahat must have length 1, or same length as betahat")#
#   }#
#   #
#   completeobs = (!is.na(betahat) & !is.na(sebetahat))#
#   if(sum(completeobs)==0){#
#     if(onlylogLR){#
#       return(list(pi=NULL, logLR = 0))#
#     }else{#
#       stop("Error: all input values are missing")#
#     }#
#   }  #
#   #
#   if(is.null(mixsd)){#
#     mixsd= autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)#
#   }#
#   if(pointmass){#
#     mixsd = c(0,mixsd)#
#   }#
#   #
#   k=length(mixsd)  #
#   null.comp = which.min(mixsd) #which component is the "null"#
#   #
#   if(!is.numeric(prior)){#
#     if(prior=="nullbiased"){ # set up prior to favour "null"#
#       prior = rep(1,k)#
#       prior[null.comp] = 10 #prior 10-1 in favour of null#
#     }else if(prior=="uniform"){#
#       prior = rep(1,k)#
#     }#
#   }#
#   #
#   if(length(prior)!=k | !is.numeric(prior)){#
#     stop("invalid prior specification")#
#   }#
#   #
#   if(missing(g)){#
#     pi = prior^2 #default is to initialize pi at prior (mean)#
#     if(randomstart){pi=rgamma(k,1,1)}#
#     pi=normalize(pi)#
#     g=normalmix(pi,rep(0,k),mixsd)#
#     maxiter = 5000#
#   } else {#
#     maxiter = 1; # if g is specified, don't iterate the EM #
#   }#
#   #
#   pi.fit=EMest(betahat[completeobs],sebetahat[completeobs],g,prior,null.comp=null.comp,nullcheck=nullcheck,VB=VB,maxiter = maxiter, cxx=cxx, df=df)  #
#   #
#   if(onlylogLR){#
#     logLR = tail(pi.fit$loglik,1) - pi.fit$null.loglik#
#     return(list(pi=pi.fit$pi, logLR = logLR))#
#   }else{#
#     #
#     n=length(betahat)#
#     PosteriorMean = rep(0,length=n)#
#     PosteriorSD=rep(0,length=n)#
#     #
#     if(is.null(df)){#
#       PosteriorMean[completeobs] = postmean(pi.fit$g,betahat[completeobs],sebetahat[completeobs])#
#       PosteriorSD[completeobs] =postsd(pi.fit$g,betahat[completeobs],sebetahat[completeobs]) #
#     }#
#     else{#
#       PosteriorMean[completeobs] = postmean_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
#       PosteriorSD[completeobs] =postsd_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
#     }#
#     #FOR MISSING OBSERVATIONS, USE THE PRIOR INSTEAD OF THE POSTERIOR#
#     PosteriorMean[!completeobs] = mixmean(pi.fit$g)#
#     PosteriorSD[!completeobs] =mixsd(pi.fit$g)  #
#     #
#     result = list(fitted.g=pi.fit$g,PosteriorMean = PosteriorMean,PosteriorSD=PosteriorSD,call=match.call(),data=list(betahat = betahat, sebetahat=sebetahat))#
#     return(result)#
#   }#
#   #if(nsamp>0){#
#   #  sample = posterior_sample(post,nsamp)#
#   #}#
# }#
compute_lfsr = function(NegativeProb,ZeroProb){#
  ifelse(NegativeProb> 0.5*(1-ZeroProb),1-NegativeProb,NegativeProb+ZeroProb)#
}#
#
compute_lfsra = function(PositiveProb, NegativeProb,ZeroProb){#
  ifelse(PositiveProb<NegativeProb,2*PositiveProb+ZeroProb,2*NegativeProb+ZeroProb)  #
}  #
#
#' @title Estimate unimodal nonzero mean of a mixture model by EM algorithm#
#'#
#' @description Given the data, standard error of the data and standard deviations of the Gaussian mixture model, estimate the mean of a unimodal Gaussian mixture by an EM algorithm.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates unimodal mean \eqn{\mu} by EM algorithm. Uses the SQUAREM package to accelerate convergence of EM. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' #
#' @param betahat, a p vector of estimates #
#' @param sebetahat, a p vector of corresponding standard errors#
#' @param mixsd: vector of sds for underlying mixture components #
#' @param pi.init, the initial value of \eqn{\pi} to use. If not specified defaults to (1/k,...,1/k).#
#' @param tol, the tolerance for convergence of log-likelihood.#
#' @param maxiter the maximum number of iterations performed#
#' #
#' @return A list, including the estimates (\eqn{\mu}) and (\eqn{\pi}), the log likelihood for each iteration (NQ)#
#' and a flag to indicate convergence#
#'  #
#' @export#
#' #
#' #
nonzeromeanEM = function(betahat, sebetahat, mixsd, pi.init=NULL,tol=1e-7,maxiter=5000){#
  if(is.null(pi.init)){#
    pi.init = rep(1/length(mixsd),length(mixsd))# Use as starting point for pi#
  }#
  mupi=c(mean(betahat),pi.init)#
  res=squarem(par=mupi,fixptfn=nonzeromeanEMfixpoint,objfn=nonzeromeanEMobj,betahat=betahat,sebetahat=sebetahat,mixsd=mixsd,control=list(maxiter=maxiter,tol=tol))#
  return(list(nonzeromean=res$par[1],pi=res$par[-1],NQ=-res$value.objfn,niter = res$iter, converged=res$convergence,post=res$par))#
}#
#
nonzeromeanEMfixpoint = function(mupi,betahat,sebetahat,mixsd){#
	#omegamatrix=matrix(NA,nrow=length(betahat),ncol=length(mixsd))#
	mu=mupi[1]#
	pimean=mupi[-1]#
	sdmat = sqrt(outer(sebetahat ^2,mixsd^2,"+")) #
	xmat=matrix(rep(betahat,length(mixsd)),ncol=length(mixsd))#
	omegamatrix=t(t(dnorm(xmat,mean=mu,sd=sdmat))*pimean)#
	omegamatrix=omegamatrix /rowSums(omegamatrix)#
	pinew=normalize(colSums(omegamatrix))#
	munew=sum(omegamatrix*xmat/(sdmat^2))/sum(omegamatrix/(sdmat^2))#
	mupi=c(munew,pinew)#
	return(mupi)#
}#
#
nonzeromeanEMobj = function(mupi,betahat,sebetahat,mixsd){#
	mu=mupi[1]#
	pimean=mupi[-1]#
	sdmat = sqrt(outer(sebetahat ^2,mixsd^2,"+")) #
	xmat=matrix(rep(betahat,length(mixsd)),ncol=length(mixsd))#
	omegamatrix=t(t(dnorm(xmat,mean=mu,sd=sdmat))*pimean)#
	omegamatrix=omegamatrix /rowSums(omegamatrix)#
	NegativeQ=-sum(omegamatrix*dnorm(xmat,mean=mu,sd=sdmat,log=TRUE))#
	return(NegativeQ)#
}#
#' @title Estimate posterior distribution on mixture proportions of a mixture model by a Variational Bayes EM algorithm#
#'#
#' @description Given the individual component likelihoods for a mixture model, estimates the posterior on #
#' the mixture proportions by an VBEM algorithm. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates posterior on mixture proportions \eqn{\pi} by Variational Bayes, #
#' with a Dirichlet prior on \eqn{\pi}. #
#' Algorithm adapted from Bishop (2009), Pattern Recognition and Machine Learning, Chapter 10.#
#' #
#' @param matrix_lik: a n by k matrix with (j,k)th element equal to \eqn{f_k(x_j)}.#
#' @param prior: a k vector of the parameters of the Dirichlet prior on \eqn{\pi}. Recommended to be rep(1,k)#
#' @param post.init: the initial value of the posterior parameters. If not specified defaults to the prior parameters.#
#' @param tol: the tolerance for convergence of log-likelihood bound.#
#' @param maxiter: the maximum number of iterations performed#
#' #
#' @return A list, whose components include point estimates (pihat), #
#' the parameters of the fitted posterior on \eqn{\pi} (pipost),#
#' the bound on the log likelihood for each iteration (B)#
#' and a flag to indicate convergence (converged).#
#'  #
#' @export#
#' #
mixVBEM = function(matrix_lik, prior, pi.init = NULL,tol=1e-7, maxiter=5000){#
  k=ncol(matrix_lik)#
  if(is.null(pi.init)){#
    pi.init = rep(1,k)# Use as starting point for pi#
  } #
  res = squarem(par=pi.init,fixptfn=VBfixpoint, objfn=VBnegpenloglik,matrix_lik=matrix_lik, prior=prior, control=list(maxiter=maxiter,tol=tol))#
  return(list(pihat = res$par/sum(res$par), B=res$value.objfn, niter = res$iter, converged=res$convergence,post=res$par))#
}#
VBfixpoint = function(pipost, matrix_lik, prior){  #
  n=nrow(matrix_lik)#
  k=ncol(matrix_lik)#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost*matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  pipostnew = colSums(classprob) + prior#
  return(pipostnew)#
}#
#
VBnegpenloglik=function(pipost,matrix_lik,prior){#
  return(-VBpenloglik(pipost,matrix_lik,prior))#
}#
#
VBpenloglik = function(pipost, matrix_lik, prior){#
  n=nrow(matrix_lik)#
  k=ncol(matrix_lik)#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost*matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  B= sum(classprob*log(avgpipost*matrix_lik),na.rm=TRUE) - diriKL(prior,pipost) - sum(classprob*log(classprob)) #
  return(B)#
}#
#' @title Estimate mixture proportions of a mixture model by EM algorithm#
#'#
#' @description Given the individual component likelihoods for a mixture model, estimates the mixture proportions by an EM algorithm.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates mixture proportions \eqn{\pi} by maximum likelihood, or by maximum a posteriori (MAP) estimation for a Dirichlet prior on $\pi$ #
#' (if a prior is specified).  Uses the SQUAREM package to accelerate convergence of EM. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' #
#' @param matrix_lik, a n by k matrix with (j,k)th element equal to \eqn{f_k(x_j)}.#
#' @param prior, a k vector of the parameters of the Dirichlet prior on \eqn{\pi}. Recommended to be rep(1,k)#
#' @param pi.init, the initial value of \eqn{\pi} to use. If not specified defaults to (1/k,...,1/k).#
#' @param tol, the tolerance for convergence of log-likelihood.#
#' @param maxiter the maximum number of iterations performed#
#' #
#' @return A list, including the estimates (pihat), the log likelihood for each interation (B)#
#' and a flag to indicate convergence#
#'  #
#' @export#
#' #
#' #
mixEM = function(matrix_lik, prior, pi.init = NULL,tol=1e-7, maxiter=5000){#
  if(is.null(pi.init)){#
    pi.init = rep(1/k,k)# Use as starting point for pi#
  } #
  res = squarem(par=pi.init,fixptfn=fixpoint, objfn=negpenloglik,matrix_lik=matrix_lik, prior=prior, control=list(maxiter=maxiter,tol=tol))#
  return(list(pihat = normalize(pmax(0,res$par)), B=res$value.objfn, #
              niter = res$iter, converged=res$convergence))#
}#
#
# helper functions used by mixEM#
normalize = function(x){return(x/sum(x))}#
#
fixpoint = function(pi, matrix_lik, prior){  #
  pi = normalize(pmax(0,pi)) #avoid occasional problems with negative pis due to rounding#
  m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
  m.rowsum = rowSums(m)#
  classprob = m/m.rowsum #an n by k matrix#
  pinew = normalize(colSums(classprob) + prior - 1)#
  return(pinew)#
}#
#
negpenloglik = function(pi,matrix_lik,prior){return(-penloglik(pi,matrix_lik,prior))}#
#
penloglik = function(pi, matrix_lik, prior){#
  pi = normalize(pmax(0,pi))#
  m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
  m.rowsum = rowSums(m)#
  loglik = sum(log(m.rowsum))#
  subset = (prior != 1.0)#
  priordens = sum((prior-1)[subset]*log(pi[subset]))#
  return(loglik+priordens)#
}#
#
#The kth element of this vector is the derivative #
#of the loglik for $\pi=(\pi_0,...,1-\pi_0,...)$ with respect to $\pi_0$ at $\pi_0=1$.#
gradient = function(matrix_lik){#
  n = nrow(matrix_lik)#
  grad = n - colSums(matrix_lik/matrix_lik[,1]) #
  return(grad)#
}#
#
# mixEM = function(matrix_lik, prior, pi.init = NULL,tol=0.0001, maxiter=5000){#
#   n=nrow(matrix_lik)#
#   k=ncol(matrix_lik)#
#   B = rep(0,maxiter)#
#   pi = pi.init#
#   if(is.null(pi.init)){#
#     pi = rep(1/k,k)# Use as starting point for pi#
#   } #
#   pi = ifelse(pi<1e-5,1e-5,pi) #set any estimates that are too small to be just very small#
#   pi = normalize(pi)#
#   #
#   loglik = rep(0,maxiter)#
#   priordens= rep(0,maxiter)#
#   m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
#   m.rowsum = rowSums(m)#
#   loglik[1] = sum(log(m.rowsum))#
#   priordens[1] = sum((prior-1)*log(pi)) #
#   classprob = m/m.rowsum #an n by k matrix#
#   i=1#
#   if(maxiter >= 2){#
#     for(i in 2:maxiter){  #
#       pi = colSums(classprob) + prior-1#
#       pi = ifelse(pi<1e-5,1e-5,pi) #set any estimates that are less than zero, which can happen with prior<1, to 0#
#       pi = normalize(pi)#
#         #
#       #Now re-estimate pi#
#       m  = t(pi * t(matrix_lik)) #
#       m.rowsum = rowSums(m)#
#       loglik[i] = sum(log(m.rowsum))#
#       priordens[i] = sum((prior-1)*log(pi)) #
#       classprob = m/m.rowsum#
#     #
#     #
#       if(abs(loglik[i]+priordens[i]-loglik[i-1]-priordens[i-1])<tol) break;#
#     }#
#   }#
#   converged=(abs(loglik[i]+priordens[i]-loglik[i-1]-priordens[i-1])<tol)#
#   if(!converged){#
#       warning("EM algorithm in function mixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
#   }#
#   return(list(pihat = pi, B=loglik[1:i], #
#               niter = i, converged=converged))#
# }#
#estimate mixture proportions of sigmaa by EM algorithm#
#prior gives the parameter of a Dirichlet prior on pi#
#(prior is used to encourage results towards smallest value of sigma when#
#likelihood is flat)#
#nullcheck indicates whether to check whether the loglike exceeds the null#
#(may not want to use if prior is used)#
#VB provides an approach to estimate the approximate posterior distribution#
#of mixture proportions of sigmaa by variational Bayes method#
#(use Dirichlet prior and approximate Dirichlet posterior)#
#if cxx TRUE use cpp version of R function mixEM#
EMest = function(betahat,sebetahat,g,prior,null.comp=1,nullcheck=TRUE,VB=FALSE, maxiter=5000, cxx=TRUE, df=NULL){ #
  pi.init = g$pi#
  k=ncomp(g)#
  n = length(betahat)#
  tol = min(0.1/n,1e-5) # set convergence criteria to be more stringent for larger samples#
  if(is.null(df)){#
    matrix_lik = t(compdens_conv(g,betahat,sebetahat))#
  }#
  else{#
    matrix_lik = t(compdens_conv_t(g,betahat,sebetahat,df))#
  }#
  #checks whether the gradient at pi0=1 is positive (suggesting that this is a fixed point)#
  #if(nullcheck){#
  #  if(all(gradient(matrix_lik)>=0)){#
  #    pi.init=rep(0,k)#
  #    pi.init[null.comp]=1 #this will make pi.init=(1,0,0...,0) which is a fixed point of the EM#
  #  }#
  #}#
  if(VB==TRUE){#
    EMfit=mixVBEM(matrix_lik,prior,maxiter=maxiter)}#
  else{#
    if (cxx==TRUE){#
      EMfit = cxxMixEM(matrix_lik,prior,pi.init,1e-5, maxiter) #currently use different convergence criteria for cxx version #
      if(!EMfit$converged){#
        warning("EM algorithm in function cxxMixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
      }#
    }#
    else{#
      EMfit = mixEM(matrix_lik,prior,pi.init,tol, maxiter)#
      if(!EMfit$converged & !(maxiter==1)){#
        warning("EM algorithm in function mixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
      }#
    }#
  }#
  pi = EMfit$pihat     #
  penloglik = EMfit$B #
  converged = EMfit$converged#
  niter = EMfit$niter#
  loglik.final =  penloglik(pi,matrix_lik,1) #compute penloglik without penalty#
  null.loglik = sum(log(matrix_lik[,null.comp]))  #
  if(nullcheck==TRUE & VB==FALSE){ #null check doesn't work with VB yet#
    pinull = rep(0,k)#
    pinull[null.comp]=1#
    null.penloglik = penloglik(pinull,matrix_lik,prior)#
    final.penloglik = penloglik(pi,matrix_lik,prior)#
    if(null.penloglik > final.penloglik){ #check whether exceeded "null" likelihood where everything is null#
      pi=pinull#
      loglik.final=penloglik(pi,matrix_lik,1)#
    }#
  }#
  g$pi=pi#
  return(list(loglik=loglik.final,null.loglik=null.loglik,#
              matrix_lik=matrix_lik,converged=converged,g=g))#
}#
#' @title Compute Posterior#
#'#
#' @description Return the posterior on beta given a prior (g) that is a mixture of normals (class normalmix) #
#' and observation betahat \sim N(beta,sebetahat)#
#'#
#' @details This can be used to obt#
#'#
#' @param g: a normalmix with components indicating the prior; works only if g has means 0#
#' @param betahat (n vector of observations) #
#' @param sebetahat (n vector of standard errors/deviations of observations)#
#' #
#' @return A list, (pi1,mu1,sigma1) whose components are each k by n matrices#
#' where k is number of mixture components in g, n is number of observations in betahat#
#' #
#' @export#
#' #
#' #
posterior_dist = function(g,betahat,sebetahat){#
  if(class(g)!="normalmix"){#
    stop("Error: posterior_dist implemented only for g of class normalmix")#
  }#
  pi0 = g$pi#
  mu0 = g$mean#
  sigma0 = g$sd  #
  k= length(pi0)#
  n= length(betahat)#
  if(!all.equal(g$mean,rep(0,k))) stop("Error: posterior_dist currently only implemented for zero-centered priors")#
  pi1 = pi0 * t(matrix_dens(betahat,sebetahat,sigma0))#
  pi1 = apply(pi1, 2, normalize) #pi1 is now an k by n matrix#
  #make k by n matrix versions of sigma0^2 and sebetahat^2#
  # and mu0 and betahat#
  s0m2 = matrix(sigma0^2,nrow=k,ncol=n,byrow=FALSE)#
  sebm2 = matrix(sebetahat^2,nrow=k,ncol=n, byrow=TRUE)#
  mu0m = matrix(mu0,nrow=k,ncol=n,byrow=FALSE)#
  bhatm = matrix(betahat,nrow=k,ncol=n,byrow=TRUE)#
  sigma1 = (s0m2*sebm2/(s0m2 + sebm2))^(0.5)  #
  w = sebm2/(s0m2 + sebm2)#
  mu1 = w*mu0m + (1-w)*bhatm#
  #WHERE DATA ARE MISSING, SET POSTERIOR = PRIOR#
  ismiss = (is.na(betahat) | is.na(sebetahat)) #
  pi1[,ismiss] = pi0#
  mu1[,ismiss] = mu0#
  sigma1[,ismiss] = sigma0#
  return(list(pi=pi1,mu=mu1,sigma=sigma1))#
}#
#
#return matrix of densities of observations (betahat) #
# assuming betahat_j \sim N(0, sebetahat_j^2 + sigmaavec_k^2)#
#normalized by maximum of each column#
#INPUT#
#betahat is n vector, #
#sebetahat is n vector, #
#sigmaavec is k vector#
#return is n by k matrix of the normal likelihoods, #
# with (j,k)th element the density of N(betahat_j; mean=0, var = sebetahat_j^2 + sigmaavec_k^2)#
#normalized to have maximum 1 in each column#
matrix_dens = function(betahat, sebetahat, sigmaavec){#
  k = length(sigmaavec)#
  n = length(betahat)#
  ldens = dnorm(betahat,0,sqrt(outer(sebetahat^2,sigmaavec^2,FUN="+")),log=TRUE)#
  maxldens = apply(ldens, 1, max)#
  ldens = ldens - maxldens#
  return(exp(ldens))#
}#
#
#return the "effective" estimate#
#that is the effect size betanew whose z score betanew/se#
#would give the same p value as betahat/se compared to a t with df#
effective.effect=function(betahat,se,df){#
  p = pt(betahat/se,df)#
  qnorm(p,sd=se)#
}#
#' @title Function to compute q values from local false discovery rates#
#'#
#' @description Computes q values from a vector of local fdr estimates#
#'#
#' @details The q value for a given lfdr is an estimate of the (tail) False Discovery Rate #
#' for all findings with a smaller lfdr, and is found by the average of the lfdr for#
#' all more significant findings. See Storey (2003), Annals of Statistics, for definition of q value.  #
#' #
#' #
#' @param lfdr, a vector of local fdr estimates#
#'#
#' @return vector of q values#
#' #
#' @export#
qval.from.lfdr = function(lfdr){#
  o = order(lfdr)#
  qvalue=rep(NA,length(lfdr))#
  qvalue[o] = (cumsum(sort(lfdr))/(1:sum(!is.na(lfdr))))#
  return(qvalue)#
}#
#
# try to select a default range for the sigmaa values#
# that should be used, based on the values of betahat and sebetahat#
# mult is the multiplier by which the sds differ across the grid#
autoselect.mixsd = function(betahat,sebetahat,mult){#
  sigmaamin = min(sebetahat)/10 #so that the minimum is small compared with measurement precision#
  if(all(betahat^2<sebetahat^2)){#
    sigmaamax = 8*sigmaamin #to deal with the occassional odd case where this could happen; 8 is arbitrary#
  } else {#
    sigmaamax = 2*sqrt(max(betahat^2-sebetahat^2)) #this computes a rough largest value you'd want to use, based on idea that sigmaamax^2 + sebetahat^2 should be at least betahat^2   #
  }#
  if(mult==0){#
    return(c(0,sigmaamax/2))#
  }else{#
    npoint = ceiling(log2(sigmaamax/sigmaamin)/log2(mult))#
    return(mult^((-npoint):0) * sigmaamax)#
  }#
}#
#return the KL-divergence between 2 dirichlet distributions#
#p,q are the vectors of dirichlet parameters of same lengths#
diriKL = function(p,q){#
  p.sum = sum(p)#
  q.sum = sum(q)#
  k = length(q)#
  KL = lgamma(q.sum)-lgamma(p.sum)+sum((q-p)*(digamma(q)-digamma(rep(q.sum,k))))+sum(lgamma(p)-lgamma(q))#
  return(KL)#
}#
#
#helper function for VBEM#
VB.update = function(matrix_lik, pipost){#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost * matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  B = sum(classprob*log(avgpipost*matrix_lik),na.rm=TRUE) - diriKL(prior,pipost) #negative free energy#
  return(list(classprob=classprob,B=B))#
}
GENERIC FUNCTIONS #############################
# find matrix of densities at y, for each component of the mixture#
# INPUT y is an n-vector#
# OUTPUT k by n matrix of densities#
compdens = function(x,y,log=FALSE){#
  UseMethod("compdens")#
}#
compdens.default = function(x,y,log=FALSE){#
  stop(paste("Invalid class", class(m), "for first argument in",  match.call()))  #
}#
#
#standard deviations#
comp_sd = function(m){#
  UseMethod("comp_sd")#
}#
comp_sd.default = function(m){#
  stop("method comp_sd not written for this class")#
}#
#
#second moments#
comp_mean2 = function(m){#
  UseMethod("comp_mean2")#
}#
comp_mean2.default = function(m){#
  comp_sd(m)^2 + comp_mean(m)^2#
}#
#return the overall mean of the mixture#
mixmean = function(m){#
  UseMethod("mixmean")#
}#
mixmean.default = function(m){#
  sum(m$pi * comp_mean(m))#
}#
#
#return the overall second moment of the mixture#
mixmean2 = function(m){#
  UseMethod("mixmean2")#
}#
mixmean2.default = function(m){#
  sum(m$pi * comp_mean2(m))#
}#
#
#return the overall sd of the mixture#
mixsd = function(m){#
  UseMethod("mixsd")#
}#
mixsd.default = function(m){#
  sqrt(mixmean2(m)-mixmean(m)^2)#
}#
#
#means#
comp_mean = function(m){#
  UseMethod("comp_mean")#
}#
comp_mean.default = function(m){#
  stop("method comp_mean not written for this class")#
}#
#
#number of components#
ncomp = function(m){#
  UseMethod("ncomp")#
}#
ncomp.default = function(m){#
  return(length(m$pi))#
}#
#
#return mixture proportions, a generic function#
mixprop = function(m){#
  UseMethod("mixprop")#
}#
mixprop.default = function(m){#
  m$pi#
}#
#
#' @title mixcdf#
#'#
#' @description Returns cdf for a mixture (generic function)#
#' #
#' @details None#
#' #
#' @param x a mixture (eg of type normalmix or unimix)#
#' @param y locations at which cdf to be computed#
#' @param lower.tail: boolean indicating whether to report lower tail#
#' #
#' @return an object of class normalmix#
#' #
#' @export#
#' #
#' @examples mixcdf(normalmix(c(0.5,0.5),c(0,0),c(1,2)),seq(-4,4,length=100))#
#' #
mixcdf = function(x,y,lower.tail=TRUE){#
  UseMethod("mixcdf")#
}#
#' @title mixcdf.default#
#' @export#
#' #
mixcdf.default = function(x,y,lower.tail=TRUE){#
  x$pi %*% comp_cdf(x,y,lower.tail)#
}#
#
#find cdf for each component, a generic function#
comp_cdf = function(x,y,lower.tail=TRUE){#
  UseMethod("comp_cdf")#
}#
comp_cdf.default = function(x,y,lower.tail=TRUE){#
  stop("comp_cdf not implemented for this class")#
}#
#find density at y, a generic function#
dens = function(x,y){#
  UseMethod("dens")#
}#
dens.default = function(x,y){#
  return (x$pi %*% compdens(x, y))#
}#
#
#find log likelihood of data in x (a vector) for mixture in m#
loglik = function(m,x){#
  UseMethod("loglik")#
}#
loglik.default = function(m,x){#
  sum(log(dens(m,x)))#
}#
#
#find log likelihood of data in betahat, when #
#the mixture m is convolved with a normal with sd betahatsd#
#betahatsd is an n vector#
#betahat is an n vector#
#v is the degree of freedom#
#' @title loglik_conv#
#' #
#' @export#
#' #
loglik_conv = function(m,betahat,betahatsd,v,FUN="+"){#
  UseMethod("loglik_conv")#
}#
#' @title loglik_conv.default#
#' #
#' @export#
#' #
loglik_conv.default = function(m,betahat,betahatsd,v,FUN="+"){#
  sum(log(dens_conv(m,betahat,betahatsd,v,FUN)))#
}#
#
#compute the density of the components of the mixture m#
#when convoluted with a normal with standard deviation s#
#or a scaled (se) student.t with df v#
#the density is evaluated at x#
#x and s are n-vectors#
#m is a mixture with k components#
#output is a k by n matrix of densities#
compdens_conv = function(m,x,s,v,FUN="+"){#
  UseMethod("compdens_conv")#
}#
compdens_conv.default = function(m,x,s,v,FUN="+"){#
  stop(paste("Invalid class", class(m), "for first argument in",  match.call()))  #
}#
#
#compute density of mixture m convoluted with normal of sd (s) or student t with df v#
#at locations x#
#m is a mixture#
#x is an n vector#
#s is an n vector or integer#
dens_conv = function(m,x,s,v,FUN="+"){#
  UseMethod("dens_conv")#
}#
dens_conv.default = function(m,x,s,v,FUN="+"){#
  colSums(m$pi * compdens_conv(m,x,s,v,FUN))#
}#
#
#compute the posterior prob that each observation#
#came from each component of the mixture m#
#output a k by n vector of probabilities#
#computed by weighting the component densities by pi#
#and then normalizing#
comppostprob=function(m,x,s,v){#
 UseMethod("comppostprob") #
}#
comppostprob.default = function(m,x,s,v){#
  tmp= (t(m$pi * compdens_conv(m,x,s,v))/dens_conv(m,x,s,v))#
  ismissing = (is.na(x) | is.na(s))#
  tmp[ismissing,]=m$pi#
  t(tmp)#
}#
# evaluate cdf of posterior distribution of beta at c#
# m is the prior on beta, a mixture#
# c is location of evaluation#
# assumption is betahat | beta \sim N(beta,sebetahat)#
# m is a mixture with k components#
# c a scalar#
# betahat, sebetahat are n vectors #
# output is a k by n matrix#
compcdf_post=function(m,c,betahat,sebetahat,v){#
  UseMethod("compcdf_post")#
}#
compcdf_post.default=function(m,c,betahat,sebetahat,v){#
  stop("method compcdf_post not written for this class")#
}#
cdf_post = function(m,c,betahat,sebetahat,v){#
  UseMethod("cdf_post")#
}#
cdf_post.default=function(m,c,betahat,sebetahat,v){#
  colSums(comppostprob(m,betahat,sebetahat,v)*compcdf_post(m,c,betahat,sebetahat,v))#
}#
#
#output posterior mean for beta for prior mixture m,#
#given observations betahat, sebetahat, df v#
postmean = function(m, betahat,sebetahat,v){#
  UseMethod("postmean")#
}#
postmean.default = function(m,betahat,sebetahat,v){#
  colSums(comppostprob(m,betahat,sebetahat,v) * comp_postmean(m,betahat,sebetahat,v))#
}#
#output posterior mean-squared value for beta for prior mixture m,#
#given observations betahat, sebetahat, df v#
postmean2 = function(m, betahat,sebetahat,v){#
  UseMethod("postmean2")#
}#
postmean2.default = function(m,betahat,sebetahat,v){#
  colSums(comppostprob(m,betahat,sebetahat,v) * comp_postmean2(m,betahat,sebetahat,v))#
}#
#
#output posterior sd for beta for prior mixture m,#
#given observations betahat, sebetahat, df v#
postsd = function(m,betahat,sebetahat,v){#
  UseMethod("postsd")#
}#
postsd.default = function(m,betahat,sebetahat,v){#
  sqrt(postmean2(m,betahat,sebetahat,v)-postmean(m,betahat,sebetahat,v)^2)#
}#
#
#output posterior mean-squared value for beta for prior mixture m,#
#given observations betahat, sebetahat, df v#
comp_postmean2 = function(m,betahat,sebetahat,v){#
  UseMethod("comp_postmean2")#
}#
comp_postmean2.default = function(m,betahat,sebetahat,v){#
  comp_postsd(m,betahat,sebetahat,v)^2 + comp_postmean(m,betahat,sebetahat,v)^2#
}#
#output posterior mean for beta for each component of prior mixture m,#
#given observations betahat, sebetahat, df v#
comp_postmean = function(m, betahat,sebetahat,v){#
  UseMethod("comp_postmean")#
}#
comp_postmean.default = function(m,betahat,sebetahat,v){#
  stop("method comp_postmean not written for this class")#
}#
#output posterior sd for beta for each component of prior mixture m,#
#given observations betahat, sebetahat, df v#
comp_postsd = function(m, betahat,sebetahat,v){#
  UseMethod("comp_postsd")#
}#
comp_postsd.default = function(m,betahat,sebetahat,v){#
  stop("method comp_postsd not written for this class")#
}#
#
#find nice limits of mixture m for plotting#
min_lim = function(m){#
  UseMethod("min_lim")#
}#
min_lim.default=function(m){#
  -5#
}#
#
max_lim = function(m){#
  UseMethod("max_lim")#
}#
max_lim.default=function(m){#
  5#
}#
#plot density of mixture#
plot_dens = function(m,npoints=100,...){#
  UseMethod("plot_dens")#
}#
plot_dens.default = function(m,npoints=100,...){#
  x = seq(min_lim(m),max_lim(m),length=npoints)#
  plot(x,dens(m,x),type="l",xlab="density",ylab="x",...)#
}#
#
plot_post_cdf = function(m,betahat,sebetahat,v,npoints=100,...){#
  UseMethod("plot_post_cdf")#
}#
plot_post_cdf.default = function(m,betahat,sebetahat,v,npoints=100,...){#
  x = seq(min_lim(m),max_lim(m),length=npoints)#
  x_cdf = vapply(x,cdf_post,FUN.VALUE=betahat,m=m,betahat=betahat,sebetahat=sebetahat,v=v)#
  plot(x,x_cdf,type="l",xlab="x",ylab="cdf",...)#
 # for(i in 2:nrow(x_cdf)){#
 #   lines(x,x_cdf[i,],col=i)#
 # }#
}#
#
############################### METHODS FOR normalmix class ############################
#
#' @title Constructor for normalmix class#
#'#
#' @description Creates an object of class normalmix (finite mixture of univariate normals)#
#' #
#' @details None#
#' #
#' @param pi vector of mixture proportions#
#' @param mean vector of means#
#' @param sd: vector of standard deviations#
#' #
#' @return an object of class normalmix#
#' #
#' @export#
#' #
#' @examples normalmix(c(0.5,0.5),c(0,0),c(1,2))#
#' #
normalmix = function(pi,mean,sd){#
  structure(data.frame(pi,mean,sd),class="normalmix")#
}#
#
comp_sd.normalmix = function(m){#
  m$sd#
}#
#
comp_mean.normalmix = function(m){#
  m$mean#
}#
#
compdens.normalmix = function(x,y,log=FALSE){#
  k=ncomp(x)#
  n=length(y)#
  d = matrix(rep(y,rep(k,n)),nrow=k)#
  return(matrix(dnorm(d, x$mean, x$sd, log),nrow=k))  #
}#
#
#density of convolution of each component of a normal mixture with N(0,s^2) or s*t(v) at x#
# x an n-vector at which density is to be evaluated#
#return a k by n matrix#
#Note that convolution of two normals is normal, so it works that way#
compdens_conv.normalmix = function(m,x,s,v,FUN="+"){#
  if(!is.null(v)){#
  	stop("method comp_postsd of normal mixture not written for df!null")#
  }#
  if(length(s)==1){s=rep(s,length(x))}#
  sdmat = sqrt(outer(s^2,m$sd^2,FUN)) #n by k matrix of standard deviations of convolutions#
  return(t(dnorm(outer(x,m$mean,FUN="-")/sdmat)/sdmat))#
}#
comp_cdf.normalmix = function(x,y,lower.tail=TRUE){#
  vapply(y,pnorm,x$mean,x$mean,x$sd,lower.tail)#
}#
#
#c is a scalar#
#m a mixture with k components#
#betahat a vector of n observations#
#sebetahat an n vector of standard errors#
#return a k by n matrix of the posterior cdf#
compcdf_post.normalmix=function(m,c,betahat,sebetahat,v){#
  if(!is.null(v)){#
  	stop("Error: normal mixture for student-t likelihood is not yet implemented")#
  }  #
  k = length(m$pi)#
  n=length(betahat)#
  #compute posterior standard deviation (s1) and posterior mean (m1)#
  s1 = sqrt(outer(sebetahat^2,m$sd^2,FUN="*")/outer(sebetahat^2,m$sd^2,FUN="+"))#
  ismissing = (is.na(betahat) | is.na(sebetahat))#
  s1[ismissing,]=m$sd#
  m1 = t(comp_postmean(m,betahat,sebetahat,v))#
  t(pnorm(c,mean=m1,sd=s1))#
}#
#
#return posterior mean for each component of prior m, given observations betahat and sebetahat#
#input, m is a mixture with k components#
#betahat, sebetahat are n vectors#
#output is a k by n matrix#
comp_postmean.normalmix = function(m,betahat,sebetahat,v){#
  if(!isnull(v)){#
  	stop("method comp_postmean of normal mixture not written for df!=NULL")#
  }#
  tmp=(outer(sebetahat^2,m$mean, FUN="*") + outer(betahat,m$sd^2, FUN="*"))/outer(sebetahat^2,m$sd^2,FUN="+")#
  ismissing = (is.na(betahat) | is.na(sebetahat))#
  tmp[ismissing,]=m$mean #return prior mean when missing data#
  t(tmp)#
}#
#return posterior standard deviation for each component of prior m, given observations betahat and sebetahat#
#input, m is a mixture with k components#
#betahat, sebetahat are n vectors#
#output is a k by n matrix#
comp_postsd.normalmix = function(m,betahat,sebetahat,v){#
  if(!is.null(v)){#
  	stop("method comp_postsd of normal mixture not written for df!=NULL")#
  }#
  t(sqrt(outer(sebetahat^2,m$sd^2,FUN="*")/outer(sebetahat^2,m$sd^2,FUN="+")))#
}#
############################### METHODS FOR unimix class ############################
#
#constructor; pi, a and b are vectors; kth component is Uniform(a[k],b[k])#
unimix = function(pi,a,b){#
  structure(data.frame(pi,a,b),class="unimix")#
}#
#
comp_cdf.unimix = function(m,y,lower.tail=TRUE){#
  vapply(y,punif,m$a,min=m$a,max=m$b,lower.tail)#
}#
#
comp_sd.unimix = function(m){#
  (m$b-m$a)/sqrt(12)#
}#
#
comp_mean.unimix = function(m){#
  (m$a+m$b)/2#
}#
compdens.unimix = function(x,y,log=FALSE){#
  k=ncomp(x)#
  n=length(y)#
  d = matrix(rep(y,rep(k,n)),nrow=k)#
  return(matrix(dunif(d, x$a, x$b, log),nrow=k))  #
}#
#
#density of convolution of each component of a unif mixture with N(0,s) at x#
# x an n-vector#
#return a k by n matrix#
compdens_conv.unimix = function(m,x,s,v, FUN="+"){#
  if(FUN!="+") stop("Error; compdens_conv not implemented for uniform with FUN!=+")#
  if(is.null(v)){#
    compdens= t(pnorm(outer(x,m$a,FUN="-")/s)-pnorm(outer(x,m$b,FUN="-")/s))/(m$b-m$a)#
    compdens[m$a==m$b,]=t(dnorm(outer(x,m$a,FUN="-")/s)/s)[m$a==m$b,]#
  }#
  else{#
    compdens= t(pt(outer(x,m$a,FUN="-")/s,df=v)-pt(outer(x,m$b,FUN="-")/s,df=v))/(m$b-m$a)#
    compdens[m$a==m$b,]=t(dt(outer(x,m$a,FUN="-")/s,df=v)/s)[m$a==m$b,]#
  }#
  return(compdens)#
}#
#c is a scalar#
#m a mixture with k components#
#betahat a vector of n observations#
#sebetahat an n vector of standard errors#
#return a k by n matrix of the posterior cdf#
compcdf_post.unimix=function(m,c,betahat,sebetahat,v){#
  k = length(m$pi)#
  n=length(betahat)#
  tmp = matrix(1,nrow=k,ncol=n)#
  tmp[m$a > c,] = 0#
  subset = m$a<=c & m$b>c # subset of components (1..k) with nontrivial cdf#
  if(sum(subset)>0){#
  	if(is.null(v)){#
      pna = pnorm(outer(betahat,m$a[subset],FUN="-")/sebetahat)#
      pnc = pnorm(outer(betahat,rep(c,sum(subset)),FUN="-")/sebetahat)#
      pnb = pnorm(outer(betahat,m$b[subset],FUN="-")/sebetahat)#
    }#
    else{#
      pna = pt(outer(betahat,m$a[subset],FUN="-")/sebetahat, df=v)#
      pnc = pt(outer(betahat,rep(c,sum(subset)),FUN="-")/sebetahat, df=v)#
      pnb = pt(outer(betahat,m$b[subset],FUN="-")/sebetahat, df=v)#
    }#
    tmp[subset,] = t((pnc-pna)/(pnb-pna))#
  }#
  subset = (m$a == m$b) #subset of components with trivial cdf#
  tmp[subset,]= rep(m$a[subset] <= c,n)#
  tmp#
}#
#
my_etruncnorm= function(a,b,mean=0,sd=1){#
  alpha = (a-mean)/sd#
  beta =  (b-mean)/sd#
 #Flip the onese where both are positive, as the computations are more stable#
  #when both negative#
  flip = (alpha>0 & beta>0)#
  flip[is.na(flip)]=FALSE #deal with NAs#
  alpha[flip]= -alpha[flip]#
  beta[flip]=-beta[flip]#
  tmp= (-1)^flip * (mean+sd*etruncnorm(alpha,beta,0,1))#
  max_alphabeta = ifelse(alpha<beta, beta,alpha)#
  max_ab = ifelse(alpha<beta,b,a)#
  toobig = max_alphabeta<(-30)#
  toobig[is.na(toobig)]=FALSE #
  tmp[toobig] = max_ab[toobig]#
  tmp#
}#
#return posterior mean for each component of prior m, given observations betahat and sebetahat#
#input, m is a mixture with k components#
#betahat, sebetahat are n vectors#
#output is a k by n matrix#
#note that with uniform prior, posterior is truncated normal, so#
#this is computed using formula for mean of truncated normal #
comp_postmean.unimix = function(m,betahat,sebetahat,v){#
#   k= ncomp(m)#
#   n=length(betahat)#
#   a = matrix(m$a,nrow=n,ncol=k,byrow=TRUE)#
#   b = matrix(m$b,nrow=n,ncol=k,byrow=TRUE)#
#   matrix(etruncnorm(a,b,betahat,sebetahat),nrow=k,byrow=TRUE)#
  #note: etruncnorm is more stable for a and b negative than positive#
  #so maybe use this, and standardize to make the whole more stable.#
  alpha = outer(-betahat, m$a,FUN="+")/sebetahat#
  beta = outer(-betahat, m$b, FUN="+")/sebetahat#
  if(is.null(v)){#
    tmp = betahat + sebetahat*my_etruncnorm(alpha,beta,0,1)#
  }#
  else{#
  	tmp = betahat + sebetahat*my_etrunct(alpha,beta,v)#
  }#
  ismissing = is.na(betahat) | is.na(sebetahat)#
  tmp[ismissing,]= (m$a+m$b)/2#
  t(tmp)#
#   t(#
#     betahat + sebetahat* #
#       exp(dnorm(alpha,log=TRUE)- pnorm(alpha,log=TRUE))#
#    * #
#       (-expm1(dnorm(beta,log=TRUE)-dnorm(alpha,log=TRUE)))#
#     /#
#       (expm1(pnorm(beta,log=TRUE)-pnorm(alpha,log=TRUE)))#
#   )#
}#
#
#not yet implemented!#
#just returns 0s for now#
comp_postsd.unimix = function(m,betahat,sebetahat,v){#
  print("Warning: Posterior SDs not yet implemented for uniform components")#
  k= ncomp(m)#
  n=length(betahat)#
  return(matrix(NA,nrow=k,ncol=n)) #
}#
#
# the mean of a truncated student.t#
# the result is from the paper 'Moments of truncated Student-t distribution' by H.-J Kim #
#
my_etrunct= function(a,b,v){#
  A = v+a^2#
  B = v+b^2#
  F_a = pt(a,df=v)#
  F_b = pt(b,df=v)#
  G = gamma((v-1)/2)*v^(v/2)/(2*(F_b-F_a)*gamma(v/2)*gamma(1/2))#
  tmp = ifelse(a==b,a,G*(A^(-(v-1)/2)-B^(-(v-1)/2)))#
  tmp#
}
ASH UTILITY FUNCTIONS #############################
#
#' @title Summary method for ash object#
#'#
#' @description Print summary of fitted ash object#
#'#
#' @details See readme for more details#
#' #
#' @export#
#' #
summary.ash=function(a){#
  print(a$fitted.g)#
  print(tail(a$fit$loglik,1),digits=10)#
  print(a$fit$converged)#
}#
#
#' @title Print method for ash object#
#'#
#' @description Print the fitted distribution of beta values in the EB hierarchical model#
#'#
#' @details None#
#' #
#' @export#
#' #
print.ash =function(a){#
  print(a$fitted.g)#
}#
#
#' @title Plot method for ash object#
#'#
#' @description Plot the density of the underlying fitted distribution#
#'#
#' @details None#
#' #
#' @export#
#' #
plot.ash = function(a,xmin,xmax,...){#
  x = seq(xmin,xmax,length=1000)#
  y = density(a,x)#
  plot(y,type="l",...)#
}#
#
#compute the predictive density of an observation#
#given the fitted ash object a and the vector se of standard errors#
#not implemented yet#
predictive=function(a,se){#
}#
#' @title Get fitted loglikelihood for ash object#
#'#
#' @description Return the log-likelihood of the data under the fitted distribution#
#'#
#' @param a the fitted ash object#
#'#
#' @details None#
#' #
#' @export#
#' #
#'#
get_loglik = function(a){#
  return(tail(a$fit$loglik,1))#
}#
#
#' @title Get pi0 estimate for ash object#
#'#
#' @description Return estimate of the null proportion, pi0#
#'#
#' @param a the fitted ash object#
#'#
#' @details Extracts the estimate of the null proportion, pi0, from the object a#
#' #
#' @export#
#' #
get_pi0 = function(a){#
  null.comp = comp_sd(a$fitted.g)==0#
  return(sum(a$fitted.g$pi[null.comp]))#
}#
#
#' @title Compute loglikelihood for data from ash fit#
#'#
#' @description Return the log-likelihood of the data betahat, with standard errors betahatsd, #
#' under the fitted distribution in the ash object. #
#' #
#'#
#' @param a the fitted ash object#
#' @param betahat the data#
#' @param betahatsd the observed standard errors#
#' @param zscores indicates whether ash object was originally fit to z scores #
#' @details None#
#' #
#' @export#
#' #
#'#
loglik.ash = function(a,betahat,betahatsd,zscores=FALSE){#
  g=a$fitted.g#
  FUN="+"#
  if(zscores==TRUE){#
    g$sd = sqrt(g$sd^2+1) #
    FUN="*"#
  }#
  return(loglik_conv(g,betahat, betahatsd,FUN))#
}#
#
#' @title Density method for ash object#
#'#
#' @description Return the density of the underlying fitted distribution#
#'#
#' @param a the fitted ash object#
#' @param x the vector of locations at which density is to be computed#
#'#
#' @details None#
#' #
#' @export#
#' #
#'#
density.ash=function(a,x){list(x=x,y=dens(a$fitted.g,x))}#
#
#' @title cdf method for ash object#
#'#
#' @description Computed the cdf of the underlying fitted distribution#
#'#
#' @param a the fitted ash object#
#' @param x the vector of locations at which cdf is to be computed#
#' @param lower.tail (default=TRUE) whether to compute the lower or upper tail#
#'#
#' @details None#
#' #
#' @export#
#' #
#'#
cdf.ash=function(a,x,lower.tail=TRUE){#
  return(list(x=x,y=mixcdf(a$fitted.g,x,lower.tail)))#
}#
#' @title Credible Interval Computation for the ash object#
#'#
#' @description Given the ash object return by the main function ash, this function computes the corresponding credible interval of the mixture model.#
#'#
#' @details Uses default optimization function and perform component-wise credible interval computation. The computation cost is linear of the length of betahat.#
#'#
#' @param a the fitted ash object #
#' @param levels, the level for the credible interval, (default=0.95)#
#' @param tol, the tolerance for convergence of log-likelihood.#
#' #
#' @return A matrix, with first column being the posterior mean, second and third column being the lower bound and upper bound for the credible interval. #
#'  #
#' @export#
#' #
#' #
ashci = function (a,level=0.95){#
  x=a$data$betahat#
  s=a$data$sebetahat#
  m=a$fitted.g#
  lower=min(x)-qnorm(level)*(max(m$sd)+max(s))#
  upper=max(x)+qnorm(level)*(max(m$sd)+max(s))#
  CImatrix=matrix(NA,nrow=length(x),ncol=3)	#
  colnames(CImatrix)=c("Posterior Mean",(1-level)/2,(1+level)/2)#
  CImatrix[,1]=a$PosteriorMean#
  if( class(a$fitted.g) == "normalmix" | class(a$fitted.g) == "unimix" ){#
    for(i in 1:length(x)){#
	  CImatrix[i,2]=optim(par=a$PosteriorMean[i],f=ci.lower,m=m,x=x[i],s=s[i],level=level,df=df,method="Brent",lower=lower,upper=upper)$par#
	  CImatrix[i,3]=optim(par=a$PosteriorMean[i],f=ci.upper,m=m,x=x[i],s=s[i],level=level,df=df,method="Brent",lower=lower,upper=upper)$par#
	}#
  }#
  else{stop(paste("Invalid class",class(m)))}#
  return(CImatrix)#
}#
#
ci.lower=function(z,m,x,s,level,df){#
	tailprob=cdf_post(m,z,x,s,df)#
	return(abs(tailprob-(1-level)/2))#
}#
#
ci.upper=function(z,m,x,s,level,df){#
	tailprob=1-cdf_post(m,z,x,s,df)#
	return(abs(tailprob-(1-level)/2))#
}
' @useDynLib ashr#
#todo#
##
#' @title Main Adaptive SHrinkage function#
#'#
#' @description Takes vectors of estimates (betahat) and their standard errors (sebetahat), and applies#
#' shrinkage to them, using Empirical Bayes methods, to compute shrunk estimates for beta.#
#'#
#' @details See readme for more details#
#' #
#' @param betahat, a p vector of estimates #
#' @param sebetahat, a p vector of corresponding standard errors#
#' @param method: specifies how ash is to be run. Can be "shrinkage" (if main aim is shrinkage) or "fdr" (if main aim is to assess fdr or fsr)#
#' This is simply a convenient way to specify certain combinations of parameters: "shrinkage" sets pointmass=FALSE and prior="uniform";#
#' "fdr" sets pointmass=TRUE and prior="nullbiased".#
#' @param mixcompdist: distribution of components in mixture ("normal", "uniform" or "halfuniform")#
#'#
#' @param lambda1: multiplicative "inflation factor" for standard errors (like Genomic Control)#
#' @param lambda2: additive "inflation factor" for standard errors (like Genomic Control)#
#' @param nullcheck: whether to check that any fitted model exceeds the "null" likelihood#
#' in which all weight is on the first component#
#' @param df: appropriate degrees of freedom for (t) distribution of betahat/sebetahat#
#' @param randomstart: bool, indicating whether to initialize EM randomly. If FALSE, then initializes to prior mean (for EM algorithm) or prior (for VBEM)#
#' @param nullweight: scalar, the weight put on the prior of null under "fdr" method#
#' @param nonzeromean: bool, indicating whether to use a nonzero mean unimodal mixture(defaults to "FALSE")#
#' @param pointmass: bool, indicating whether to use a point mass at zero as one of components for a mixture distribution#
#' @param onlylogLR: bool, indicating whether to use this function to get logLR. Skip posterior prob, posterior mean, lfdr...#
#' @param prior: string, or numeric vector indicating Dirichlet prior on mixture proportions (defaults to "uniform", or 1,1...,1; also can be "nullbiased" 1,1/k-1,...,1/k-1 to put more weight on first component)#
#' @param mixsd: vector of sds for underlying mixture components #
#' @param VB: whether to use Variational Bayes to estimate mixture proportions (instead of EM to find MAP estimate)#
#' @param gridmult: the multiplier by which the default grid values for mixsd differ by one another. (Smaller values produce finer grids)#
#' @param minimal_output: if TRUE, just outputs the fitted g and the lfsr (useful for very big data sets where memory is an issue) #
#' @param g: the prior distribution for beta (usually estimated from the data; this is used primarily in simulated data to do computations with the "true" g)#
#' @param maxiter: maximum number of iterations of the EM algorithm#
#' @param cxx: flag to indicate whether to use the c++ (Rcpp) version#
#' #
#'#
#' @return a list with elements fitted.g is fitted mixture#
#' logLR : logP(D|mle(pi)) - logP(D|null)#
#' #
#' @export#
#' #
#' @examples #
#' beta = c(rep(0,100),rnorm(100))#
#' sebetahat = abs(rnorm(200,0,1))#
#' betahat = rnorm(200,beta,sebetahat)#
#' beta.ash = ash(betahat, sebetahat)#
#' summary(beta.ash)#
#' plot(betahat,beta.ash$PosteriorMean,xlim=c(-4,4),ylim=c(-4,4))#
#' #
#' #
#Things to do:#
# check sampling routine#
# check number of iterations#
ash = function(betahat,sebetahat,method = c("shrink","fdr"), #
               mixcompdist = c("normal","uniform","halfuniform"),#
               lambda1=1,lambda2=0,nullcheck=TRUE,df=NULL,randomstart=FALSE,#
               nullweight=10,nonzeromean=FALSE, #
               pointmass = FALSE, #
               onlylogLR = FALSE, #
               prior=c("uniform","nullbiased"), #
               mixsd=NULL, VB=FALSE,gridmult=sqrt(2),#
               minimaloutput=FALSE,#
               multiseqoutput=FALSE,#
               g=NULL,#
               maxiter = 5000,#
               cxx=FALSE){#
  #method provides a convenient interface to set a particular combinations of parameters for prior an#
  #If method is supplied, use it to set up specific values for these parameters; provide warning if values#
  #are also specified by user#
  #If method is not supplied use the user-supplied values (or defaults if user does not specify them)#
  if(!missing(method)){#
    method = match.arg(method) #
    if(method=="shrink"){#
      if(missing(prior)){#
        prior = "uniform"#
      } else {#
        warning("Specification of prior overrides default for method shrink")#
      }#
      if(missing(pointmass)){#
        pointmass=FALSE#
      } else {#
        warning("Specification of pointmass overrides default for method shrink")#
      }#
    }#
    if(method=="fdr"){#
      if(missing(prior)){#
        prior = "nullbiased"#
      } else {#
        warning("Specification of prior overrides default for method fdr")#
      }#
      if(missing(pointmass)){#
        pointmass=TRUE#
      } else {#
        warning("Specification of pointmass overrides default for method fdr")#
      }#
    }  #
  }#
  if(gridmult<=1&multiseqoutput!=TRUE)#
    stop("gridmult must be > 1")#
  mixcompdist = match.arg(mixcompdist)#
  # if(mixcompdist=="uniform" & pointmass==TRUE){#
  #    stop("point mass not yet implemented for uniform or half-uniform")#
  #  }#
  #  if(mixcompdist=="halfuniform" & pointmass==TRUE){#
  #    stop("point mass not yet implemented for uniform or half-uniform")#
  #  }#
  if(!is.numeric(prior)){#
    prior = match.arg(prior)#
  }  #
  if(length(sebetahat)==1){#
    sebetahat = rep(sebetahat,length(betahat))#
  }#
  if(length(sebetahat) != length(betahat)){#
    stop("Error: sebetahat must have length 1, or same length as betahat")#
  }#
  completeobs = (!is.na(betahat) & !is.na(sebetahat))#
  n=sum(completeobs)#
  if(n==0){#
    if(onlylogLR){#
      return(list(pi=NULL, logLR = 0))#
    }#
    else{#
      stop("Error: all input values are missing")#
    }#
  }  #
  if(!is.null(g)){#
    maxiter = 1 # if g is specified, don't iterate the EM#
    prior = rep(1,ncomp(g)) #prior is not actually used if g specified, but required to make sure EM doesn't produce warning#
    null.comp=1 #null.comp also not used, but required #
  } else {#
    if(is.null(mixsd)){#
      mixsd = autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)#
    }#
    if(pointmass){#
      mixsd = c(0,mixsd)#
    }#
    if(nonzeromean & is.null(df)){#
		nonzeromean.fit=nonzeromeanEM(betahat[completeobs], sebetahat[completeobs], mixsd, maxiter=maxiter)#
		betahat[completeobs]= betahat[completeobs] - nonzeromean.fit$nonzeromean#
	}#
	else if(nonzeromean & !is.null(df)){#
		stop("Error: Nonzero mean only implemented for df=NULL")#
	}#
    null.comp = which.min(mixsd) #which component is the "null"#
    k = length(mixsd)#
    if(!is.numeric(prior)){#
      if(prior=="nullbiased"){ # set up prior to favour "null"#
        prior = rep(1,k)#
        prior[null.comp] = nullweight #prior 10-1 in favour of null by default#
      }else if(prior=="uniform"){#
        prior = rep(1,k)#
      }#
    }#
    if(length(prior)!=k | !is.numeric(prior)){#
      stop("invalid prior specification")#
    }#
    if(randomstart){#
      pi = rgamma(k,1,1)#
    } else {#
      if(k<n){#
        pi=rep(1,k)/n #default initialization strongly favours null; puts weight 1/n on everything except null#
        pi[null.comp] = (n-k+1)/n #the motivation is data can quickly drive away from null, but tend to drive only slowly toward null.#
      } else {#
        pi=rep(1,k)/k#
      }#
    }#
    pi=normalize(pi)#
    if(!is.element(mixcompdist,c("normal","uniform","halfuniform"))) stop("Error: invalid type of mixcompdist")#
    if(mixcompdist=="normal") g=normalmix(pi,rep(0,k),mixsd)#
    if(mixcompdist=="uniform") g=unimix(pi,-mixsd,mixsd)#
    if(mixcompdist=="halfuniform"){#
      g = unimix(c(pi,pi)/2,c(-mixsd,rep(0,k)),c(rep(0,k),mixsd))#
      prior = rep(prior, 2)#
      pi = rep(pi, 2)#
    }#
  }#
  pi.fit=EMest(betahat[completeobs],lambda1*sebetahat[completeobs]+lambda2,g,prior,null.comp=null.comp,nullcheck=nullcheck,VB=VB,maxiter = maxiter, cxx=cxx, df=df)  #
  if (!onlylogLR){#
      n=length(betahat)#
      if (!multiseqoutput){#
          ZeroProb = rep(0,length=n)#
          NegativeProb = rep(0,length=n)#
      }#
      if (!minimaloutput){#
          PosteriorMean = rep(0,length=n)#
          PosteriorSD = rep(0,length=n)#
      }#
      if (!multiseqoutput){#
          ZeroProb[completeobs] = colSums(comppostprob(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)[comp_sd(pi.fit$g)==0,,drop=FALSE])#
          NegativeProb[completeobs] = cdf_post(pi.fit$g, 0, betahat[completeobs],sebetahat[completeobs],df) - ZeroProb[completeobs]#
      }#
      if (!minimaloutput){#
          PosteriorMean[completeobs] = postmean(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
          PosteriorSD[completeobs] = postsd(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
      }#
                                        #FOR MISSING OBSERVATIONS, USE THE PRIOR INSTEAD OF THE POSTERIOR#
      if (!multiseqoutput){#
          ZeroProb[!completeobs] = sum(mixprop(pi.fit$g)[comp_sd(pi.fit$g)==0])#
          NegativeProb[!completeobs] = mixcdf(pi.fit$g,0)#
          lfsr = compute_lfsr(NegativeProb,ZeroProb)#
      }#
      if (!minimaloutput){#
          PosteriorMean[!completeobs] = mixmean(pi.fit$g)#
          PosteriorSD[!completeobs] = mixsd(pi.fit$g)#
      }#
      if (!minimaloutput & !multiseqoutput){#
          PositiveProb = 1- NegativeProb-ZeroProb#
          lfsra = compute_lfsra(PositiveProb,NegativeProb,ZeroProb) #
          lfdr = ZeroProb#
          qvalue = qval.from.lfdr(lfdr)#
      }#
  }#
  if (!minimaloutput)#
      logLR = tail(pi.fit$loglik,1) - pi.fit$null.loglik#
  if (onlylogLR)#
      return(list(fitted.g=pi.fit$g, logLR = logLR, df=df))#
  else if (minimaloutput)#
      return(list(fitted.g = pi.fit$g, lfsr = lfsr, fit = pi.fit,df=df))#
  else if (multiseqoutput)#
      return(list(fitted.g = pi.fit$g, logLR = logLR, PosteriorMean = PosteriorMean, PosteriorSD = PosteriorSD, call= match.call(),df=df))#
  else{#
      result = list(fitted.g = pi.fit$g, logLR = logLR, PosteriorMean = PosteriorMean, PosteriorSD = PosteriorSD, PositiveProb = PositiveProb, NegativeProb = NegativeProb, ZeroProb = ZeroProb, lfsr = lfsr,lfsra = lfsra, lfdr = lfdr, qvalue = qvalue, fit = pi.fit, lambda1 = lambda1, lambda2 = lambda2, call = match.call(), data = list(betahat = betahat, sebetahat=sebetahat),df=df)#
      class(result) = "ash"#
      return(result)#
  }#
}#
  #if(nsamp>0){#
  #  sample = posterior_sample(post,nsamp)#
  #}#
#
# #' @title Faster version of function ash#
# #'#
# #' @description This function has similar functionality as ash, but only returns some of the outputs.#
# #'#
# #' @param betahat, a p vector of estimates#
# #' @param sebetahat, a p vector of corresponding standard errors#
# #' @param nullcheck: whether to check that any fitted model exceeds the "null" likelihood in which all weight is on the first component#
# #' @param randomstart: bool, indicating whether to initialize EM randomly. If FALSE, then initializes to prior mean (for EM algorithm) or prior (for VBEM)#
# #' @param pointmass: bool, indicating whether to use a point mass at zero as one of components for a mixture distribution#
# #' @param onlylogLR: bool, indicating whether to use this function to get logLR. Skip posterior prob, posterior mean, lfdr...#
# #' @param prior: string, or numeric vector indicating Dirichlet prior on mixture proportions (defaults to "uniform", or 1,1...,1; also can be "nullbiased" 1,1/k-1,...,1/k-1 to put more weight on first component)#
# #' @param mixsd: vector of sds for underlying mixture components#
# #' @param VB: whether to use Variational Bayes to estimate mixture proportions (instead of EM to find MAP estimate)#
# #' @param gridmult: the multiplier by which the default grid values for mixsd differ by one another. (Smaller values produce finer grids)#
# #' @param g: the prior distribution for beta (usually estimated from the data; this is used primarily in simulated data to do computations with the "true" g)#
# #' @param cxx: flag to indicate whether to use the c++ (Rcpp) version#
# #'#
# #' @return a list with elements fitted.g is fitted mixture#
# #' logLR : logP(D|mle(pi)) - logP(D|null)#
# #'#
# #' @export#
# fast.ash = function(betahat,sebetahat, #
#                     nullcheck=TRUE,randomstart=FALSE, #
#                     pointmass = TRUE,    #
#                     prior=c("nullbiased","uniform"), #
#                     mixsd=NULL, VB=FALSE,gridmult=4,#
#                     g=NULL, cxx=TRUE,#
#                     onlylogLR = FALSE,df=NULL){#
#   #
#   if(onlylogLR){#
#     pointmass <- TRUE  #
#   }#
#   #
#   #If method is supplied, use it to set up defaults; provide warning if these default values#
#   #are also specified by user#
#   if(!is.numeric(prior)){#
#     prior = match.arg(prior)#
#   }#
#   #
#   if(length(sebetahat)==1){#
#     sebetahat = rep(sebetahat,length(betahat))#
#   }#
#   if(length(sebetahat) != length(betahat)){#
#     stop("Error: sebetahat must have length 1, or same length as betahat")#
#   }#
#   #
#   completeobs = (!is.na(betahat) & !is.na(sebetahat))#
#   if(sum(completeobs)==0){#
#     if(onlylogLR){#
#       return(list(pi=NULL, logLR = 0))#
#     }else{#
#       stop("Error: all input values are missing")#
#     }#
#   }  #
#   #
#   if(is.null(mixsd)){#
#     mixsd= autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)#
#   }#
#   if(pointmass){#
#     mixsd = c(0,mixsd)#
#   }#
#   #
#   k=length(mixsd)  #
#   null.comp = which.min(mixsd) #which component is the "null"#
#   #
#   if(!is.numeric(prior)){#
#     if(prior=="nullbiased"){ # set up prior to favour "null"#
#       prior = rep(1,k)#
#       prior[null.comp] = 10 #prior 10-1 in favour of null#
#     }else if(prior=="uniform"){#
#       prior = rep(1,k)#
#     }#
#   }#
#   #
#   if(length(prior)!=k | !is.numeric(prior)){#
#     stop("invalid prior specification")#
#   }#
#   #
#   if(missing(g)){#
#     pi = prior^2 #default is to initialize pi at prior (mean)#
#     if(randomstart){pi=rgamma(k,1,1)}#
#     pi=normalize(pi)#
#     g=normalmix(pi,rep(0,k),mixsd)#
#     maxiter = 5000#
#   } else {#
#     maxiter = 1; # if g is specified, don't iterate the EM #
#   }#
#   #
#   pi.fit=EMest(betahat[completeobs],sebetahat[completeobs],g,prior,null.comp=null.comp,nullcheck=nullcheck,VB=VB,maxiter = maxiter, cxx=cxx, df=df)  #
#   #
#   if(onlylogLR){#
#     logLR = tail(pi.fit$loglik,1) - pi.fit$null.loglik#
#     return(list(pi=pi.fit$pi, logLR = logLR))#
#   }else{#
#     #
#     n=length(betahat)#
#     PosteriorMean = rep(0,length=n)#
#     PosteriorSD=rep(0,length=n)#
#     #
#     if(is.null(df)){#
#       PosteriorMean[completeobs] = postmean(pi.fit$g,betahat[completeobs],sebetahat[completeobs])#
#       PosteriorSD[completeobs] =postsd(pi.fit$g,betahat[completeobs],sebetahat[completeobs]) #
#     }#
#     else{#
#       PosteriorMean[completeobs] = postmean_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
#       PosteriorSD[completeobs] =postsd_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
#     }#
#     #FOR MISSING OBSERVATIONS, USE THE PRIOR INSTEAD OF THE POSTERIOR#
#     PosteriorMean[!completeobs] = mixmean(pi.fit$g)#
#     PosteriorSD[!completeobs] =mixsd(pi.fit$g)  #
#     #
#     result = list(fitted.g=pi.fit$g,PosteriorMean = PosteriorMean,PosteriorSD=PosteriorSD,call=match.call(),data=list(betahat = betahat, sebetahat=sebetahat))#
#     return(result)#
#   }#
#   #if(nsamp>0){#
#   #  sample = posterior_sample(post,nsamp)#
#   #}#
# }#
compute_lfsr = function(NegativeProb,ZeroProb){#
  ifelse(NegativeProb> 0.5*(1-ZeroProb),1-NegativeProb,NegativeProb+ZeroProb)#
}#
#
compute_lfsra = function(PositiveProb, NegativeProb,ZeroProb){#
  ifelse(PositiveProb<NegativeProb,2*PositiveProb+ZeroProb,2*NegativeProb+ZeroProb)  #
}  #
#
#' @title Estimate unimodal nonzero mean of a mixture model by EM algorithm#
#'#
#' @description Given the data, standard error of the data and standard deviations of the Gaussian mixture model, estimate the mean of a unimodal Gaussian mixture by an EM algorithm.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates unimodal mean \eqn{\mu} by EM algorithm. Uses the SQUAREM package to accelerate convergence of EM. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' #
#' @param betahat, a p vector of estimates #
#' @param sebetahat, a p vector of corresponding standard errors#
#' @param mixsd: vector of sds for underlying mixture components #
#' @param pi.init, the initial value of \eqn{\pi} to use. If not specified defaults to (1/k,...,1/k).#
#' @param tol, the tolerance for convergence of log-likelihood.#
#' @param maxiter the maximum number of iterations performed#
#' #
#' @return A list, including the estimates (\eqn{\mu}) and (\eqn{\pi}), the log likelihood for each iteration (NQ)#
#' and a flag to indicate convergence#
#'  #
#' @export#
#' #
#' #
nonzeromeanEM = function(betahat, sebetahat, mixsd, pi.init=NULL,tol=1e-7,maxiter=5000){#
  if(is.null(pi.init)){#
    pi.init = rep(1/length(mixsd),length(mixsd))# Use as starting point for pi#
  }#
  mupi=c(mean(betahat),pi.init)#
  res=squarem(par=mupi,fixptfn=nonzeromeanEMfixpoint,objfn=nonzeromeanEMobj,betahat=betahat,sebetahat=sebetahat,mixsd=mixsd,control=list(maxiter=maxiter,tol=tol))#
  return(list(nonzeromean=res$par[1],pi=res$par[-1],NQ=-res$value.objfn,niter = res$iter, converged=res$convergence,post=res$par))#
}#
#
nonzeromeanEMfixpoint = function(mupi,betahat,sebetahat,mixsd){#
	#omegamatrix=matrix(NA,nrow=length(betahat),ncol=length(mixsd))#
	mu=mupi[1]#
	pimean=mupi[-1]#
	sdmat = sqrt(outer(sebetahat ^2,mixsd^2,"+")) #
	xmat=matrix(rep(betahat,length(mixsd)),ncol=length(mixsd))#
	omegamatrix=t(t(dnorm(xmat,mean=mu,sd=sdmat))*pimean)#
	omegamatrix=omegamatrix /rowSums(omegamatrix)#
	pinew=normalize(colSums(omegamatrix))#
	munew=sum(omegamatrix*xmat/(sdmat^2))/sum(omegamatrix/(sdmat^2))#
	mupi=c(munew,pinew)#
	return(mupi)#
}#
#
nonzeromeanEMobj = function(mupi,betahat,sebetahat,mixsd){#
	mu=mupi[1]#
	pimean=mupi[-1]#
	sdmat = sqrt(outer(sebetahat ^2,mixsd^2,"+")) #
	xmat=matrix(rep(betahat,length(mixsd)),ncol=length(mixsd))#
	omegamatrix=t(t(dnorm(xmat,mean=mu,sd=sdmat))*pimean)#
	omegamatrix=omegamatrix /rowSums(omegamatrix)#
	NegativeQ=-sum(omegamatrix*dnorm(xmat,mean=mu,sd=sdmat,log=TRUE))#
	return(NegativeQ)#
}#
#' @title Estimate posterior distribution on mixture proportions of a mixture model by a Variational Bayes EM algorithm#
#'#
#' @description Given the individual component likelihoods for a mixture model, estimates the posterior on #
#' the mixture proportions by an VBEM algorithm. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates posterior on mixture proportions \eqn{\pi} by Variational Bayes, #
#' with a Dirichlet prior on \eqn{\pi}. #
#' Algorithm adapted from Bishop (2009), Pattern Recognition and Machine Learning, Chapter 10.#
#' #
#' @param matrix_lik: a n by k matrix with (j,k)th element equal to \eqn{f_k(x_j)}.#
#' @param prior: a k vector of the parameters of the Dirichlet prior on \eqn{\pi}. Recommended to be rep(1,k)#
#' @param post.init: the initial value of the posterior parameters. If not specified defaults to the prior parameters.#
#' @param tol: the tolerance for convergence of log-likelihood bound.#
#' @param maxiter: the maximum number of iterations performed#
#' #
#' @return A list, whose components include point estimates (pihat), #
#' the parameters of the fitted posterior on \eqn{\pi} (pipost),#
#' the bound on the log likelihood for each iteration (B)#
#' and a flag to indicate convergence (converged).#
#'  #
#' @export#
#' #
mixVBEM = function(matrix_lik, prior, pi.init = NULL,tol=1e-7, maxiter=5000){#
  k=ncol(matrix_lik)#
  if(is.null(pi.init)){#
    pi.init = rep(1,k)# Use as starting point for pi#
  } #
  res = squarem(par=pi.init,fixptfn=VBfixpoint, objfn=VBnegpenloglik,matrix_lik=matrix_lik, prior=prior, control=list(maxiter=maxiter,tol=tol))#
  return(list(pihat = res$par/sum(res$par), B=res$value.objfn, niter = res$iter, converged=res$convergence,post=res$par))#
}#
VBfixpoint = function(pipost, matrix_lik, prior){  #
  n=nrow(matrix_lik)#
  k=ncol(matrix_lik)#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost*matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  pipostnew = colSums(classprob) + prior#
  return(pipostnew)#
}#
#
VBnegpenloglik=function(pipost,matrix_lik,prior){#
  return(-VBpenloglik(pipost,matrix_lik,prior))#
}#
#
VBpenloglik = function(pipost, matrix_lik, prior){#
  n=nrow(matrix_lik)#
  k=ncol(matrix_lik)#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost*matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  B= sum(classprob*log(avgpipost*matrix_lik),na.rm=TRUE) - diriKL(prior,pipost) - sum(classprob*log(classprob)) #
  return(B)#
}#
#' @title Estimate mixture proportions of a mixture model by EM algorithm#
#'#
#' @description Given the individual component likelihoods for a mixture model, estimates the mixture proportions by an EM algorithm.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates mixture proportions \eqn{\pi} by maximum likelihood, or by maximum a posteriori (MAP) estimation for a Dirichlet prior on $\pi$ #
#' (if a prior is specified).  Uses the SQUAREM package to accelerate convergence of EM. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' #
#' @param matrix_lik, a n by k matrix with (j,k)th element equal to \eqn{f_k(x_j)}.#
#' @param prior, a k vector of the parameters of the Dirichlet prior on \eqn{\pi}. Recommended to be rep(1,k)#
#' @param pi.init, the initial value of \eqn{\pi} to use. If not specified defaults to (1/k,...,1/k).#
#' @param tol, the tolerance for convergence of log-likelihood.#
#' @param maxiter the maximum number of iterations performed#
#' #
#' @return A list, including the estimates (pihat), the log likelihood for each interation (B)#
#' and a flag to indicate convergence#
#'  #
#' @export#
#' #
#' #
mixEM = function(matrix_lik, prior, pi.init = NULL,tol=1e-7, maxiter=5000){#
  if(is.null(pi.init)){#
    pi.init = rep(1/k,k)# Use as starting point for pi#
  } #
  res = squarem(par=pi.init,fixptfn=fixpoint, objfn=negpenloglik,matrix_lik=matrix_lik, prior=prior, control=list(maxiter=maxiter,tol=tol))#
  return(list(pihat = normalize(pmax(0,res$par)), B=res$value.objfn, #
              niter = res$iter, converged=res$convergence))#
}#
#
# helper functions used by mixEM#
normalize = function(x){return(x/sum(x))}#
#
fixpoint = function(pi, matrix_lik, prior){  #
  pi = normalize(pmax(0,pi)) #avoid occasional problems with negative pis due to rounding#
  m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
  m.rowsum = rowSums(m)#
  classprob = m/m.rowsum #an n by k matrix#
  pinew = normalize(colSums(classprob) + prior - 1)#
  return(pinew)#
}#
#
negpenloglik = function(pi,matrix_lik,prior){return(-penloglik(pi,matrix_lik,prior))}#
#
penloglik = function(pi, matrix_lik, prior){#
  pi = normalize(pmax(0,pi))#
  m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
  m.rowsum = rowSums(m)#
  loglik = sum(log(m.rowsum))#
  subset = (prior != 1.0)#
  priordens = sum((prior-1)[subset]*log(pi[subset]))#
  return(loglik+priordens)#
}#
#
#The kth element of this vector is the derivative #
#of the loglik for $\pi=(\pi_0,...,1-\pi_0,...)$ with respect to $\pi_0$ at $\pi_0=1$.#
gradient = function(matrix_lik){#
  n = nrow(matrix_lik)#
  grad = n - colSums(matrix_lik/matrix_lik[,1]) #
  return(grad)#
}#
#
# mixEM = function(matrix_lik, prior, pi.init = NULL,tol=0.0001, maxiter=5000){#
#   n=nrow(matrix_lik)#
#   k=ncol(matrix_lik)#
#   B = rep(0,maxiter)#
#   pi = pi.init#
#   if(is.null(pi.init)){#
#     pi = rep(1/k,k)# Use as starting point for pi#
#   } #
#   pi = ifelse(pi<1e-5,1e-5,pi) #set any estimates that are too small to be just very small#
#   pi = normalize(pi)#
#   #
#   loglik = rep(0,maxiter)#
#   priordens= rep(0,maxiter)#
#   m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
#   m.rowsum = rowSums(m)#
#   loglik[1] = sum(log(m.rowsum))#
#   priordens[1] = sum((prior-1)*log(pi)) #
#   classprob = m/m.rowsum #an n by k matrix#
#   i=1#
#   if(maxiter >= 2){#
#     for(i in 2:maxiter){  #
#       pi = colSums(classprob) + prior-1#
#       pi = ifelse(pi<1e-5,1e-5,pi) #set any estimates that are less than zero, which can happen with prior<1, to 0#
#       pi = normalize(pi)#
#         #
#       #Now re-estimate pi#
#       m  = t(pi * t(matrix_lik)) #
#       m.rowsum = rowSums(m)#
#       loglik[i] = sum(log(m.rowsum))#
#       priordens[i] = sum((prior-1)*log(pi)) #
#       classprob = m/m.rowsum#
#     #
#     #
#       if(abs(loglik[i]+priordens[i]-loglik[i-1]-priordens[i-1])<tol) break;#
#     }#
#   }#
#   converged=(abs(loglik[i]+priordens[i]-loglik[i-1]-priordens[i-1])<tol)#
#   if(!converged){#
#       warning("EM algorithm in function mixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
#   }#
#   return(list(pihat = pi, B=loglik[1:i], #
#               niter = i, converged=converged))#
# }#
#estimate mixture proportions of sigmaa by EM algorithm#
#prior gives the parameter of a Dirichlet prior on pi#
#(prior is used to encourage results towards smallest value of sigma when#
#likelihood is flat)#
#nullcheck indicates whether to check whether the loglike exceeds the null#
#(may not want to use if prior is used)#
#VB provides an approach to estimate the approximate posterior distribution#
#of mixture proportions of sigmaa by variational Bayes method#
#(use Dirichlet prior and approximate Dirichlet posterior)#
#if cxx TRUE use cpp version of R function mixEM#
EMest = function(betahat,sebetahat,g,prior,null.comp=1,nullcheck=TRUE,VB=FALSE, maxiter=5000, cxx=TRUE, df=NULL){ #
  pi.init = g$pi#
  k=ncomp(g)#
  n = length(betahat)#
  tol = min(0.1/n,1e-5) # set convergence criteria to be more stringent for larger samples#
  if(is.null(df)){#
    matrix_lik = t(compdens_conv(g,betahat,sebetahat))#
  }#
  else{#
    matrix_lik = t(compdens_conv_t(g,betahat,sebetahat,df))#
  }#
  #checks whether the gradient at pi0=1 is positive (suggesting that this is a fixed point)#
  #if(nullcheck){#
  #  if(all(gradient(matrix_lik)>=0)){#
  #    pi.init=rep(0,k)#
  #    pi.init[null.comp]=1 #this will make pi.init=(1,0,0...,0) which is a fixed point of the EM#
  #  }#
  #}#
  if(VB==TRUE){#
    EMfit=mixVBEM(matrix_lik,prior,maxiter=maxiter)}#
  else{#
    if (cxx==TRUE){#
      EMfit = cxxMixEM(matrix_lik,prior,pi.init,1e-5, maxiter) #currently use different convergence criteria for cxx version #
      if(!EMfit$converged){#
        warning("EM algorithm in function cxxMixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
      }#
    }#
    else{#
      EMfit = mixEM(matrix_lik,prior,pi.init,tol, maxiter)#
      if(!EMfit$converged & !(maxiter==1)){#
        warning("EM algorithm in function mixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
      }#
    }#
  }#
  pi = EMfit$pihat     #
  penloglik = EMfit$B #
  converged = EMfit$converged#
  niter = EMfit$niter#
  loglik.final =  penloglik(pi,matrix_lik,1) #compute penloglik without penalty#
  null.loglik = sum(log(matrix_lik[,null.comp]))  #
  if(nullcheck==TRUE & VB==FALSE){ #null check doesn't work with VB yet#
    pinull = rep(0,k)#
    pinull[null.comp]=1#
    null.penloglik = penloglik(pinull,matrix_lik,prior)#
    final.penloglik = penloglik(pi,matrix_lik,prior)#
    if(null.penloglik > final.penloglik){ #check whether exceeded "null" likelihood where everything is null#
      pi=pinull#
      loglik.final=penloglik(pi,matrix_lik,1)#
    }#
  }#
  g$pi=pi#
  return(list(loglik=loglik.final,null.loglik=null.loglik,#
              matrix_lik=matrix_lik,converged=converged,g=g))#
}#
#' @title Compute Posterior#
#'#
#' @description Return the posterior on beta given a prior (g) that is a mixture of normals (class normalmix) #
#' and observation betahat \sim N(beta,sebetahat)#
#'#
#' @details This can be used to obt#
#'#
#' @param g: a normalmix with components indicating the prior; works only if g has means 0#
#' @param betahat (n vector of observations) #
#' @param sebetahat (n vector of standard errors/deviations of observations)#
#' #
#' @return A list, (pi1,mu1,sigma1) whose components are each k by n matrices#
#' where k is number of mixture components in g, n is number of observations in betahat#
#' #
#' @export#
#' #
#' #
posterior_dist = function(g,betahat,sebetahat){#
  if(class(g)!="normalmix"){#
    stop("Error: posterior_dist implemented only for g of class normalmix")#
  }#
  pi0 = g$pi#
  mu0 = g$mean#
  sigma0 = g$sd  #
  k= length(pi0)#
  n= length(betahat)#
  if(!all.equal(g$mean,rep(0,k))) stop("Error: posterior_dist currently only implemented for zero-centered priors")#
  pi1 = pi0 * t(matrix_dens(betahat,sebetahat,sigma0))#
  pi1 = apply(pi1, 2, normalize) #pi1 is now an k by n matrix#
  #make k by n matrix versions of sigma0^2 and sebetahat^2#
  # and mu0 and betahat#
  s0m2 = matrix(sigma0^2,nrow=k,ncol=n,byrow=FALSE)#
  sebm2 = matrix(sebetahat^2,nrow=k,ncol=n, byrow=TRUE)#
  mu0m = matrix(mu0,nrow=k,ncol=n,byrow=FALSE)#
  bhatm = matrix(betahat,nrow=k,ncol=n,byrow=TRUE)#
  sigma1 = (s0m2*sebm2/(s0m2 + sebm2))^(0.5)  #
  w = sebm2/(s0m2 + sebm2)#
  mu1 = w*mu0m + (1-w)*bhatm#
  #WHERE DATA ARE MISSING, SET POSTERIOR = PRIOR#
  ismiss = (is.na(betahat) | is.na(sebetahat)) #
  pi1[,ismiss] = pi0#
  mu1[,ismiss] = mu0#
  sigma1[,ismiss] = sigma0#
  return(list(pi=pi1,mu=mu1,sigma=sigma1))#
}#
#
#return matrix of densities of observations (betahat) #
# assuming betahat_j \sim N(0, sebetahat_j^2 + sigmaavec_k^2)#
#normalized by maximum of each column#
#INPUT#
#betahat is n vector, #
#sebetahat is n vector, #
#sigmaavec is k vector#
#return is n by k matrix of the normal likelihoods, #
# with (j,k)th element the density of N(betahat_j; mean=0, var = sebetahat_j^2 + sigmaavec_k^2)#
#normalized to have maximum 1 in each column#
matrix_dens = function(betahat, sebetahat, sigmaavec){#
  k = length(sigmaavec)#
  n = length(betahat)#
  ldens = dnorm(betahat,0,sqrt(outer(sebetahat^2,sigmaavec^2,FUN="+")),log=TRUE)#
  maxldens = apply(ldens, 1, max)#
  ldens = ldens - maxldens#
  return(exp(ldens))#
}#
#
#return the "effective" estimate#
#that is the effect size betanew whose z score betanew/se#
#would give the same p value as betahat/se compared to a t with df#
effective.effect=function(betahat,se,df){#
  p = pt(betahat/se,df)#
  qnorm(p,sd=se)#
}#
#' @title Function to compute q values from local false discovery rates#
#'#
#' @description Computes q values from a vector of local fdr estimates#
#'#
#' @details The q value for a given lfdr is an estimate of the (tail) False Discovery Rate #
#' for all findings with a smaller lfdr, and is found by the average of the lfdr for#
#' all more significant findings. See Storey (2003), Annals of Statistics, for definition of q value.  #
#' #
#' #
#' @param lfdr, a vector of local fdr estimates#
#'#
#' @return vector of q values#
#' #
#' @export#
qval.from.lfdr = function(lfdr){#
  o = order(lfdr)#
  qvalue=rep(NA,length(lfdr))#
  qvalue[o] = (cumsum(sort(lfdr))/(1:sum(!is.na(lfdr))))#
  return(qvalue)#
}#
#
# try to select a default range for the sigmaa values#
# that should be used, based on the values of betahat and sebetahat#
# mult is the multiplier by which the sds differ across the grid#
autoselect.mixsd = function(betahat,sebetahat,mult){#
  sigmaamin = min(sebetahat)/10 #so that the minimum is small compared with measurement precision#
  if(all(betahat^2<sebetahat^2)){#
    sigmaamax = 8*sigmaamin #to deal with the occassional odd case where this could happen; 8 is arbitrary#
  } else {#
    sigmaamax = 2*sqrt(max(betahat^2-sebetahat^2)) #this computes a rough largest value you'd want to use, based on idea that sigmaamax^2 + sebetahat^2 should be at least betahat^2   #
  }#
  if(mult==0){#
    return(c(0,sigmaamax/2))#
  }else{#
    npoint = ceiling(log2(sigmaamax/sigmaamin)/log2(mult))#
    return(mult^((-npoint):0) * sigmaamax)#
  }#
}#
#return the KL-divergence between 2 dirichlet distributions#
#p,q are the vectors of dirichlet parameters of same lengths#
diriKL = function(p,q){#
  p.sum = sum(p)#
  q.sum = sum(q)#
  k = length(q)#
  KL = lgamma(q.sum)-lgamma(p.sum)+sum((q-p)*(digamma(q)-digamma(rep(q.sum,k))))+sum(lgamma(p)-lgamma(q))#
  return(KL)#
}#
#
#helper function for VBEM#
VB.update = function(matrix_lik, pipost){#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost * matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  B = sum(classprob*log(avgpipost*matrix_lik),na.rm=TRUE) - diriKL(prior,pipost) #negative free energy#
  return(list(classprob=classprob,B=B))#
}
test
test.ash
simdata = function(n, betahatsd,nullproportion=1, altmean=1, altsd=1) {#
    nnull=round(n*nullproportion,0)#
    null = c(rep(1, nnull), rep(0, n - nnull))#
    beta = c(rep(0, nnull), rnorm(n - nnull, altmean, altsd))#
    betahat = rnorm(n, beta, betahatsd)#
    return(list(null = null, beta = beta, betahat = betahat, betahatsd = betahatsd))#
  }#
  set.seed(200)#
test=simdata(20,betahatsd=1,nullproportion=0.5,altmean=2,altsd=1)#
print(test$beta)#
test.ash=ash(test$betahat,test$betahatsd,method="shrink",nonzeromean=FALSE)
' @useDynLib ashr#
#todo#
##
#' @title Main Adaptive SHrinkage function#
#'#
#' @description Takes vectors of estimates (betahat) and their standard errors (sebetahat), and applies#
#' shrinkage to them, using Empirical Bayes methods, to compute shrunk estimates for beta.#
#'#
#' @details See readme for more details#
#' #
#' @param betahat, a p vector of estimates #
#' @param sebetahat, a p vector of corresponding standard errors#
#' @param method: specifies how ash is to be run. Can be "shrinkage" (if main aim is shrinkage) or "fdr" (if main aim is to assess fdr or fsr)#
#' This is simply a convenient way to specify certain combinations of parameters: "shrinkage" sets pointmass=FALSE and prior="uniform";#
#' "fdr" sets pointmass=TRUE and prior="nullbiased".#
#' @param mixcompdist: distribution of components in mixture ("normal", "uniform" or "halfuniform")#
#'#
#' @param lambda1: multiplicative "inflation factor" for standard errors (like Genomic Control)#
#' @param lambda2: additive "inflation factor" for standard errors (like Genomic Control)#
#' @param nullcheck: whether to check that any fitted model exceeds the "null" likelihood#
#' in which all weight is on the first component#
#' @param df: appropriate degrees of freedom for (t) distribution of betahat/sebetahat#
#' @param randomstart: bool, indicating whether to initialize EM randomly. If FALSE, then initializes to prior mean (for EM algorithm) or prior (for VBEM)#
#' @param nullweight: scalar, the weight put on the prior of null under "fdr" method#
#' @param nonzeromean: bool, indicating whether to use a nonzero mean unimodal mixture(defaults to "FALSE")#
#' @param pointmass: bool, indicating whether to use a point mass at zero as one of components for a mixture distribution#
#' @param onlylogLR: bool, indicating whether to use this function to get logLR. Skip posterior prob, posterior mean, lfdr...#
#' @param prior: string, or numeric vector indicating Dirichlet prior on mixture proportions (defaults to "uniform", or 1,1...,1; also can be "nullbiased" 1,1/k-1,...,1/k-1 to put more weight on first component)#
#' @param mixsd: vector of sds for underlying mixture components #
#' @param VB: whether to use Variational Bayes to estimate mixture proportions (instead of EM to find MAP estimate)#
#' @param gridmult: the multiplier by which the default grid values for mixsd differ by one another. (Smaller values produce finer grids)#
#' @param minimal_output: if TRUE, just outputs the fitted g and the lfsr (useful for very big data sets where memory is an issue) #
#' @param g: the prior distribution for beta (usually estimated from the data; this is used primarily in simulated data to do computations with the "true" g)#
#' @param maxiter: maximum number of iterations of the EM algorithm#
#' @param cxx: flag to indicate whether to use the c++ (Rcpp) version#
#' #
#'#
#' @return a list with elements fitted.g is fitted mixture#
#' logLR : logP(D|mle(pi)) - logP(D|null)#
#' #
#' @export#
#' #
#' @examples #
#' beta = c(rep(0,100),rnorm(100))#
#' sebetahat = abs(rnorm(200,0,1))#
#' betahat = rnorm(200,beta,sebetahat)#
#' beta.ash = ash(betahat, sebetahat)#
#' summary(beta.ash)#
#' plot(betahat,beta.ash$PosteriorMean,xlim=c(-4,4),ylim=c(-4,4))#
#' #
#' #
#Things to do:#
# check sampling routine#
# check number of iterations#
ash = function(betahat,sebetahat,method = c("shrink","fdr"), #
               mixcompdist = c("normal","uniform","halfuniform"),#
               lambda1=1,lambda2=0,nullcheck=TRUE,df=NULL,randomstart=FALSE,#
               nullweight=10,nonzeromean=FALSE, #
               pointmass = FALSE, #
               onlylogLR = FALSE, #
               prior=c("uniform","nullbiased"), #
               mixsd=NULL, VB=FALSE,gridmult=sqrt(2),#
               minimaloutput=FALSE,#
               multiseqoutput=FALSE,#
               g=NULL,#
               maxiter = 5000,#
               cxx=FALSE){#
  #method provides a convenient interface to set a particular combinations of parameters for prior an#
  #If method is supplied, use it to set up specific values for these parameters; provide warning if values#
  #are also specified by user#
  #If method is not supplied use the user-supplied values (or defaults if user does not specify them)#
  if(!missing(method)){#
    method = match.arg(method) #
    if(method=="shrink"){#
      if(missing(prior)){#
        prior = "uniform"#
      } else {#
        warning("Specification of prior overrides default for method shrink")#
      }#
      if(missing(pointmass)){#
        pointmass=FALSE#
      } else {#
        warning("Specification of pointmass overrides default for method shrink")#
      }#
    }#
    if(method=="fdr"){#
      if(missing(prior)){#
        prior = "nullbiased"#
      } else {#
        warning("Specification of prior overrides default for method fdr")#
      }#
      if(missing(pointmass)){#
        pointmass=TRUE#
      } else {#
        warning("Specification of pointmass overrides default for method fdr")#
      }#
    }  #
  }#
  if(gridmult<=1&multiseqoutput!=TRUE)#
    stop("gridmult must be > 1")#
  mixcompdist = match.arg(mixcompdist)#
  # if(mixcompdist=="uniform" & pointmass==TRUE){#
  #    stop("point mass not yet implemented for uniform or half-uniform")#
  #  }#
  #  if(mixcompdist=="halfuniform" & pointmass==TRUE){#
  #    stop("point mass not yet implemented for uniform or half-uniform")#
  #  }#
  if(!is.numeric(prior)){#
    prior = match.arg(prior)#
  }  #
  if(length(sebetahat)==1){#
    sebetahat = rep(sebetahat,length(betahat))#
  }#
  if(length(sebetahat) != length(betahat)){#
    stop("Error: sebetahat must have length 1, or same length as betahat")#
  }#
  completeobs = (!is.na(betahat) & !is.na(sebetahat))#
  n=sum(completeobs)#
  if(n==0){#
    if(onlylogLR){#
      return(list(pi=NULL, logLR = 0))#
    }#
    else{#
      stop("Error: all input values are missing")#
    }#
  }  #
  if(!is.null(g)){#
    maxiter = 1 # if g is specified, don't iterate the EM#
    prior = rep(1,ncomp(g)) #prior is not actually used if g specified, but required to make sure EM doesn't produce warning#
    null.comp=1 #null.comp also not used, but required #
  } else {#
    if(is.null(mixsd)){#
      mixsd = autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)#
    }#
    if(pointmass){#
      mixsd = c(0,mixsd)#
    }#
    if(nonzeromean & is.null(df)){#
		nonzeromean.fit=nonzeromeanEM(betahat[completeobs], sebetahat[completeobs], mixsd, maxiter=maxiter)#
		betahat[completeobs]= betahat[completeobs] - nonzeromean.fit$nonzeromean#
	}#
	else if(nonzeromean & !is.null(df)){#
		stop("Error: Nonzero mean only implemented for df=NULL")#
	}#
    null.comp = which.min(mixsd) #which component is the "null"#
    k = length(mixsd)#
    if(!is.numeric(prior)){#
      if(prior=="nullbiased"){ # set up prior to favour "null"#
        prior = rep(1,k)#
        prior[null.comp] = nullweight #prior 10-1 in favour of null by default#
      }else if(prior=="uniform"){#
        prior = rep(1,k)#
      }#
    }#
    if(length(prior)!=k | !is.numeric(prior)){#
      stop("invalid prior specification")#
    }#
    if(randomstart){#
      pi = rgamma(k,1,1)#
    } else {#
      if(k<n){#
        pi=rep(1,k)/n #default initialization strongly favours null; puts weight 1/n on everything except null#
        pi[null.comp] = (n-k+1)/n #the motivation is data can quickly drive away from null, but tend to drive only slowly toward null.#
      } else {#
        pi=rep(1,k)/k#
      }#
    }#
    pi=normalize(pi)#
    if(!is.element(mixcompdist,c("normal","uniform","halfuniform"))) stop("Error: invalid type of mixcompdist")#
    if(mixcompdist=="normal") g=normalmix(pi,rep(0,k),mixsd)#
    if(mixcompdist=="uniform") g=unimix(pi,-mixsd,mixsd)#
    if(mixcompdist=="halfuniform"){#
      g = unimix(c(pi,pi)/2,c(-mixsd,rep(0,k)),c(rep(0,k),mixsd))#
      prior = rep(prior, 2)#
      pi = rep(pi, 2)#
    }#
  }#
  pi.fit=EMest(betahat[completeobs],lambda1*sebetahat[completeobs]+lambda2,g,prior,null.comp=null.comp,nullcheck=nullcheck,VB=VB,maxiter = maxiter, cxx=cxx, df=df)  #
  if (!onlylogLR){#
      n=length(betahat)#
      if (!multiseqoutput){#
          ZeroProb = rep(0,length=n)#
          NegativeProb = rep(0,length=n)#
      }#
      if (!minimaloutput){#
          PosteriorMean = rep(0,length=n)#
          PosteriorSD = rep(0,length=n)#
      }#
      if (!multiseqoutput){#
          ZeroProb[completeobs] = colSums(comppostprob(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)[comp_sd(pi.fit$g)==0,,drop=FALSE])#
          NegativeProb[completeobs] = cdf_post(pi.fit$g, 0, betahat[completeobs],sebetahat[completeobs],df) - ZeroProb[completeobs]#
      }#
      if (!minimaloutput){#
          PosteriorMean[completeobs] = postmean(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
          PosteriorSD[completeobs] = postsd(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
      }#
                                        #FOR MISSING OBSERVATIONS, USE THE PRIOR INSTEAD OF THE POSTERIOR#
      if (!multiseqoutput){#
          ZeroProb[!completeobs] = sum(mixprop(pi.fit$g)[comp_sd(pi.fit$g)==0])#
          NegativeProb[!completeobs] = mixcdf(pi.fit$g,0)#
          lfsr = compute_lfsr(NegativeProb,ZeroProb)#
      }#
      if (!minimaloutput){#
          PosteriorMean[!completeobs] = mixmean(pi.fit$g)#
          PosteriorSD[!completeobs] = mixsd(pi.fit$g)#
      }#
      if (!minimaloutput & !multiseqoutput){#
          PositiveProb = 1- NegativeProb-ZeroProb#
          lfsra = compute_lfsra(PositiveProb,NegativeProb,ZeroProb) #
          lfdr = ZeroProb#
          qvalue = qval.from.lfdr(lfdr)#
      }#
  }#
  if (!minimaloutput)#
      logLR = tail(pi.fit$loglik,1) - pi.fit$null.loglik#
  if (onlylogLR)#
      return(list(fitted.g=pi.fit$g, logLR = logLR, df=df))#
  else if (minimaloutput)#
      return(list(fitted.g = pi.fit$g, lfsr = lfsr, fit = pi.fit,df=df))#
  else if (multiseqoutput)#
      return(list(fitted.g = pi.fit$g, logLR = logLR, PosteriorMean = PosteriorMean, PosteriorSD = PosteriorSD, call= match.call(),df=df))#
  else{#
      result = list(fitted.g = pi.fit$g, logLR = logLR, PosteriorMean = PosteriorMean, PosteriorSD = PosteriorSD, PositiveProb = PositiveProb, NegativeProb = NegativeProb, ZeroProb = ZeroProb, lfsr = lfsr,lfsra = lfsra, lfdr = lfdr, qvalue = qvalue, fit = pi.fit, lambda1 = lambda1, lambda2 = lambda2, call = match.call(), data = list(betahat = betahat, sebetahat=sebetahat),df=df)#
      class(result) = "ash"#
      return(result)#
  }#
}#
  #if(nsamp>0){#
  #  sample = posterior_sample(post,nsamp)#
  #}#
#
# #' @title Faster version of function ash#
# #'#
# #' @description This function has similar functionality as ash, but only returns some of the outputs.#
# #'#
# #' @param betahat, a p vector of estimates#
# #' @param sebetahat, a p vector of corresponding standard errors#
# #' @param nullcheck: whether to check that any fitted model exceeds the "null" likelihood in which all weight is on the first component#
# #' @param randomstart: bool, indicating whether to initialize EM randomly. If FALSE, then initializes to prior mean (for EM algorithm) or prior (for VBEM)#
# #' @param pointmass: bool, indicating whether to use a point mass at zero as one of components for a mixture distribution#
# #' @param onlylogLR: bool, indicating whether to use this function to get logLR. Skip posterior prob, posterior mean, lfdr...#
# #' @param prior: string, or numeric vector indicating Dirichlet prior on mixture proportions (defaults to "uniform", or 1,1...,1; also can be "nullbiased" 1,1/k-1,...,1/k-1 to put more weight on first component)#
# #' @param mixsd: vector of sds for underlying mixture components#
# #' @param VB: whether to use Variational Bayes to estimate mixture proportions (instead of EM to find MAP estimate)#
# #' @param gridmult: the multiplier by which the default grid values for mixsd differ by one another. (Smaller values produce finer grids)#
# #' @param g: the prior distribution for beta (usually estimated from the data; this is used primarily in simulated data to do computations with the "true" g)#
# #' @param cxx: flag to indicate whether to use the c++ (Rcpp) version#
# #'#
# #' @return a list with elements fitted.g is fitted mixture#
# #' logLR : logP(D|mle(pi)) - logP(D|null)#
# #'#
# #' @export#
# fast.ash = function(betahat,sebetahat, #
#                     nullcheck=TRUE,randomstart=FALSE, #
#                     pointmass = TRUE,    #
#                     prior=c("nullbiased","uniform"), #
#                     mixsd=NULL, VB=FALSE,gridmult=4,#
#                     g=NULL, cxx=TRUE,#
#                     onlylogLR = FALSE,df=NULL){#
#   #
#   if(onlylogLR){#
#     pointmass <- TRUE  #
#   }#
#   #
#   #If method is supplied, use it to set up defaults; provide warning if these default values#
#   #are also specified by user#
#   if(!is.numeric(prior)){#
#     prior = match.arg(prior)#
#   }#
#   #
#   if(length(sebetahat)==1){#
#     sebetahat = rep(sebetahat,length(betahat))#
#   }#
#   if(length(sebetahat) != length(betahat)){#
#     stop("Error: sebetahat must have length 1, or same length as betahat")#
#   }#
#   #
#   completeobs = (!is.na(betahat) & !is.na(sebetahat))#
#   if(sum(completeobs)==0){#
#     if(onlylogLR){#
#       return(list(pi=NULL, logLR = 0))#
#     }else{#
#       stop("Error: all input values are missing")#
#     }#
#   }  #
#   #
#   if(is.null(mixsd)){#
#     mixsd= autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)#
#   }#
#   if(pointmass){#
#     mixsd = c(0,mixsd)#
#   }#
#   #
#   k=length(mixsd)  #
#   null.comp = which.min(mixsd) #which component is the "null"#
#   #
#   if(!is.numeric(prior)){#
#     if(prior=="nullbiased"){ # set up prior to favour "null"#
#       prior = rep(1,k)#
#       prior[null.comp] = 10 #prior 10-1 in favour of null#
#     }else if(prior=="uniform"){#
#       prior = rep(1,k)#
#     }#
#   }#
#   #
#   if(length(prior)!=k | !is.numeric(prior)){#
#     stop("invalid prior specification")#
#   }#
#   #
#   if(missing(g)){#
#     pi = prior^2 #default is to initialize pi at prior (mean)#
#     if(randomstart){pi=rgamma(k,1,1)}#
#     pi=normalize(pi)#
#     g=normalmix(pi,rep(0,k),mixsd)#
#     maxiter = 5000#
#   } else {#
#     maxiter = 1; # if g is specified, don't iterate the EM #
#   }#
#   #
#   pi.fit=EMest(betahat[completeobs],sebetahat[completeobs],g,prior,null.comp=null.comp,nullcheck=nullcheck,VB=VB,maxiter = maxiter, cxx=cxx, df=df)  #
#   #
#   if(onlylogLR){#
#     logLR = tail(pi.fit$loglik,1) - pi.fit$null.loglik#
#     return(list(pi=pi.fit$pi, logLR = logLR))#
#   }else{#
#     #
#     n=length(betahat)#
#     PosteriorMean = rep(0,length=n)#
#     PosteriorSD=rep(0,length=n)#
#     #
#     if(is.null(df)){#
#       PosteriorMean[completeobs] = postmean(pi.fit$g,betahat[completeobs],sebetahat[completeobs])#
#       PosteriorSD[completeobs] =postsd(pi.fit$g,betahat[completeobs],sebetahat[completeobs]) #
#     }#
#     else{#
#       PosteriorMean[completeobs] = postmean_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
#       PosteriorSD[completeobs] =postsd_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
#     }#
#     #FOR MISSING OBSERVATIONS, USE THE PRIOR INSTEAD OF THE POSTERIOR#
#     PosteriorMean[!completeobs] = mixmean(pi.fit$g)#
#     PosteriorSD[!completeobs] =mixsd(pi.fit$g)  #
#     #
#     result = list(fitted.g=pi.fit$g,PosteriorMean = PosteriorMean,PosteriorSD=PosteriorSD,call=match.call(),data=list(betahat = betahat, sebetahat=sebetahat))#
#     return(result)#
#   }#
#   #if(nsamp>0){#
#   #  sample = posterior_sample(post,nsamp)#
#   #}#
# }#
compute_lfsr = function(NegativeProb,ZeroProb){#
  ifelse(NegativeProb> 0.5*(1-ZeroProb),1-NegativeProb,NegativeProb+ZeroProb)#
}#
#
compute_lfsra = function(PositiveProb, NegativeProb,ZeroProb){#
  ifelse(PositiveProb<NegativeProb,2*PositiveProb+ZeroProb,2*NegativeProb+ZeroProb)  #
}  #
#
#' @title Estimate unimodal nonzero mean of a mixture model by EM algorithm#
#'#
#' @description Given the data, standard error of the data and standard deviations of the Gaussian mixture model, estimate the mean of a unimodal Gaussian mixture by an EM algorithm.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates unimodal mean \eqn{\mu} by EM algorithm. Uses the SQUAREM package to accelerate convergence of EM. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' #
#' @param betahat, a p vector of estimates #
#' @param sebetahat, a p vector of corresponding standard errors#
#' @param mixsd: vector of sds for underlying mixture components #
#' @param pi.init, the initial value of \eqn{\pi} to use. If not specified defaults to (1/k,...,1/k).#
#' @param tol, the tolerance for convergence of log-likelihood.#
#' @param maxiter the maximum number of iterations performed#
#' #
#' @return A list, including the estimates (\eqn{\mu}) and (\eqn{\pi}), the log likelihood for each iteration (NQ)#
#' and a flag to indicate convergence#
#'  #
#' @export#
#' #
#' #
nonzeromeanEM = function(betahat, sebetahat, mixsd, pi.init=NULL,tol=1e-7,maxiter=5000){#
  if(is.null(pi.init)){#
    pi.init = rep(1/length(mixsd),length(mixsd))# Use as starting point for pi#
  }#
  mupi=c(mean(betahat),pi.init)#
  res=squarem(par=mupi,fixptfn=nonzeromeanEMfixpoint,objfn=nonzeromeanEMobj,betahat=betahat,sebetahat=sebetahat,mixsd=mixsd,control=list(maxiter=maxiter,tol=tol))#
  return(list(nonzeromean=res$par[1],pi=res$par[-1],NQ=-res$value.objfn,niter = res$iter, converged=res$convergence,post=res$par))#
}#
#
nonzeromeanEMfixpoint = function(mupi,betahat,sebetahat,mixsd){#
	#omegamatrix=matrix(NA,nrow=length(betahat),ncol=length(mixsd))#
	mu=mupi[1]#
	pimean=mupi[-1]#
	sdmat = sqrt(outer(sebetahat ^2,mixsd^2,"+")) #
	xmat=matrix(rep(betahat,length(mixsd)),ncol=length(mixsd))#
	omegamatrix=t(t(dnorm(xmat,mean=mu,sd=sdmat))*pimean)#
	omegamatrix=omegamatrix /rowSums(omegamatrix)#
	pinew=normalize(colSums(omegamatrix))#
	munew=sum(omegamatrix*xmat/(sdmat^2))/sum(omegamatrix/(sdmat^2))#
	mupi=c(munew,pinew)#
	return(mupi)#
}#
#
nonzeromeanEMobj = function(mupi,betahat,sebetahat,mixsd){#
	mu=mupi[1]#
	pimean=mupi[-1]#
	sdmat = sqrt(outer(sebetahat ^2,mixsd^2,"+")) #
	xmat=matrix(rep(betahat,length(mixsd)),ncol=length(mixsd))#
	omegamatrix=t(t(dnorm(xmat,mean=mu,sd=sdmat))*pimean)#
	omegamatrix=omegamatrix /rowSums(omegamatrix)#
	NegativeQ=-sum(omegamatrix*dnorm(xmat,mean=mu,sd=sdmat,log=TRUE))#
	return(NegativeQ)#
}#
#' @title Estimate posterior distribution on mixture proportions of a mixture model by a Variational Bayes EM algorithm#
#'#
#' @description Given the individual component likelihoods for a mixture model, estimates the posterior on #
#' the mixture proportions by an VBEM algorithm. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates posterior on mixture proportions \eqn{\pi} by Variational Bayes, #
#' with a Dirichlet prior on \eqn{\pi}. #
#' Algorithm adapted from Bishop (2009), Pattern Recognition and Machine Learning, Chapter 10.#
#' #
#' @param matrix_lik: a n by k matrix with (j,k)th element equal to \eqn{f_k(x_j)}.#
#' @param prior: a k vector of the parameters of the Dirichlet prior on \eqn{\pi}. Recommended to be rep(1,k)#
#' @param post.init: the initial value of the posterior parameters. If not specified defaults to the prior parameters.#
#' @param tol: the tolerance for convergence of log-likelihood bound.#
#' @param maxiter: the maximum number of iterations performed#
#' #
#' @return A list, whose components include point estimates (pihat), #
#' the parameters of the fitted posterior on \eqn{\pi} (pipost),#
#' the bound on the log likelihood for each iteration (B)#
#' and a flag to indicate convergence (converged).#
#'  #
#' @export#
#' #
mixVBEM = function(matrix_lik, prior, pi.init = NULL,tol=1e-7, maxiter=5000){#
  k=ncol(matrix_lik)#
  if(is.null(pi.init)){#
    pi.init = rep(1,k)# Use as starting point for pi#
  } #
  res = squarem(par=pi.init,fixptfn=VBfixpoint, objfn=VBnegpenloglik,matrix_lik=matrix_lik, prior=prior, control=list(maxiter=maxiter,tol=tol))#
  return(list(pihat = res$par/sum(res$par), B=res$value.objfn, niter = res$iter, converged=res$convergence,post=res$par))#
}#
VBfixpoint = function(pipost, matrix_lik, prior){  #
  n=nrow(matrix_lik)#
  k=ncol(matrix_lik)#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost*matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  pipostnew = colSums(classprob) + prior#
  return(pipostnew)#
}#
#
VBnegpenloglik=function(pipost,matrix_lik,prior){#
  return(-VBpenloglik(pipost,matrix_lik,prior))#
}#
#
VBpenloglik = function(pipost, matrix_lik, prior){#
  n=nrow(matrix_lik)#
  k=ncol(matrix_lik)#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost*matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  B= sum(classprob*log(avgpipost*matrix_lik),na.rm=TRUE) - diriKL(prior,pipost) - sum(classprob*log(classprob)) #
  return(B)#
}#
#' @title Estimate mixture proportions of a mixture model by EM algorithm#
#'#
#' @description Given the individual component likelihoods for a mixture model, estimates the mixture proportions by an EM algorithm.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates mixture proportions \eqn{\pi} by maximum likelihood, or by maximum a posteriori (MAP) estimation for a Dirichlet prior on $\pi$ #
#' (if a prior is specified).  Uses the SQUAREM package to accelerate convergence of EM. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' #
#' @param matrix_lik, a n by k matrix with (j,k)th element equal to \eqn{f_k(x_j)}.#
#' @param prior, a k vector of the parameters of the Dirichlet prior on \eqn{\pi}. Recommended to be rep(1,k)#
#' @param pi.init, the initial value of \eqn{\pi} to use. If not specified defaults to (1/k,...,1/k).#
#' @param tol, the tolerance for convergence of log-likelihood.#
#' @param maxiter the maximum number of iterations performed#
#' #
#' @return A list, including the estimates (pihat), the log likelihood for each interation (B)#
#' and a flag to indicate convergence#
#'  #
#' @export#
#' #
#' #
mixEM = function(matrix_lik, prior, pi.init = NULL,tol=1e-7, maxiter=5000){#
  if(is.null(pi.init)){#
    pi.init = rep(1/k,k)# Use as starting point for pi#
  } #
  res = squarem(par=pi.init,fixptfn=fixpoint, objfn=negpenloglik,matrix_lik=matrix_lik, prior=prior, control=list(maxiter=maxiter,tol=tol))#
  return(list(pihat = normalize(pmax(0,res$par)), B=res$value.objfn, #
              niter = res$iter, converged=res$convergence))#
}#
#
# helper functions used by mixEM#
normalize = function(x){return(x/sum(x))}#
#
fixpoint = function(pi, matrix_lik, prior){  #
  pi = normalize(pmax(0,pi)) #avoid occasional problems with negative pis due to rounding#
  m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
  m.rowsum = rowSums(m)#
  classprob = m/m.rowsum #an n by k matrix#
  pinew = normalize(colSums(classprob) + prior - 1)#
  return(pinew)#
}#
#
negpenloglik = function(pi,matrix_lik,prior){return(-penloglik(pi,matrix_lik,prior))}#
#
penloglik = function(pi, matrix_lik, prior){#
  pi = normalize(pmax(0,pi))#
  m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
  m.rowsum = rowSums(m)#
  loglik = sum(log(m.rowsum))#
  subset = (prior != 1.0)#
  priordens = sum((prior-1)[subset]*log(pi[subset]))#
  return(loglik+priordens)#
}#
#
#The kth element of this vector is the derivative #
#of the loglik for $\pi=(\pi_0,...,1-\pi_0,...)$ with respect to $\pi_0$ at $\pi_0=1$.#
gradient = function(matrix_lik){#
  n = nrow(matrix_lik)#
  grad = n - colSums(matrix_lik/matrix_lik[,1]) #
  return(grad)#
}#
#
# mixEM = function(matrix_lik, prior, pi.init = NULL,tol=0.0001, maxiter=5000){#
#   n=nrow(matrix_lik)#
#   k=ncol(matrix_lik)#
#   B = rep(0,maxiter)#
#   pi = pi.init#
#   if(is.null(pi.init)){#
#     pi = rep(1/k,k)# Use as starting point for pi#
#   } #
#   pi = ifelse(pi<1e-5,1e-5,pi) #set any estimates that are too small to be just very small#
#   pi = normalize(pi)#
#   #
#   loglik = rep(0,maxiter)#
#   priordens= rep(0,maxiter)#
#   m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
#   m.rowsum = rowSums(m)#
#   loglik[1] = sum(log(m.rowsum))#
#   priordens[1] = sum((prior-1)*log(pi)) #
#   classprob = m/m.rowsum #an n by k matrix#
#   i=1#
#   if(maxiter >= 2){#
#     for(i in 2:maxiter){  #
#       pi = colSums(classprob) + prior-1#
#       pi = ifelse(pi<1e-5,1e-5,pi) #set any estimates that are less than zero, which can happen with prior<1, to 0#
#       pi = normalize(pi)#
#         #
#       #Now re-estimate pi#
#       m  = t(pi * t(matrix_lik)) #
#       m.rowsum = rowSums(m)#
#       loglik[i] = sum(log(m.rowsum))#
#       priordens[i] = sum((prior-1)*log(pi)) #
#       classprob = m/m.rowsum#
#     #
#     #
#       if(abs(loglik[i]+priordens[i]-loglik[i-1]-priordens[i-1])<tol) break;#
#     }#
#   }#
#   converged=(abs(loglik[i]+priordens[i]-loglik[i-1]-priordens[i-1])<tol)#
#   if(!converged){#
#       warning("EM algorithm in function mixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
#   }#
#   return(list(pihat = pi, B=loglik[1:i], #
#               niter = i, converged=converged))#
# }#
#estimate mixture proportions of sigmaa by EM algorithm#
#prior gives the parameter of a Dirichlet prior on pi#
#(prior is used to encourage results towards smallest value of sigma when#
#likelihood is flat)#
#nullcheck indicates whether to check whether the loglike exceeds the null#
#(may not want to use if prior is used)#
#VB provides an approach to estimate the approximate posterior distribution#
#of mixture proportions of sigmaa by variational Bayes method#
#(use Dirichlet prior and approximate Dirichlet posterior)#
#if cxx TRUE use cpp version of R function mixEM#
EMest = function(betahat,sebetahat,g,prior,null.comp=1,nullcheck=TRUE,VB=FALSE, maxiter=5000, cxx=TRUE, df=NULL){ #
  pi.init = g$pi#
  k=ncomp(g)#
  n = length(betahat)#
  tol = min(0.1/n,1e-5) # set convergence criteria to be more stringent for larger samples#
  matrix_lik = t(compdens_conv(g,betahat,sebetahat,df))#
  #checks whether the gradient at pi0=1 is positive (suggesting that this is a fixed point)#
  #if(nullcheck){#
  #  if(all(gradient(matrix_lik)>=0)){#
  #    pi.init=rep(0,k)#
  #    pi.init[null.comp]=1 #this will make pi.init=(1,0,0...,0) which is a fixed point of the EM#
  #  }#
  #}#
  if(VB==TRUE){#
    EMfit=mixVBEM(matrix_lik,prior,maxiter=maxiter)}#
  else{#
    if (cxx==TRUE){#
      EMfit = cxxMixEM(matrix_lik,prior,pi.init,1e-5, maxiter) #currently use different convergence criteria for cxx version #
      if(!EMfit$converged){#
        warning("EM algorithm in function cxxMixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
      }#
    }#
    else{#
      EMfit = mixEM(matrix_lik,prior,pi.init,tol, maxiter)#
      if(!EMfit$converged & !(maxiter==1)){#
        warning("EM algorithm in function mixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
      }#
    }#
  }#
  pi = EMfit$pihat     #
  penloglik = EMfit$B #
  converged = EMfit$converged#
  niter = EMfit$niter#
  loglik.final =  penloglik(pi,matrix_lik,1) #compute penloglik without penalty#
  null.loglik = sum(log(matrix_lik[,null.comp]))  #
  if(nullcheck==TRUE & VB==FALSE){ #null check doesn't work with VB yet#
    pinull = rep(0,k)#
    pinull[null.comp]=1#
    null.penloglik = penloglik(pinull,matrix_lik,prior)#
    final.penloglik = penloglik(pi,matrix_lik,prior)#
    if(null.penloglik > final.penloglik){ #check whether exceeded "null" likelihood where everything is null#
      pi=pinull#
      loglik.final=penloglik(pi,matrix_lik,1)#
    }#
  }#
  g$pi=pi#
  return(list(loglik=loglik.final,null.loglik=null.loglik,#
              matrix_lik=matrix_lik,converged=converged,g=g))#
}#
#' @title Compute Posterior#
#'#
#' @description Return the posterior on beta given a prior (g) that is a mixture of normals (class normalmix) #
#' and observation betahat \sim N(beta,sebetahat)#
#'#
#' @details This can be used to obt#
#'#
#' @param g: a normalmix with components indicating the prior; works only if g has means 0#
#' @param betahat (n vector of observations) #
#' @param sebetahat (n vector of standard errors/deviations of observations)#
#' #
#' @return A list, (pi1,mu1,sigma1) whose components are each k by n matrices#
#' where k is number of mixture components in g, n is number of observations in betahat#
#' #
#' @export#
#' #
#' #
posterior_dist = function(g,betahat,sebetahat){#
  if(class(g)!="normalmix"){#
    stop("Error: posterior_dist implemented only for g of class normalmix")#
  }#
  pi0 = g$pi#
  mu0 = g$mean#
  sigma0 = g$sd  #
  k= length(pi0)#
  n= length(betahat)#
  if(!all.equal(g$mean,rep(0,k))) stop("Error: posterior_dist currently only implemented for zero-centered priors")#
  pi1 = pi0 * t(matrix_dens(betahat,sebetahat,sigma0))#
  pi1 = apply(pi1, 2, normalize) #pi1 is now an k by n matrix#
  #make k by n matrix versions of sigma0^2 and sebetahat^2#
  # and mu0 and betahat#
  s0m2 = matrix(sigma0^2,nrow=k,ncol=n,byrow=FALSE)#
  sebm2 = matrix(sebetahat^2,nrow=k,ncol=n, byrow=TRUE)#
  mu0m = matrix(mu0,nrow=k,ncol=n,byrow=FALSE)#
  bhatm = matrix(betahat,nrow=k,ncol=n,byrow=TRUE)#
  sigma1 = (s0m2*sebm2/(s0m2 + sebm2))^(0.5)  #
  w = sebm2/(s0m2 + sebm2)#
  mu1 = w*mu0m + (1-w)*bhatm#
  #WHERE DATA ARE MISSING, SET POSTERIOR = PRIOR#
  ismiss = (is.na(betahat) | is.na(sebetahat)) #
  pi1[,ismiss] = pi0#
  mu1[,ismiss] = mu0#
  sigma1[,ismiss] = sigma0#
  return(list(pi=pi1,mu=mu1,sigma=sigma1))#
}#
#
#return matrix of densities of observations (betahat) #
# assuming betahat_j \sim N(0, sebetahat_j^2 + sigmaavec_k^2)#
#normalized by maximum of each column#
#INPUT#
#betahat is n vector, #
#sebetahat is n vector, #
#sigmaavec is k vector#
#return is n by k matrix of the normal likelihoods, #
# with (j,k)th element the density of N(betahat_j; mean=0, var = sebetahat_j^2 + sigmaavec_k^2)#
#normalized to have maximum 1 in each column#
matrix_dens = function(betahat, sebetahat, sigmaavec){#
  k = length(sigmaavec)#
  n = length(betahat)#
  ldens = dnorm(betahat,0,sqrt(outer(sebetahat^2,sigmaavec^2,FUN="+")),log=TRUE)#
  maxldens = apply(ldens, 1, max)#
  ldens = ldens - maxldens#
  return(exp(ldens))#
}#
#
#return the "effective" estimate#
#that is the effect size betanew whose z score betanew/se#
#would give the same p value as betahat/se compared to a t with df#
effective.effect=function(betahat,se,df){#
  p = pt(betahat/se,df)#
  qnorm(p,sd=se)#
}#
#' @title Function to compute q values from local false discovery rates#
#'#
#' @description Computes q values from a vector of local fdr estimates#
#'#
#' @details The q value for a given lfdr is an estimate of the (tail) False Discovery Rate #
#' for all findings with a smaller lfdr, and is found by the average of the lfdr for#
#' all more significant findings. See Storey (2003), Annals of Statistics, for definition of q value.  #
#' #
#' #
#' @param lfdr, a vector of local fdr estimates#
#'#
#' @return vector of q values#
#' #
#' @export#
qval.from.lfdr = function(lfdr){#
  o = order(lfdr)#
  qvalue=rep(NA,length(lfdr))#
  qvalue[o] = (cumsum(sort(lfdr))/(1:sum(!is.na(lfdr))))#
  return(qvalue)#
}#
#
# try to select a default range for the sigmaa values#
# that should be used, based on the values of betahat and sebetahat#
# mult is the multiplier by which the sds differ across the grid#
autoselect.mixsd = function(betahat,sebetahat,mult){#
  sigmaamin = min(sebetahat)/10 #so that the minimum is small compared with measurement precision#
  if(all(betahat^2<sebetahat^2)){#
    sigmaamax = 8*sigmaamin #to deal with the occassional odd case where this could happen; 8 is arbitrary#
  } else {#
    sigmaamax = 2*sqrt(max(betahat^2-sebetahat^2)) #this computes a rough largest value you'd want to use, based on idea that sigmaamax^2 + sebetahat^2 should be at least betahat^2   #
  }#
  if(mult==0){#
    return(c(0,sigmaamax/2))#
  }else{#
    npoint = ceiling(log2(sigmaamax/sigmaamin)/log2(mult))#
    return(mult^((-npoint):0) * sigmaamax)#
  }#
}#
#return the KL-divergence between 2 dirichlet distributions#
#p,q are the vectors of dirichlet parameters of same lengths#
diriKL = function(p,q){#
  p.sum = sum(p)#
  q.sum = sum(q)#
  k = length(q)#
  KL = lgamma(q.sum)-lgamma(p.sum)+sum((q-p)*(digamma(q)-digamma(rep(q.sum,k))))+sum(lgamma(p)-lgamma(q))#
  return(KL)#
}#
#
#helper function for VBEM#
VB.update = function(matrix_lik, pipost){#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost * matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  B = sum(classprob*log(avgpipost*matrix_lik),na.rm=TRUE) - diriKL(prior,pipost) #negative free energy#
  return(list(classprob=classprob,B=B))#
}
ASH UTILITY FUNCTIONS #############################
#
#' @title Summary method for ash object#
#'#
#' @description Print summary of fitted ash object#
#'#
#' @details See readme for more details#
#' #
#' @export#
#' #
summary.ash=function(a){#
  print(a$fitted.g)#
  print(tail(a$fit$loglik,1),digits=10)#
  print(a$fit$converged)#
}#
#
#' @title Print method for ash object#
#'#
#' @description Print the fitted distribution of beta values in the EB hierarchical model#
#'#
#' @details None#
#' #
#' @export#
#' #
print.ash =function(a){#
  print(a$fitted.g)#
}#
#
#' @title Plot method for ash object#
#'#
#' @description Plot the density of the underlying fitted distribution#
#'#
#' @details None#
#' #
#' @export#
#' #
plot.ash = function(a,xmin,xmax,...){#
  x = seq(xmin,xmax,length=1000)#
  y = density(a,x)#
  plot(y,type="l",...)#
}#
#
#compute the predictive density of an observation#
#given the fitted ash object a and the vector se of standard errors#
#not implemented yet#
predictive=function(a,se){#
}#
#' @title Get fitted loglikelihood for ash object#
#'#
#' @description Return the log-likelihood of the data under the fitted distribution#
#'#
#' @param a the fitted ash object#
#'#
#' @details None#
#' #
#' @export#
#' #
#'#
get_loglik = function(a){#
  return(tail(a$fit$loglik,1))#
}#
#
#' @title Get pi0 estimate for ash object#
#'#
#' @description Return estimate of the null proportion, pi0#
#'#
#' @param a the fitted ash object#
#'#
#' @details Extracts the estimate of the null proportion, pi0, from the object a#
#' #
#' @export#
#' #
get_pi0 = function(a){#
  null.comp = comp_sd(a$fitted.g)==0#
  return(sum(a$fitted.g$pi[null.comp]))#
}#
#
#' @title Compute loglikelihood for data from ash fit#
#'#
#' @description Return the log-likelihood of the data betahat, with standard errors betahatsd, #
#' under the fitted distribution in the ash object. #
#' #
#'#
#' @param a the fitted ash object#
#' @param betahat the data#
#' @param betahatsd the observed standard errors#
#' @param zscores indicates whether ash object was originally fit to z scores #
#' @details None#
#' #
#' @export#
#' #
#'#
loglik.ash = function(a,betahat,betahatsd,zscores=FALSE){#
  g=a$fitted.g#
  FUN="+"#
  if(zscores==TRUE){#
    g$sd = sqrt(g$sd^2+1) #
    FUN="*"#
  }#
  return(loglik_conv(g,betahat, betahatsd,FUN))#
}#
#
#' @title Density method for ash object#
#'#
#' @description Return the density of the underlying fitted distribution#
#'#
#' @param a the fitted ash object#
#' @param x the vector of locations at which density is to be computed#
#'#
#' @details None#
#' #
#' @export#
#' #
#'#
density.ash=function(a,x){list(x=x,y=dens(a$fitted.g,x))}#
#
#' @title cdf method for ash object#
#'#
#' @description Computed the cdf of the underlying fitted distribution#
#'#
#' @param a the fitted ash object#
#' @param x the vector of locations at which cdf is to be computed#
#' @param lower.tail (default=TRUE) whether to compute the lower or upper tail#
#'#
#' @details None#
#' #
#' @export#
#' #
#'#
cdf.ash=function(a,x,lower.tail=TRUE){#
  return(list(x=x,y=mixcdf(a$fitted.g,x,lower.tail)))#
}#
#' @title Credible Interval Computation for the ash object#
#'#
#' @description Given the ash object return by the main function ash, this function computes the corresponding credible interval of the mixture model.#
#'#
#' @details Uses default optimization function and perform component-wise credible interval computation. The computation cost is linear of the length of betahat.#
#'#
#' @param a the fitted ash object #
#' @param levels, the level for the credible interval, (default=0.95)#
#' @param tol, the tolerance for convergence of log-likelihood.#
#' #
#' @return A matrix, with first column being the posterior mean, second and third column being the lower bound and upper bound for the credible interval. #
#'  #
#' @export#
#' #
#' #
ashci = function (a,level=0.95){#
  x=a$data$betahat#
  s=a$data$sebetahat#
  m=a$fitted.g#
  lower=min(x)-qnorm(level)*(max(m$sd)+max(s))#
  upper=max(x)+qnorm(level)*(max(m$sd)+max(s))#
  CImatrix=matrix(NA,nrow=length(x),ncol=3)	#
  colnames(CImatrix)=c("Posterior Mean",(1-level)/2,(1+level)/2)#
  CImatrix[,1]=a$PosteriorMean#
  if( class(a$fitted.g) == "normalmix" | class(a$fitted.g) == "unimix" ){#
    for(i in 1:length(x)){#
	  CImatrix[i,2]=optim(par=a$PosteriorMean[i],f=ci.lower,m=m,x=x[i],s=s[i],level=level,df=df,method="Brent",lower=lower,upper=upper)$par#
	  CImatrix[i,3]=optim(par=a$PosteriorMean[i],f=ci.upper,m=m,x=x[i],s=s[i],level=level,df=df,method="Brent",lower=lower,upper=upper)$par#
	}#
  }#
  else{stop(paste("Invalid class",class(m)))}#
  return(CImatrix)#
}#
#
ci.lower=function(z,m,x,s,level,df){#
	tailprob=cdf_post(m,z,x,s,df)#
	return(abs(tailprob-(1-level)/2))#
}#
#
ci.upper=function(z,m,x,s,level,df){#
	tailprob=1-cdf_post(m,z,x,s,df)#
	return(abs(tailprob-(1-level)/2))#
}
GENERIC FUNCTIONS #############################
# find matrix of densities at y, for each component of the mixture#
# INPUT y is an n-vector#
# OUTPUT k by n matrix of densities#
compdens = function(x,y,log=FALSE){#
  UseMethod("compdens")#
}#
compdens.default = function(x,y,log=FALSE){#
  stop(paste("Invalid class", class(m), "for first argument in",  match.call()))  #
}#
#
#standard deviations#
comp_sd = function(m){#
  UseMethod("comp_sd")#
}#
comp_sd.default = function(m){#
  stop("method comp_sd not written for this class")#
}#
#
#second moments#
comp_mean2 = function(m){#
  UseMethod("comp_mean2")#
}#
comp_mean2.default = function(m){#
  comp_sd(m)^2 + comp_mean(m)^2#
}#
#return the overall mean of the mixture#
mixmean = function(m){#
  UseMethod("mixmean")#
}#
mixmean.default = function(m){#
  sum(m$pi * comp_mean(m))#
}#
#
#return the overall second moment of the mixture#
mixmean2 = function(m){#
  UseMethod("mixmean2")#
}#
mixmean2.default = function(m){#
  sum(m$pi * comp_mean2(m))#
}#
#
#return the overall sd of the mixture#
mixsd = function(m){#
  UseMethod("mixsd")#
}#
mixsd.default = function(m){#
  sqrt(mixmean2(m)-mixmean(m)^2)#
}#
#
#means#
comp_mean = function(m){#
  UseMethod("comp_mean")#
}#
comp_mean.default = function(m){#
  stop("method comp_mean not written for this class")#
}#
#
#number of components#
ncomp = function(m){#
  UseMethod("ncomp")#
}#
ncomp.default = function(m){#
  return(length(m$pi))#
}#
#
#return mixture proportions, a generic function#
mixprop = function(m){#
  UseMethod("mixprop")#
}#
mixprop.default = function(m){#
  m$pi#
}#
#
#' @title mixcdf#
#'#
#' @description Returns cdf for a mixture (generic function)#
#' #
#' @details None#
#' #
#' @param x a mixture (eg of type normalmix or unimix)#
#' @param y locations at which cdf to be computed#
#' @param lower.tail: boolean indicating whether to report lower tail#
#' #
#' @return an object of class normalmix#
#' #
#' @export#
#' #
#' @examples mixcdf(normalmix(c(0.5,0.5),c(0,0),c(1,2)),seq(-4,4,length=100))#
#' #
mixcdf = function(x,y,lower.tail=TRUE){#
  UseMethod("mixcdf")#
}#
#' @title mixcdf.default#
#' @export#
#' #
mixcdf.default = function(x,y,lower.tail=TRUE){#
  x$pi %*% comp_cdf(x,y,lower.tail)#
}#
#
#find cdf for each component, a generic function#
comp_cdf = function(x,y,lower.tail=TRUE){#
  UseMethod("comp_cdf")#
}#
comp_cdf.default = function(x,y,lower.tail=TRUE){#
  stop("comp_cdf not implemented for this class")#
}#
#find density at y, a generic function#
dens = function(x,y){#
  UseMethod("dens")#
}#
dens.default = function(x,y){#
  return (x$pi %*% compdens(x, y))#
}#
#
#find log likelihood of data in x (a vector) for mixture in m#
loglik = function(m,x){#
  UseMethod("loglik")#
}#
loglik.default = function(m,x){#
  sum(log(dens(m,x)))#
}#
#
#find log likelihood of data in betahat, when #
#the mixture m is convolved with a normal with sd betahatsd#
#betahatsd is an n vector#
#betahat is an n vector#
#v is the degree of freedom#
#' @title loglik_conv#
#' #
#' @export#
#' #
loglik_conv = function(m,betahat,betahatsd,v,FUN="+"){#
  UseMethod("loglik_conv")#
}#
#' @title loglik_conv.default#
#' #
#' @export#
#' #
loglik_conv.default = function(m,betahat,betahatsd,v,FUN="+"){#
  sum(log(dens_conv(m,betahat,betahatsd,v,FUN)))#
}#
#
#compute the density of the components of the mixture m#
#when convoluted with a normal with standard deviation s#
#or a scaled (se) student.t with df v#
#the density is evaluated at x#
#x and s are n-vectors#
#m is a mixture with k components#
#output is a k by n matrix of densities#
compdens_conv = function(m,x,s,v,FUN="+"){#
  UseMethod("compdens_conv")#
}#
compdens_conv.default = function(m,x,s,v,FUN="+"){#
  stop(paste("Invalid class", class(m), "for first argument in",  match.call()))  #
}#
#
#compute density of mixture m convoluted with normal of sd (s) or student t with df v#
#at locations x#
#m is a mixture#
#x is an n vector#
#s is an n vector or integer#
dens_conv = function(m,x,s,v,FUN="+"){#
  UseMethod("dens_conv")#
}#
dens_conv.default = function(m,x,s,v,FUN="+"){#
  colSums(m$pi * compdens_conv(m,x,s,v,FUN))#
}#
#
#compute the posterior prob that each observation#
#came from each component of the mixture m#
#output a k by n vector of probabilities#
#computed by weighting the component densities by pi#
#and then normalizing#
comppostprob=function(m,x,s,v){#
 UseMethod("comppostprob") #
}#
comppostprob.default = function(m,x,s,v){#
  tmp= (t(m$pi * compdens_conv(m,x,s,v))/dens_conv(m,x,s,v))#
  ismissing = (is.na(x) | is.na(s))#
  tmp[ismissing,]=m$pi#
  t(tmp)#
}#
# evaluate cdf of posterior distribution of beta at c#
# m is the prior on beta, a mixture#
# c is location of evaluation#
# assumption is betahat | beta \sim N(beta,sebetahat)#
# m is a mixture with k components#
# c a scalar#
# betahat, sebetahat are n vectors #
# output is a k by n matrix#
compcdf_post=function(m,c,betahat,sebetahat,v){#
  UseMethod("compcdf_post")#
}#
compcdf_post.default=function(m,c,betahat,sebetahat,v){#
  stop("method compcdf_post not written for this class")#
}#
cdf_post = function(m,c,betahat,sebetahat,v){#
  UseMethod("cdf_post")#
}#
cdf_post.default=function(m,c,betahat,sebetahat,v){#
  colSums(comppostprob(m,betahat,sebetahat,v)*compcdf_post(m,c,betahat,sebetahat,v))#
}#
#
#output posterior mean for beta for prior mixture m,#
#given observations betahat, sebetahat, df v#
postmean = function(m, betahat,sebetahat,v){#
  UseMethod("postmean")#
}#
postmean.default = function(m,betahat,sebetahat,v){#
  colSums(comppostprob(m,betahat,sebetahat,v) * comp_postmean(m,betahat,sebetahat,v))#
}#
#output posterior mean-squared value for beta for prior mixture m,#
#given observations betahat, sebetahat, df v#
postmean2 = function(m, betahat,sebetahat,v){#
  UseMethod("postmean2")#
}#
postmean2.default = function(m,betahat,sebetahat,v){#
  colSums(comppostprob(m,betahat,sebetahat,v) * comp_postmean2(m,betahat,sebetahat,v))#
}#
#
#output posterior sd for beta for prior mixture m,#
#given observations betahat, sebetahat, df v#
postsd = function(m,betahat,sebetahat,v){#
  UseMethod("postsd")#
}#
postsd.default = function(m,betahat,sebetahat,v){#
  sqrt(postmean2(m,betahat,sebetahat,v)-postmean(m,betahat,sebetahat,v)^2)#
}#
#
#output posterior mean-squared value for beta for prior mixture m,#
#given observations betahat, sebetahat, df v#
comp_postmean2 = function(m,betahat,sebetahat,v){#
  UseMethod("comp_postmean2")#
}#
comp_postmean2.default = function(m,betahat,sebetahat,v){#
  comp_postsd(m,betahat,sebetahat,v)^2 + comp_postmean(m,betahat,sebetahat,v)^2#
}#
#output posterior mean for beta for each component of prior mixture m,#
#given observations betahat, sebetahat, df v#
comp_postmean = function(m, betahat,sebetahat,v){#
  UseMethod("comp_postmean")#
}#
comp_postmean.default = function(m,betahat,sebetahat,v){#
  stop("method comp_postmean not written for this class")#
}#
#output posterior sd for beta for each component of prior mixture m,#
#given observations betahat, sebetahat, df v#
comp_postsd = function(m, betahat,sebetahat,v){#
  UseMethod("comp_postsd")#
}#
comp_postsd.default = function(m,betahat,sebetahat,v){#
  stop("method comp_postsd not written for this class")#
}#
#
#find nice limits of mixture m for plotting#
min_lim = function(m){#
  UseMethod("min_lim")#
}#
min_lim.default=function(m){#
  -5#
}#
#
max_lim = function(m){#
  UseMethod("max_lim")#
}#
max_lim.default=function(m){#
  5#
}#
#plot density of mixture#
plot_dens = function(m,npoints=100,...){#
  UseMethod("plot_dens")#
}#
plot_dens.default = function(m,npoints=100,...){#
  x = seq(min_lim(m),max_lim(m),length=npoints)#
  plot(x,dens(m,x),type="l",xlab="density",ylab="x",...)#
}#
#
plot_post_cdf = function(m,betahat,sebetahat,v,npoints=100,...){#
  UseMethod("plot_post_cdf")#
}#
plot_post_cdf.default = function(m,betahat,sebetahat,v,npoints=100,...){#
  x = seq(min_lim(m),max_lim(m),length=npoints)#
  x_cdf = vapply(x,cdf_post,FUN.VALUE=betahat,m=m,betahat=betahat,sebetahat=sebetahat,v=v)#
  plot(x,x_cdf,type="l",xlab="x",ylab="cdf",...)#
 # for(i in 2:nrow(x_cdf)){#
 #   lines(x,x_cdf[i,],col=i)#
 # }#
}#
#
############################### METHODS FOR normalmix class ############################
#
#' @title Constructor for normalmix class#
#'#
#' @description Creates an object of class normalmix (finite mixture of univariate normals)#
#' #
#' @details None#
#' #
#' @param pi vector of mixture proportions#
#' @param mean vector of means#
#' @param sd: vector of standard deviations#
#' #
#' @return an object of class normalmix#
#' #
#' @export#
#' #
#' @examples normalmix(c(0.5,0.5),c(0,0),c(1,2))#
#' #
normalmix = function(pi,mean,sd){#
  structure(data.frame(pi,mean,sd),class="normalmix")#
}#
#
comp_sd.normalmix = function(m){#
  m$sd#
}#
#
comp_mean.normalmix = function(m){#
  m$mean#
}#
#
compdens.normalmix = function(x,y,log=FALSE){#
  k=ncomp(x)#
  n=length(y)#
  d = matrix(rep(y,rep(k,n)),nrow=k)#
  return(matrix(dnorm(d, x$mean, x$sd, log),nrow=k))  #
}#
#
#density of convolution of each component of a normal mixture with N(0,s^2) or s*t(v) at x#
# x an n-vector at which density is to be evaluated#
#return a k by n matrix#
#Note that convolution of two normals is normal, so it works that way#
compdens_conv.normalmix = function(m,x,s,v,FUN="+"){#
  if(!is.null(v)){#
  	stop("method comp_postsd of normal mixture not written for df!null")#
  }#
  if(length(s)==1){s=rep(s,length(x))}#
  sdmat = sqrt(outer(s^2,m$sd^2,FUN)) #n by k matrix of standard deviations of convolutions#
  return(t(dnorm(outer(x,m$mean,FUN="-")/sdmat)/sdmat))#
}#
comp_cdf.normalmix = function(x,y,lower.tail=TRUE){#
  vapply(y,pnorm,x$mean,x$mean,x$sd,lower.tail)#
}#
#
#c is a scalar#
#m a mixture with k components#
#betahat a vector of n observations#
#sebetahat an n vector of standard errors#
#return a k by n matrix of the posterior cdf#
compcdf_post.normalmix=function(m,c,betahat,sebetahat,v){#
  if(!is.null(v)){#
  	stop("Error: normal mixture for student-t likelihood is not yet implemented")#
  }  #
  k = length(m$pi)#
  n=length(betahat)#
  #compute posterior standard deviation (s1) and posterior mean (m1)#
  s1 = sqrt(outer(sebetahat^2,m$sd^2,FUN="*")/outer(sebetahat^2,m$sd^2,FUN="+"))#
  ismissing = (is.na(betahat) | is.na(sebetahat))#
  s1[ismissing,]=m$sd#
  m1 = t(comp_postmean(m,betahat,sebetahat,v))#
  t(pnorm(c,mean=m1,sd=s1))#
}#
#
#return posterior mean for each component of prior m, given observations betahat and sebetahat#
#input, m is a mixture with k components#
#betahat, sebetahat are n vectors#
#output is a k by n matrix#
comp_postmean.normalmix = function(m,betahat,sebetahat,v){#
  if(!isnull(v)){#
  	stop("method comp_postmean of normal mixture not written for df!=NULL")#
  }#
  tmp=(outer(sebetahat^2,m$mean, FUN="*") + outer(betahat,m$sd^2, FUN="*"))/outer(sebetahat^2,m$sd^2,FUN="+")#
  ismissing = (is.na(betahat) | is.na(sebetahat))#
  tmp[ismissing,]=m$mean #return prior mean when missing data#
  t(tmp)#
}#
#return posterior standard deviation for each component of prior m, given observations betahat and sebetahat#
#input, m is a mixture with k components#
#betahat, sebetahat are n vectors#
#output is a k by n matrix#
comp_postsd.normalmix = function(m,betahat,sebetahat,v){#
  if(!is.null(v)){#
  	stop("method comp_postsd of normal mixture not written for df!=NULL")#
  }#
  t(sqrt(outer(sebetahat^2,m$sd^2,FUN="*")/outer(sebetahat^2,m$sd^2,FUN="+")))#
}#
############################### METHODS FOR unimix class ############################
#
#constructor; pi, a and b are vectors; kth component is Uniform(a[k],b[k])#
unimix = function(pi,a,b){#
  structure(data.frame(pi,a,b),class="unimix")#
}#
#
comp_cdf.unimix = function(m,y,lower.tail=TRUE){#
  vapply(y,punif,m$a,min=m$a,max=m$b,lower.tail)#
}#
#
comp_sd.unimix = function(m){#
  (m$b-m$a)/sqrt(12)#
}#
#
comp_mean.unimix = function(m){#
  (m$a+m$b)/2#
}#
compdens.unimix = function(x,y,log=FALSE){#
  k=ncomp(x)#
  n=length(y)#
  d = matrix(rep(y,rep(k,n)),nrow=k)#
  return(matrix(dunif(d, x$a, x$b, log),nrow=k))  #
}#
#
#density of convolution of each component of a unif mixture with N(0,s) at x#
# x an n-vector#
#return a k by n matrix#
compdens_conv.unimix = function(m,x,s,v, FUN="+"){#
  if(FUN!="+") stop("Error; compdens_conv not implemented for uniform with FUN!=+")#
  if(is.null(v)){#
    compdens= t(pnorm(outer(x,m$a,FUN="-")/s)-pnorm(outer(x,m$b,FUN="-")/s))/(m$b-m$a)#
    compdens[m$a==m$b,]=t(dnorm(outer(x,m$a,FUN="-")/s)/s)[m$a==m$b,]#
  }#
  else{#
    compdens= t(pt(outer(x,m$a,FUN="-")/s,df=v)-pt(outer(x,m$b,FUN="-")/s,df=v))/(m$b-m$a)#
    compdens[m$a==m$b,]=t(dt(outer(x,m$a,FUN="-")/s,df=v)/s)[m$a==m$b,]#
  }#
  return(compdens)#
}#
#c is a scalar#
#m a mixture with k components#
#betahat a vector of n observations#
#sebetahat an n vector of standard errors#
#return a k by n matrix of the posterior cdf#
compcdf_post.unimix=function(m,c,betahat,sebetahat,v){#
  k = length(m$pi)#
  n=length(betahat)#
  tmp = matrix(1,nrow=k,ncol=n)#
  tmp[m$a > c,] = 0#
  subset = m$a<=c & m$b>c # subset of components (1..k) with nontrivial cdf#
  if(sum(subset)>0){#
  	if(is.null(v)){#
      pna = pnorm(outer(betahat,m$a[subset],FUN="-")/sebetahat)#
      pnc = pnorm(outer(betahat,rep(c,sum(subset)),FUN="-")/sebetahat)#
      pnb = pnorm(outer(betahat,m$b[subset],FUN="-")/sebetahat)#
    }#
    else{#
      pna = pt(outer(betahat,m$a[subset],FUN="-")/sebetahat, df=v)#
      pnc = pt(outer(betahat,rep(c,sum(subset)),FUN="-")/sebetahat, df=v)#
      pnb = pt(outer(betahat,m$b[subset],FUN="-")/sebetahat, df=v)#
    }#
    tmp[subset,] = t((pnc-pna)/(pnb-pna))#
  }#
  subset = (m$a == m$b) #subset of components with trivial cdf#
  tmp[subset,]= rep(m$a[subset] <= c,n)#
  tmp#
}#
#
my_etruncnorm= function(a,b,mean=0,sd=1){#
  alpha = (a-mean)/sd#
  beta =  (b-mean)/sd#
 #Flip the onese where both are positive, as the computations are more stable#
  #when both negative#
  flip = (alpha>0 & beta>0)#
  flip[is.na(flip)]=FALSE #deal with NAs#
  alpha[flip]= -alpha[flip]#
  beta[flip]=-beta[flip]#
  tmp= (-1)^flip * (mean+sd*etruncnorm(alpha,beta,0,1))#
  max_alphabeta = ifelse(alpha<beta, beta,alpha)#
  max_ab = ifelse(alpha<beta,b,a)#
  toobig = max_alphabeta<(-30)#
  toobig[is.na(toobig)]=FALSE #
  tmp[toobig] = max_ab[toobig]#
  tmp#
}#
#return posterior mean for each component of prior m, given observations betahat and sebetahat#
#input, m is a mixture with k components#
#betahat, sebetahat are n vectors#
#output is a k by n matrix#
#note that with uniform prior, posterior is truncated normal, so#
#this is computed using formula for mean of truncated normal #
comp_postmean.unimix = function(m,betahat,sebetahat,v){#
#   k= ncomp(m)#
#   n=length(betahat)#
#   a = matrix(m$a,nrow=n,ncol=k,byrow=TRUE)#
#   b = matrix(m$b,nrow=n,ncol=k,byrow=TRUE)#
#   matrix(etruncnorm(a,b,betahat,sebetahat),nrow=k,byrow=TRUE)#
  #note: etruncnorm is more stable for a and b negative than positive#
  #so maybe use this, and standardize to make the whole more stable.#
  alpha = outer(-betahat, m$a,FUN="+")/sebetahat#
  beta = outer(-betahat, m$b, FUN="+")/sebetahat#
  if(is.null(v)){#
    tmp = betahat + sebetahat*my_etruncnorm(alpha,beta,0,1)#
  }#
  else{#
  	tmp = betahat + sebetahat*my_etrunct(alpha,beta,v)#
  }#
  ismissing = is.na(betahat) | is.na(sebetahat)#
  tmp[ismissing,]= (m$a+m$b)/2#
  t(tmp)#
#   t(#
#     betahat + sebetahat* #
#       exp(dnorm(alpha,log=TRUE)- pnorm(alpha,log=TRUE))#
#    * #
#       (-expm1(dnorm(beta,log=TRUE)-dnorm(alpha,log=TRUE)))#
#     /#
#       (expm1(pnorm(beta,log=TRUE)-pnorm(alpha,log=TRUE)))#
#   )#
}#
#
#not yet implemented!#
#just returns 0s for now#
comp_postsd.unimix = function(m,betahat,sebetahat,v){#
  print("Warning: Posterior SDs not yet implemented for uniform components")#
  k= ncomp(m)#
  n=length(betahat)#
  return(matrix(NA,nrow=k,ncol=n)) #
}#
#
# the mean of a truncated student.t#
# the result is from the paper 'Moments of truncated Student-t distribution' by H.-J Kim #
#
my_etrunct= function(a,b,v){#
  A = v+a^2#
  B = v+b^2#
  F_a = pt(a,df=v)#
  F_b = pt(b,df=v)#
  G = gamma((v-1)/2)*v^(v/2)/(2*(F_b-F_a)*gamma(v/2)*gamma(1/2))#
  tmp = ifelse(a==b,a,G*(A^(-(v-1)/2)-B^(-(v-1)/2)))#
  tmp#
}
simdata = function(n, betahatsd,nullproportion=1, altmean=1, altsd=1) {#
    nnull=round(n*nullproportion,0)#
    null = c(rep(1, nnull), rep(0, n - nnull))#
    beta = c(rep(0, nnull), rnorm(n - nnull, altmean, altsd))#
    betahat = rnorm(n, beta, betahatsd)#
    return(list(null = null, beta = beta, betahat = betahat, betahatsd = betahatsd))#
  }#
  set.seed(200)#
test=simdata(20,betahatsd=1,nullproportion=0.5,altmean=2,altsd=1)#
print(test$beta)#
test.ash=ash(test$betahat,test$betahatsd,method="shrink",nonzeromean=FALSE)
library(knitr)#
library(devtools)#
library(roxygen2)#
library(testthat)#
library(SQUAREM)#
library(truncnorm)
test.ash=ash(test$betahat,test$betahatsd,method="shrink",nonzeromean=FALSE)
GENERIC FUNCTIONS #############################
# find matrix of densities at y, for each component of the mixture#
# INPUT y is an n-vector#
# OUTPUT k by n matrix of densities#
compdens = function(x,y,log=FALSE){#
  UseMethod("compdens")#
}#
compdens.default = function(x,y,log=FALSE){#
  stop(paste("Invalid class", class(m), "for first argument in",  match.call()))  #
}#
#
#standard deviations#
comp_sd = function(m){#
  UseMethod("comp_sd")#
}#
comp_sd.default = function(m){#
  stop("method comp_sd not written for this class")#
}#
#
#second moments#
comp_mean2 = function(m){#
  UseMethod("comp_mean2")#
}#
comp_mean2.default = function(m){#
  comp_sd(m)^2 + comp_mean(m)^2#
}#
#return the overall mean of the mixture#
mixmean = function(m){#
  UseMethod("mixmean")#
}#
mixmean.default = function(m){#
  sum(m$pi * comp_mean(m))#
}#
#
#return the overall second moment of the mixture#
mixmean2 = function(m){#
  UseMethod("mixmean2")#
}#
mixmean2.default = function(m){#
  sum(m$pi * comp_mean2(m))#
}#
#
#return the overall sd of the mixture#
mixsd = function(m){#
  UseMethod("mixsd")#
}#
mixsd.default = function(m){#
  sqrt(mixmean2(m)-mixmean(m)^2)#
}#
#
#means#
comp_mean = function(m){#
  UseMethod("comp_mean")#
}#
comp_mean.default = function(m){#
  stop("method comp_mean not written for this class")#
}#
#
#number of components#
ncomp = function(m){#
  UseMethod("ncomp")#
}#
ncomp.default = function(m){#
  return(length(m$pi))#
}#
#
#return mixture proportions, a generic function#
mixprop = function(m){#
  UseMethod("mixprop")#
}#
mixprop.default = function(m){#
  m$pi#
}#
#
#' @title mixcdf#
#'#
#' @description Returns cdf for a mixture (generic function)#
#' #
#' @details None#
#' #
#' @param x a mixture (eg of type normalmix or unimix)#
#' @param y locations at which cdf to be computed#
#' @param lower.tail: boolean indicating whether to report lower tail#
#' #
#' @return an object of class normalmix#
#' #
#' @export#
#' #
#' @examples mixcdf(normalmix(c(0.5,0.5),c(0,0),c(1,2)),seq(-4,4,length=100))#
#' #
mixcdf = function(x,y,lower.tail=TRUE){#
  UseMethod("mixcdf")#
}#
#' @title mixcdf.default#
#' @export#
#' #
mixcdf.default = function(x,y,lower.tail=TRUE){#
  x$pi %*% comp_cdf(x,y,lower.tail)#
}#
#
#find cdf for each component, a generic function#
comp_cdf = function(x,y,lower.tail=TRUE){#
  UseMethod("comp_cdf")#
}#
comp_cdf.default = function(x,y,lower.tail=TRUE){#
  stop("comp_cdf not implemented for this class")#
}#
#find density at y, a generic function#
dens = function(x,y){#
  UseMethod("dens")#
}#
dens.default = function(x,y){#
  return (x$pi %*% compdens(x, y))#
}#
#
#find log likelihood of data in x (a vector) for mixture in m#
loglik = function(m,x){#
  UseMethod("loglik")#
}#
loglik.default = function(m,x){#
  sum(log(dens(m,x)))#
}#
#
#find log likelihood of data in betahat, when #
#the mixture m is convolved with a normal with sd betahatsd#
#betahatsd is an n vector#
#betahat is an n vector#
#v is the degree of freedom#
#' @title loglik_conv#
#' #
#' @export#
#' #
loglik_conv = function(m,betahat,betahatsd,v,FUN="+"){#
  UseMethod("loglik_conv")#
}#
#' @title loglik_conv.default#
#' #
#' @export#
#' #
loglik_conv.default = function(m,betahat,betahatsd,v,FUN="+"){#
  sum(log(dens_conv(m,betahat,betahatsd,v,FUN)))#
}#
#
#compute the density of the components of the mixture m#
#when convoluted with a normal with standard deviation s#
#or a scaled (se) student.t with df v#
#the density is evaluated at x#
#x and s are n-vectors#
#m is a mixture with k components#
#output is a k by n matrix of densities#
compdens_conv = function(m,x,s,v,FUN="+"){#
  UseMethod("compdens_conv")#
}#
compdens_conv.default = function(m,x,s,v,FUN="+"){#
  stop(paste("Invalid class", class(m), "for first argument in",  match.call()))  #
}#
#
#compute density of mixture m convoluted with normal of sd (s) or student t with df v#
#at locations x#
#m is a mixture#
#x is an n vector#
#s is an n vector or integer#
dens_conv = function(m,x,s,v,FUN="+"){#
  UseMethod("dens_conv")#
}#
dens_conv.default = function(m,x,s,v,FUN="+"){#
  colSums(m$pi * compdens_conv(m,x,s,v,FUN))#
}#
#
#compute the posterior prob that each observation#
#came from each component of the mixture m#
#output a k by n vector of probabilities#
#computed by weighting the component densities by pi#
#and then normalizing#
comppostprob=function(m,x,s,v){#
 UseMethod("comppostprob") #
}#
comppostprob.default = function(m,x,s,v){#
  tmp= (t(m$pi * compdens_conv(m,x,s,v))/dens_conv(m,x,s,v))#
  ismissing = (is.na(x) | is.na(s))#
  tmp[ismissing,]=m$pi#
  t(tmp)#
}#
# evaluate cdf of posterior distribution of beta at c#
# m is the prior on beta, a mixture#
# c is location of evaluation#
# assumption is betahat | beta \sim N(beta,sebetahat)#
# m is a mixture with k components#
# c a scalar#
# betahat, sebetahat are n vectors #
# output is a k by n matrix#
compcdf_post=function(m,c,betahat,sebetahat,v){#
  UseMethod("compcdf_post")#
}#
compcdf_post.default=function(m,c,betahat,sebetahat,v){#
  stop("method compcdf_post not written for this class")#
}#
cdf_post = function(m,c,betahat,sebetahat,v){#
  UseMethod("cdf_post")#
}#
cdf_post.default=function(m,c,betahat,sebetahat,v){#
  colSums(comppostprob(m,betahat,sebetahat,v)*compcdf_post(m,c,betahat,sebetahat,v))#
}#
#
#output posterior mean for beta for prior mixture m,#
#given observations betahat, sebetahat, df v#
postmean = function(m, betahat,sebetahat,v){#
  UseMethod("postmean")#
}#
postmean.default = function(m,betahat,sebetahat,v){#
  colSums(comppostprob(m,betahat,sebetahat,v) * comp_postmean(m,betahat,sebetahat,v))#
}#
#output posterior mean-squared value for beta for prior mixture m,#
#given observations betahat, sebetahat, df v#
postmean2 = function(m, betahat,sebetahat,v){#
  UseMethod("postmean2")#
}#
postmean2.default = function(m,betahat,sebetahat,v){#
  colSums(comppostprob(m,betahat,sebetahat,v) * comp_postmean2(m,betahat,sebetahat,v))#
}#
#
#output posterior sd for beta for prior mixture m,#
#given observations betahat, sebetahat, df v#
postsd = function(m,betahat,sebetahat,v){#
  UseMethod("postsd")#
}#
postsd.default = function(m,betahat,sebetahat,v){#
  sqrt(postmean2(m,betahat,sebetahat,v)-postmean(m,betahat,sebetahat,v)^2)#
}#
#
#output posterior mean-squared value for beta for prior mixture m,#
#given observations betahat, sebetahat, df v#
comp_postmean2 = function(m,betahat,sebetahat,v){#
  UseMethod("comp_postmean2")#
}#
comp_postmean2.default = function(m,betahat,sebetahat,v){#
  comp_postsd(m,betahat,sebetahat,v)^2 + comp_postmean(m,betahat,sebetahat,v)^2#
}#
#output posterior mean for beta for each component of prior mixture m,#
#given observations betahat, sebetahat, df v#
comp_postmean = function(m, betahat,sebetahat,v){#
  UseMethod("comp_postmean")#
}#
comp_postmean.default = function(m,betahat,sebetahat,v){#
  stop("method comp_postmean not written for this class")#
}#
#output posterior sd for beta for each component of prior mixture m,#
#given observations betahat, sebetahat, df v#
comp_postsd = function(m, betahat,sebetahat,v){#
  UseMethod("comp_postsd")#
}#
comp_postsd.default = function(m,betahat,sebetahat,v){#
  stop("method comp_postsd not written for this class")#
}#
#
#find nice limits of mixture m for plotting#
min_lim = function(m){#
  UseMethod("min_lim")#
}#
min_lim.default=function(m){#
  -5#
}#
#
max_lim = function(m){#
  UseMethod("max_lim")#
}#
max_lim.default=function(m){#
  5#
}#
#plot density of mixture#
plot_dens = function(m,npoints=100,...){#
  UseMethod("plot_dens")#
}#
plot_dens.default = function(m,npoints=100,...){#
  x = seq(min_lim(m),max_lim(m),length=npoints)#
  plot(x,dens(m,x),type="l",xlab="density",ylab="x",...)#
}#
#
plot_post_cdf = function(m,betahat,sebetahat,v,npoints=100,...){#
  UseMethod("plot_post_cdf")#
}#
plot_post_cdf.default = function(m,betahat,sebetahat,v,npoints=100,...){#
  x = seq(min_lim(m),max_lim(m),length=npoints)#
  x_cdf = vapply(x,cdf_post,FUN.VALUE=betahat,m=m,betahat=betahat,sebetahat=sebetahat,v=v)#
  plot(x,x_cdf,type="l",xlab="x",ylab="cdf",...)#
 # for(i in 2:nrow(x_cdf)){#
 #   lines(x,x_cdf[i,],col=i)#
 # }#
}#
#
############################### METHODS FOR normalmix class ############################
#
#' @title Constructor for normalmix class#
#'#
#' @description Creates an object of class normalmix (finite mixture of univariate normals)#
#' #
#' @details None#
#' #
#' @param pi vector of mixture proportions#
#' @param mean vector of means#
#' @param sd: vector of standard deviations#
#' #
#' @return an object of class normalmix#
#' #
#' @export#
#' #
#' @examples normalmix(c(0.5,0.5),c(0,0),c(1,2))#
#' #
normalmix = function(pi,mean,sd){#
  structure(data.frame(pi,mean,sd),class="normalmix")#
}#
#
comp_sd.normalmix = function(m){#
  m$sd#
}#
#
comp_mean.normalmix = function(m){#
  m$mean#
}#
#
compdens.normalmix = function(x,y,log=FALSE){#
  k=ncomp(x)#
  n=length(y)#
  d = matrix(rep(y,rep(k,n)),nrow=k)#
  return(matrix(dnorm(d, x$mean, x$sd, log),nrow=k))  #
}#
#
#density of convolution of each component of a normal mixture with N(0,s^2) or s*t(v) at x#
# x an n-vector at which density is to be evaluated#
#return a k by n matrix#
#Note that convolution of two normals is normal, so it works that way#
compdens_conv.normalmix = function(m,x,s,v,FUN="+"){#
  if(!is.null(v)){#
  	stop("method comp_postsd of normal mixture not written for df!null")#
  }#
  if(length(s)==1){s=rep(s,length(x))}#
  sdmat = sqrt(outer(s^2,m$sd^2,FUN)) #n by k matrix of standard deviations of convolutions#
  return(t(dnorm(outer(x,m$mean,FUN="-")/sdmat)/sdmat))#
}#
comp_cdf.normalmix = function(x,y,lower.tail=TRUE){#
  vapply(y,pnorm,x$mean,x$mean,x$sd,lower.tail)#
}#
#
#c is a scalar#
#m a mixture with k components#
#betahat a vector of n observations#
#sebetahat an n vector of standard errors#
#return a k by n matrix of the posterior cdf#
compcdf_post.normalmix=function(m,c,betahat,sebetahat,v){#
  if(!is.null(v)){#
  	stop("Error: normal mixture for student-t likelihood is not yet implemented")#
  }  #
  k = length(m$pi)#
  n=length(betahat)#
  #compute posterior standard deviation (s1) and posterior mean (m1)#
  s1 = sqrt(outer(sebetahat^2,m$sd^2,FUN="*")/outer(sebetahat^2,m$sd^2,FUN="+"))#
  ismissing = (is.na(betahat) | is.na(sebetahat))#
  s1[ismissing,]=m$sd#
  m1 = t(comp_postmean(m,betahat,sebetahat,v))#
  t(pnorm(c,mean=m1,sd=s1))#
}#
#
#return posterior mean for each component of prior m, given observations betahat and sebetahat#
#input, m is a mixture with k components#
#betahat, sebetahat are n vectors#
#output is a k by n matrix#
comp_postmean.normalmix = function(m,betahat,sebetahat,v){#
  if(!is.null(v)){#
  	stop("method comp_postmean of normal mixture not written for df!=NULL")#
  }#
  tmp=(outer(sebetahat^2,m$mean, FUN="*") + outer(betahat,m$sd^2, FUN="*"))/outer(sebetahat^2,m$sd^2,FUN="+")#
  ismissing = (is.na(betahat) | is.na(sebetahat))#
  tmp[ismissing,]=m$mean #return prior mean when missing data#
  t(tmp)#
}#
#return posterior standard deviation for each component of prior m, given observations betahat and sebetahat#
#input, m is a mixture with k components#
#betahat, sebetahat are n vectors#
#output is a k by n matrix#
comp_postsd.normalmix = function(m,betahat,sebetahat,v){#
  if(!is.null(v)){#
  	stop("method comp_postsd of normal mixture not written for df!=NULL")#
  }#
  t(sqrt(outer(sebetahat^2,m$sd^2,FUN="*")/outer(sebetahat^2,m$sd^2,FUN="+")))#
}#
############################### METHODS FOR unimix class ############################
#
#constructor; pi, a and b are vectors; kth component is Uniform(a[k],b[k])#
unimix = function(pi,a,b){#
  structure(data.frame(pi,a,b),class="unimix")#
}#
#
comp_cdf.unimix = function(m,y,lower.tail=TRUE){#
  vapply(y,punif,m$a,min=m$a,max=m$b,lower.tail)#
}#
#
comp_sd.unimix = function(m){#
  (m$b-m$a)/sqrt(12)#
}#
#
comp_mean.unimix = function(m){#
  (m$a+m$b)/2#
}#
compdens.unimix = function(x,y,log=FALSE){#
  k=ncomp(x)#
  n=length(y)#
  d = matrix(rep(y,rep(k,n)),nrow=k)#
  return(matrix(dunif(d, x$a, x$b, log),nrow=k))  #
}#
#
#density of convolution of each component of a unif mixture with N(0,s) at x#
# x an n-vector#
#return a k by n matrix#
compdens_conv.unimix = function(m,x,s,v, FUN="+"){#
  if(FUN!="+") stop("Error; compdens_conv not implemented for uniform with FUN!=+")#
  if(is.null(v)){#
    compdens= t(pnorm(outer(x,m$a,FUN="-")/s)-pnorm(outer(x,m$b,FUN="-")/s))/(m$b-m$a)#
    compdens[m$a==m$b,]=t(dnorm(outer(x,m$a,FUN="-")/s)/s)[m$a==m$b,]#
  }#
  else{#
    compdens= t(pt(outer(x,m$a,FUN="-")/s,df=v)-pt(outer(x,m$b,FUN="-")/s,df=v))/(m$b-m$a)#
    compdens[m$a==m$b,]=t(dt(outer(x,m$a,FUN="-")/s,df=v)/s)[m$a==m$b,]#
  }#
  return(compdens)#
}#
#c is a scalar#
#m a mixture with k components#
#betahat a vector of n observations#
#sebetahat an n vector of standard errors#
#return a k by n matrix of the posterior cdf#
compcdf_post.unimix=function(m,c,betahat,sebetahat,v){#
  k = length(m$pi)#
  n=length(betahat)#
  tmp = matrix(1,nrow=k,ncol=n)#
  tmp[m$a > c,] = 0#
  subset = m$a<=c & m$b>c # subset of components (1..k) with nontrivial cdf#
  if(sum(subset)>0){#
  	if(is.null(v)){#
      pna = pnorm(outer(betahat,m$a[subset],FUN="-")/sebetahat)#
      pnc = pnorm(outer(betahat,rep(c,sum(subset)),FUN="-")/sebetahat)#
      pnb = pnorm(outer(betahat,m$b[subset],FUN="-")/sebetahat)#
    }#
    else{#
      pna = pt(outer(betahat,m$a[subset],FUN="-")/sebetahat, df=v)#
      pnc = pt(outer(betahat,rep(c,sum(subset)),FUN="-")/sebetahat, df=v)#
      pnb = pt(outer(betahat,m$b[subset],FUN="-")/sebetahat, df=v)#
    }#
    tmp[subset,] = t((pnc-pna)/(pnb-pna))#
  }#
  subset = (m$a == m$b) #subset of components with trivial cdf#
  tmp[subset,]= rep(m$a[subset] <= c,n)#
  tmp#
}#
#
my_etruncnorm= function(a,b,mean=0,sd=1){#
  alpha = (a-mean)/sd#
  beta =  (b-mean)/sd#
 #Flip the onese where both are positive, as the computations are more stable#
  #when both negative#
  flip = (alpha>0 & beta>0)#
  flip[is.na(flip)]=FALSE #deal with NAs#
  alpha[flip]= -alpha[flip]#
  beta[flip]=-beta[flip]#
  tmp= (-1)^flip * (mean+sd*etruncnorm(alpha,beta,0,1))#
  max_alphabeta = ifelse(alpha<beta, beta,alpha)#
  max_ab = ifelse(alpha<beta,b,a)#
  toobig = max_alphabeta<(-30)#
  toobig[is.na(toobig)]=FALSE #
  tmp[toobig] = max_ab[toobig]#
  tmp#
}#
#return posterior mean for each component of prior m, given observations betahat and sebetahat#
#input, m is a mixture with k components#
#betahat, sebetahat are n vectors#
#output is a k by n matrix#
#note that with uniform prior, posterior is truncated normal, so#
#this is computed using formula for mean of truncated normal #
comp_postmean.unimix = function(m,betahat,sebetahat,v){#
#   k= ncomp(m)#
#   n=length(betahat)#
#   a = matrix(m$a,nrow=n,ncol=k,byrow=TRUE)#
#   b = matrix(m$b,nrow=n,ncol=k,byrow=TRUE)#
#   matrix(etruncnorm(a,b,betahat,sebetahat),nrow=k,byrow=TRUE)#
  #note: etruncnorm is more stable for a and b negative than positive#
  #so maybe use this, and standardize to make the whole more stable.#
  alpha = outer(-betahat, m$a,FUN="+")/sebetahat#
  beta = outer(-betahat, m$b, FUN="+")/sebetahat#
  if(is.null(v)){#
    tmp = betahat + sebetahat*my_etruncnorm(alpha,beta,0,1)#
  }#
  else{#
  	tmp = betahat + sebetahat*my_etrunct(alpha,beta,v)#
  }#
  ismissing = is.na(betahat) | is.na(sebetahat)#
  tmp[ismissing,]= (m$a+m$b)/2#
  t(tmp)#
#   t(#
#     betahat + sebetahat* #
#       exp(dnorm(alpha,log=TRUE)- pnorm(alpha,log=TRUE))#
#    * #
#       (-expm1(dnorm(beta,log=TRUE)-dnorm(alpha,log=TRUE)))#
#     /#
#       (expm1(pnorm(beta,log=TRUE)-pnorm(alpha,log=TRUE)))#
#   )#
}#
#
#not yet implemented!#
#just returns 0s for now#
comp_postsd.unimix = function(m,betahat,sebetahat,v){#
  print("Warning: Posterior SDs not yet implemented for uniform components")#
  k= ncomp(m)#
  n=length(betahat)#
  return(matrix(NA,nrow=k,ncol=n)) #
}#
#
# the mean of a truncated student.t#
# the result is from the paper 'Moments of truncated Student-t distribution' by H.-J Kim #
#
my_etrunct= function(a,b,v){#
  A = v+a^2#
  B = v+b^2#
  F_a = pt(a,df=v)#
  F_b = pt(b,df=v)#
  G = gamma((v-1)/2)*v^(v/2)/(2*(F_b-F_a)*gamma(v/2)*gamma(1/2))#
  tmp = ifelse(a==b,a,G*(A^(-(v-1)/2)-B^(-(v-1)/2)))#
  tmp#
}
test.ash=ash(test$betahat,test$betahatsd,method="shrink",nonzeromean=FALSE)
test.ash
test.ash=ash(test$betahat,test$betahatsd,method="shrink",nonzeromean=TRUE)
test.ash
beta
betahat
test
test=simdata(20,betahatsd=1,nullproportion=0.5,altmean=2,altsd=1)#
test=simdata(100,betahatsd=1,nullproportion=0.5,altmean=2,altsd=1)
' @useDynLib ashr#
#todo#
##
#' @title Main Adaptive SHrinkage function#
#'#
#' @description Takes vectors of estimates (betahat) and their standard errors (sebetahat), and applies#
#' shrinkage to them, using Empirical Bayes methods, to compute shrunk estimates for beta.#
#'#
#' @details See readme for more details#
#' #
#' @param betahat, a p vector of estimates #
#' @param sebetahat, a p vector of corresponding standard errors#
#' @param method: specifies how ash is to be run. Can be "shrinkage" (if main aim is shrinkage) or "fdr" (if main aim is to assess fdr or fsr)#
#' This is simply a convenient way to specify certain combinations of parameters: "shrinkage" sets pointmass=FALSE and prior="uniform";#
#' "fdr" sets pointmass=TRUE and prior="nullbiased".#
#' @param mixcompdist: distribution of components in mixture ("normal", "uniform" or "halfuniform")#
#'#
#' @param lambda1: multiplicative "inflation factor" for standard errors (like Genomic Control)#
#' @param lambda2: additive "inflation factor" for standard errors (like Genomic Control)#
#' @param nullcheck: whether to check that any fitted model exceeds the "null" likelihood#
#' in which all weight is on the first component#
#' @param df: appropriate degrees of freedom for (t) distribution of betahat/sebetahat#
#' @param randomstart: bool, indicating whether to initialize EM randomly. If FALSE, then initializes to prior mean (for EM algorithm) or prior (for VBEM)#
#' @param nullweight: scalar, the weight put on the prior of null under "fdr" method#
#' @param nonzeromean: bool, indicating whether to use a nonzero mean unimodal mixture(defaults to "FALSE")#
#' @param pointmass: bool, indicating whether to use a point mass at zero as one of components for a mixture distribution#
#' @param onlylogLR: bool, indicating whether to use this function to get logLR. Skip posterior prob, posterior mean, lfdr...#
#' @param prior: string, or numeric vector indicating Dirichlet prior on mixture proportions (defaults to "uniform", or 1,1...,1; also can be "nullbiased" 1,1/k-1,...,1/k-1 to put more weight on first component)#
#' @param mixsd: vector of sds for underlying mixture components #
#' @param VB: whether to use Variational Bayes to estimate mixture proportions (instead of EM to find MAP estimate)#
#' @param gridmult: the multiplier by which the default grid values for mixsd differ by one another. (Smaller values produce finer grids)#
#' @param minimal_output: if TRUE, just outputs the fitted g and the lfsr (useful for very big data sets where memory is an issue) #
#' @param g: the prior distribution for beta (usually estimated from the data; this is used primarily in simulated data to do computations with the "true" g)#
#' @param maxiter: maximum number of iterations of the EM algorithm#
#' @param cxx: flag to indicate whether to use the c++ (Rcpp) version#
#' #
#'#
#' @return a list with elements fitted.g is fitted mixture#
#' logLR : logP(D|mle(pi)) - logP(D|null)#
#' #
#' @export#
#' #
#' @examples #
#' beta = c(rep(0,100),rnorm(100))#
#' sebetahat = abs(rnorm(200,0,1))#
#' betahat = rnorm(200,beta,sebetahat)#
#' beta.ash = ash(betahat, sebetahat)#
#' summary(beta.ash)#
#' plot(betahat,beta.ash$PosteriorMean,xlim=c(-4,4),ylim=c(-4,4))#
#' #
#' #
#Things to do:#
# check sampling routine#
# check number of iterations#
ash = function(betahat,sebetahat,method = c("shrink","fdr"), #
               mixcompdist = c("normal","uniform","halfuniform"),#
               lambda1=1,lambda2=0,nullcheck=TRUE,df=NULL,randomstart=FALSE,#
               nullweight=10,nonzeromean=FALSE, #
               pointmass = FALSE, #
               onlylogLR = FALSE, #
               prior=c("uniform","nullbiased"), #
               mixsd=NULL, VB=FALSE,gridmult=sqrt(2),#
               minimaloutput=FALSE,#
               multiseqoutput=FALSE,#
               g=NULL,#
               maxiter = 5000,#
               cxx=FALSE){#
  #method provides a convenient interface to set a particular combinations of parameters for prior an#
  #If method is supplied, use it to set up specific values for these parameters; provide warning if values#
  #are also specified by user#
  #If method is not supplied use the user-supplied values (or defaults if user does not specify them)#
  if(!missing(method)){#
    method = match.arg(method) #
    if(method=="shrink"){#
      if(missing(prior)){#
        prior = "uniform"#
      } else {#
        warning("Specification of prior overrides default for method shrink")#
      }#
      if(missing(pointmass)){#
        pointmass=FALSE#
      } else {#
        warning("Specification of pointmass overrides default for method shrink")#
      }#
    }#
    if(method=="fdr"){#
      if(missing(prior)){#
        prior = "nullbiased"#
      } else {#
        warning("Specification of prior overrides default for method fdr")#
      }#
      if(missing(pointmass)){#
        pointmass=TRUE#
      } else {#
        warning("Specification of pointmass overrides default for method fdr")#
      }#
    }  #
  }#
  if(gridmult<=1&multiseqoutput!=TRUE)#
    stop("gridmult must be > 1")#
  mixcompdist = match.arg(mixcompdist)#
  # if(mixcompdist=="uniform" & pointmass==TRUE){#
  #    stop("point mass not yet implemented for uniform or half-uniform")#
  #  }#
  #  if(mixcompdist=="halfuniform" & pointmass==TRUE){#
  #    stop("point mass not yet implemented for uniform or half-uniform")#
  #  }#
  if(!is.numeric(prior)){#
    prior = match.arg(prior)#
  }  #
  if(length(sebetahat)==1){#
    sebetahat = rep(sebetahat,length(betahat))#
  }#
  if(length(sebetahat) != length(betahat)){#
    stop("Error: sebetahat must have length 1, or same length as betahat")#
  }#
  completeobs = (!is.na(betahat) & !is.na(sebetahat))#
  n=sum(completeobs)#
  if(n==0){#
    if(onlylogLR){#
      return(list(pi=NULL, logLR = 0))#
    }#
    else{#
      stop("Error: all input values are missing")#
    }#
  }  #
  if(!is.null(g)){#
    maxiter = 1 # if g is specified, don't iterate the EM#
    prior = rep(1,ncomp(g)) #prior is not actually used if g specified, but required to make sure EM doesn't produce warning#
    null.comp=1 #null.comp also not used, but required #
  } else {#
    if(is.null(mixsd)){#
      mixsd = autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)#
    }#
    if(pointmass){#
      mixsd = c(0,mixsd)#
    }#
    if(nonzeromean & is.null(df)){#
		nonzeromean.fit=nonzeromeanEM(betahat[completeobs], sebetahat[completeobs], mixsd, maxiter=maxiter)#
		betahat[completeobs]= betahat[completeobs] - nonzeromean.fit$nonzeromean#
	}#
	else if(nonzeromean & !is.null(df)){#
		stop("Error: Nonzero mean only implemented for df=NULL")#
	}#
    null.comp = which.min(mixsd) #which component is the "null"#
    k = length(mixsd)#
    if(!is.numeric(prior)){#
      if(prior=="nullbiased"){ # set up prior to favour "null"#
        prior = rep(1,k)#
        prior[null.comp] = nullweight #prior 10-1 in favour of null by default#
      }else if(prior=="uniform"){#
        prior = rep(1,k)#
      }#
    }#
    if(length(prior)!=k | !is.numeric(prior)){#
      stop("invalid prior specification")#
    }#
    if(randomstart){#
      pi = rgamma(k,1,1)#
    } else {#
      if(k<n){#
        pi=rep(1,k)/n #default initialization strongly favours null; puts weight 1/n on everything except null#
        pi[null.comp] = (n-k+1)/n #the motivation is data can quickly drive away from null, but tend to drive only slowly toward null.#
      } else {#
        pi=rep(1,k)/k#
      }#
    }#
    pi=normalize(pi)#
    if(!is.element(mixcompdist,c("normal","uniform","halfuniform"))) stop("Error: invalid type of mixcompdist")#
    if(mixcompdist=="normal") g=normalmix(pi,rep(0,k),mixsd)#
    if(mixcompdist=="uniform") g=unimix(pi,-mixsd,mixsd)#
    if(mixcompdist=="halfuniform"){#
      g = unimix(c(pi,pi)/2,c(-mixsd,rep(0,k)),c(rep(0,k),mixsd))#
      prior = rep(prior, 2)#
      pi = rep(pi, 2)#
    }#
  }#
  pi.fit=EMest(betahat[completeobs],lambda1*sebetahat[completeobs]+lambda2,g,prior,null.comp=null.comp,nullcheck=nullcheck,VB=VB,maxiter = maxiter, cxx=cxx, df=df)  #
  if (!onlylogLR){#
      n=length(betahat)#
      if (!multiseqoutput){#
          ZeroProb = rep(0,length=n)#
          NegativeProb = rep(0,length=n)#
      }#
      if (!minimaloutput){#
          PosteriorMean = rep(0,length=n)#
          PosteriorSD = rep(0,length=n)#
      }#
      if (!multiseqoutput){#
          ZeroProb[completeobs] = colSums(comppostprob(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)[comp_sd(pi.fit$g)==0,,drop=FALSE])#
          NegativeProb[completeobs] = cdf_post(pi.fit$g, 0, betahat[completeobs],sebetahat[completeobs],df) - ZeroProb[completeobs]#
      }#
      if (!minimaloutput){#
          PosteriorMean[completeobs] = postmean(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
          PosteriorSD[completeobs] = postsd(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
      }#
                                        #FOR MISSING OBSERVATIONS, USE THE PRIOR INSTEAD OF THE POSTERIOR#
      if (!multiseqoutput){#
          ZeroProb[!completeobs] = sum(mixprop(pi.fit$g)[comp_sd(pi.fit$g)==0])#
          NegativeProb[!completeobs] = mixcdf(pi.fit$g,0)#
          lfsr = compute_lfsr(NegativeProb,ZeroProb)#
      }#
      if (!minimaloutput){#
          PosteriorMean[!completeobs] = mixmean(pi.fit$g)#
          PosteriorSD[!completeobs] = mixsd(pi.fit$g)#
      }#
      if (!minimaloutput & !multiseqoutput){#
          PositiveProb = 1- NegativeProb-ZeroProb#
          lfsra = compute_lfsra(PositiveProb,NegativeProb,ZeroProb) #
          lfdr = ZeroProb#
          qvalue = qval.from.lfdr(lfdr)#
      }#
  }#
  if (!minimaloutput)#
      logLR = tail(pi.fit$loglik,1) - pi.fit$null.loglik#
  if(nonzeromean & is.null(df)){#
      #Adding back the nonzero mean#
      betahat[completeobs]= betahat[completeobs]+nonzeromean.fit$nonzeromean#
      pi.fit$g$mean =nonzeromean.fit$nonzeromean#
      PosteriorMean= PosteriorMean + nonzeromean.fit$nonzeromean      #
  }	   #
  if (onlylogLR)#
      return(list(fitted.g=pi.fit$g, logLR = logLR, df=df))#
  else if (minimaloutput)#
      return(list(fitted.g = pi.fit$g, lfsr = lfsr, fit = pi.fit,df=df))#
  else if (multiseqoutput)#
      return(list(fitted.g = pi.fit$g, logLR = logLR, PosteriorMean = PosteriorMean, PosteriorSD = PosteriorSD, call= match.call(),df=df))#
  else{#
      result = list(fitted.g = pi.fit$g, logLR = logLR, PosteriorMean = PosteriorMean, PosteriorSD = PosteriorSD, PositiveProb = PositiveProb, NegativeProb = NegativeProb, ZeroProb = ZeroProb, lfsr = lfsr,lfsra = lfsra, lfdr = lfdr, qvalue = qvalue, fit = pi.fit, lambda1 = lambda1, lambda2 = lambda2, call = match.call(), data = list(betahat = betahat, sebetahat=sebetahat),df=df)#
      class(result) = "ash"#
      return(result)#
  }#
}#
  #if(nsamp>0){#
  #  sample = posterior_sample(post,nsamp)#
  #}#
#
# #' @title Faster version of function ash#
# #'#
# #' @description This function has similar functionality as ash, but only returns some of the outputs.#
# #'#
# #' @param betahat, a p vector of estimates#
# #' @param sebetahat, a p vector of corresponding standard errors#
# #' @param nullcheck: whether to check that any fitted model exceeds the "null" likelihood in which all weight is on the first component#
# #' @param randomstart: bool, indicating whether to initialize EM randomly. If FALSE, then initializes to prior mean (for EM algorithm) or prior (for VBEM)#
# #' @param pointmass: bool, indicating whether to use a point mass at zero as one of components for a mixture distribution#
# #' @param onlylogLR: bool, indicating whether to use this function to get logLR. Skip posterior prob, posterior mean, lfdr...#
# #' @param prior: string, or numeric vector indicating Dirichlet prior on mixture proportions (defaults to "uniform", or 1,1...,1; also can be "nullbiased" 1,1/k-1,...,1/k-1 to put more weight on first component)#
# #' @param mixsd: vector of sds for underlying mixture components#
# #' @param VB: whether to use Variational Bayes to estimate mixture proportions (instead of EM to find MAP estimate)#
# #' @param gridmult: the multiplier by which the default grid values for mixsd differ by one another. (Smaller values produce finer grids)#
# #' @param g: the prior distribution for beta (usually estimated from the data; this is used primarily in simulated data to do computations with the "true" g)#
# #' @param cxx: flag to indicate whether to use the c++ (Rcpp) version#
# #'#
# #' @return a list with elements fitted.g is fitted mixture#
# #' logLR : logP(D|mle(pi)) - logP(D|null)#
# #'#
# #' @export#
# fast.ash = function(betahat,sebetahat, #
#                     nullcheck=TRUE,randomstart=FALSE, #
#                     pointmass = TRUE,    #
#                     prior=c("nullbiased","uniform"), #
#                     mixsd=NULL, VB=FALSE,gridmult=4,#
#                     g=NULL, cxx=TRUE,#
#                     onlylogLR = FALSE,df=NULL){#
#   #
#   if(onlylogLR){#
#     pointmass <- TRUE  #
#   }#
#   #
#   #If method is supplied, use it to set up defaults; provide warning if these default values#
#   #are also specified by user#
#   if(!is.numeric(prior)){#
#     prior = match.arg(prior)#
#   }#
#   #
#   if(length(sebetahat)==1){#
#     sebetahat = rep(sebetahat,length(betahat))#
#   }#
#   if(length(sebetahat) != length(betahat)){#
#     stop("Error: sebetahat must have length 1, or same length as betahat")#
#   }#
#   #
#   completeobs = (!is.na(betahat) & !is.na(sebetahat))#
#   if(sum(completeobs)==0){#
#     if(onlylogLR){#
#       return(list(pi=NULL, logLR = 0))#
#     }else{#
#       stop("Error: all input values are missing")#
#     }#
#   }  #
#   #
#   if(is.null(mixsd)){#
#     mixsd= autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)#
#   }#
#   if(pointmass){#
#     mixsd = c(0,mixsd)#
#   }#
#   #
#   k=length(mixsd)  #
#   null.comp = which.min(mixsd) #which component is the "null"#
#   #
#   if(!is.numeric(prior)){#
#     if(prior=="nullbiased"){ # set up prior to favour "null"#
#       prior = rep(1,k)#
#       prior[null.comp] = 10 #prior 10-1 in favour of null#
#     }else if(prior=="uniform"){#
#       prior = rep(1,k)#
#     }#
#   }#
#   #
#   if(length(prior)!=k | !is.numeric(prior)){#
#     stop("invalid prior specification")#
#   }#
#   #
#   if(missing(g)){#
#     pi = prior^2 #default is to initialize pi at prior (mean)#
#     if(randomstart){pi=rgamma(k,1,1)}#
#     pi=normalize(pi)#
#     g=normalmix(pi,rep(0,k),mixsd)#
#     maxiter = 5000#
#   } else {#
#     maxiter = 1; # if g is specified, don't iterate the EM #
#   }#
#   #
#   pi.fit=EMest(betahat[completeobs],sebetahat[completeobs],g,prior,null.comp=null.comp,nullcheck=nullcheck,VB=VB,maxiter = maxiter, cxx=cxx, df=df)  #
#   #
#   if(onlylogLR){#
#     logLR = tail(pi.fit$loglik,1) - pi.fit$null.loglik#
#     return(list(pi=pi.fit$pi, logLR = logLR))#
#   }else{#
#     #
#     n=length(betahat)#
#     PosteriorMean = rep(0,length=n)#
#     PosteriorSD=rep(0,length=n)#
#     #
#     if(is.null(df)){#
#       PosteriorMean[completeobs] = postmean(pi.fit$g,betahat[completeobs],sebetahat[completeobs])#
#       PosteriorSD[completeobs] =postsd(pi.fit$g,betahat[completeobs],sebetahat[completeobs]) #
#     }#
#     else{#
#       PosteriorMean[completeobs] = postmean_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
#       PosteriorSD[completeobs] =postsd_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
#     }#
#     #FOR MISSING OBSERVATIONS, USE THE PRIOR INSTEAD OF THE POSTERIOR#
#     PosteriorMean[!completeobs] = mixmean(pi.fit$g)#
#     PosteriorSD[!completeobs] =mixsd(pi.fit$g)  #
#     #
#     result = list(fitted.g=pi.fit$g,PosteriorMean = PosteriorMean,PosteriorSD=PosteriorSD,call=match.call(),data=list(betahat = betahat, sebetahat=sebetahat))#
#     return(result)#
#   }#
#   #if(nsamp>0){#
#   #  sample = posterior_sample(post,nsamp)#
#   #}#
# }#
compute_lfsr = function(NegativeProb,ZeroProb){#
  ifelse(NegativeProb> 0.5*(1-ZeroProb),1-NegativeProb,NegativeProb+ZeroProb)#
}#
#
compute_lfsra = function(PositiveProb, NegativeProb,ZeroProb){#
  ifelse(PositiveProb<NegativeProb,2*PositiveProb+ZeroProb,2*NegativeProb+ZeroProb)  #
}  #
#
#' @title Estimate unimodal nonzero mean of a mixture model by EM algorithm#
#'#
#' @description Given the data, standard error of the data and standard deviations of the Gaussian mixture model, estimate the mean of a unimodal Gaussian mixture by an EM algorithm.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates unimodal mean \eqn{\mu} by EM algorithm. Uses the SQUAREM package to accelerate convergence of EM. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' #
#' @param betahat, a p vector of estimates #
#' @param sebetahat, a p vector of corresponding standard errors#
#' @param mixsd: vector of sds for underlying mixture components #
#' @param pi.init, the initial value of \eqn{\pi} to use. If not specified defaults to (1/k,...,1/k).#
#' @param tol, the tolerance for convergence of log-likelihood.#
#' @param maxiter the maximum number of iterations performed#
#' #
#' @return A list, including the estimates (\eqn{\mu}) and (\eqn{\pi}), the log likelihood for each iteration (NQ)#
#' and a flag to indicate convergence#
#'  #
#' @export#
#' #
#' #
nonzeromeanEM = function(betahat, sebetahat, mixsd, pi.init=NULL,tol=1e-7,maxiter=5000){#
  if(is.null(pi.init)){#
    pi.init = rep(1/length(mixsd),length(mixsd))# Use as starting point for pi#
  }#
  mupi=c(mean(betahat),pi.init)#
  res=squarem(par=mupi,fixptfn=nonzeromeanEMfixpoint,objfn=nonzeromeanEMobj,betahat=betahat,sebetahat=sebetahat,mixsd=mixsd,control=list(maxiter=maxiter,tol=tol))#
  return(list(nonzeromean=res$par[1],pi=res$par[-1],NQ=-res$value.objfn,niter = res$iter, converged=res$convergence,post=res$par))#
}#
#
nonzeromeanEMfixpoint = function(mupi,betahat,sebetahat,mixsd){#
	#omegamatrix=matrix(NA,nrow=length(betahat),ncol=length(mixsd))#
	mu=mupi[1]#
	pimean=mupi[-1]#
	sdmat = sqrt(outer(sebetahat ^2,mixsd^2,"+")) #
	xmat=matrix(rep(betahat,length(mixsd)),ncol=length(mixsd))#
	omegamatrix=t(t(dnorm(xmat,mean=mu,sd=sdmat))*pimean)#
	omegamatrix=omegamatrix /rowSums(omegamatrix)#
	pinew=normalize(colSums(omegamatrix))#
	munew=sum(omegamatrix*xmat/(sdmat^2))/sum(omegamatrix/(sdmat^2))#
	mupi=c(munew,pinew)#
	return(mupi)#
}#
#
nonzeromeanEMobj = function(mupi,betahat,sebetahat,mixsd){#
	mu=mupi[1]#
	pimean=mupi[-1]#
	sdmat = sqrt(outer(sebetahat ^2,mixsd^2,"+")) #
	xmat=matrix(rep(betahat,length(mixsd)),ncol=length(mixsd))#
	omegamatrix=t(t(dnorm(xmat,mean=mu,sd=sdmat))*pimean)#
	omegamatrix=omegamatrix /rowSums(omegamatrix)#
	NegativeQ=-sum(omegamatrix*dnorm(xmat,mean=mu,sd=sdmat,log=TRUE))#
	return(NegativeQ)#
}#
#' @title Estimate posterior distribution on mixture proportions of a mixture model by a Variational Bayes EM algorithm#
#'#
#' @description Given the individual component likelihoods for a mixture model, estimates the posterior on #
#' the mixture proportions by an VBEM algorithm. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates posterior on mixture proportions \eqn{\pi} by Variational Bayes, #
#' with a Dirichlet prior on \eqn{\pi}. #
#' Algorithm adapted from Bishop (2009), Pattern Recognition and Machine Learning, Chapter 10.#
#' #
#' @param matrix_lik: a n by k matrix with (j,k)th element equal to \eqn{f_k(x_j)}.#
#' @param prior: a k vector of the parameters of the Dirichlet prior on \eqn{\pi}. Recommended to be rep(1,k)#
#' @param post.init: the initial value of the posterior parameters. If not specified defaults to the prior parameters.#
#' @param tol: the tolerance for convergence of log-likelihood bound.#
#' @param maxiter: the maximum number of iterations performed#
#' #
#' @return A list, whose components include point estimates (pihat), #
#' the parameters of the fitted posterior on \eqn{\pi} (pipost),#
#' the bound on the log likelihood for each iteration (B)#
#' and a flag to indicate convergence (converged).#
#'  #
#' @export#
#' #
mixVBEM = function(matrix_lik, prior, pi.init = NULL,tol=1e-7, maxiter=5000){#
  k=ncol(matrix_lik)#
  if(is.null(pi.init)){#
    pi.init = rep(1,k)# Use as starting point for pi#
  } #
  res = squarem(par=pi.init,fixptfn=VBfixpoint, objfn=VBnegpenloglik,matrix_lik=matrix_lik, prior=prior, control=list(maxiter=maxiter,tol=tol))#
  return(list(pihat = res$par/sum(res$par), B=res$value.objfn, niter = res$iter, converged=res$convergence,post=res$par))#
}#
VBfixpoint = function(pipost, matrix_lik, prior){  #
  n=nrow(matrix_lik)#
  k=ncol(matrix_lik)#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost*matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  pipostnew = colSums(classprob) + prior#
  return(pipostnew)#
}#
#
VBnegpenloglik=function(pipost,matrix_lik,prior){#
  return(-VBpenloglik(pipost,matrix_lik,prior))#
}#
#
VBpenloglik = function(pipost, matrix_lik, prior){#
  n=nrow(matrix_lik)#
  k=ncol(matrix_lik)#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost*matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  B= sum(classprob*log(avgpipost*matrix_lik),na.rm=TRUE) - diriKL(prior,pipost) - sum(classprob*log(classprob)) #
  return(B)#
}#
#' @title Estimate mixture proportions of a mixture model by EM algorithm#
#'#
#' @description Given the individual component likelihoods for a mixture model, estimates the mixture proportions by an EM algorithm.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates mixture proportions \eqn{\pi} by maximum likelihood, or by maximum a posteriori (MAP) estimation for a Dirichlet prior on $\pi$ #
#' (if a prior is specified).  Uses the SQUAREM package to accelerate convergence of EM. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' #
#' @param matrix_lik, a n by k matrix with (j,k)th element equal to \eqn{f_k(x_j)}.#
#' @param prior, a k vector of the parameters of the Dirichlet prior on \eqn{\pi}. Recommended to be rep(1,k)#
#' @param pi.init, the initial value of \eqn{\pi} to use. If not specified defaults to (1/k,...,1/k).#
#' @param tol, the tolerance for convergence of log-likelihood.#
#' @param maxiter the maximum number of iterations performed#
#' #
#' @return A list, including the estimates (pihat), the log likelihood for each interation (B)#
#' and a flag to indicate convergence#
#'  #
#' @export#
#' #
#' #
mixEM = function(matrix_lik, prior, pi.init = NULL,tol=1e-7, maxiter=5000){#
  if(is.null(pi.init)){#
    pi.init = rep(1/k,k)# Use as starting point for pi#
  } #
  res = squarem(par=pi.init,fixptfn=fixpoint, objfn=negpenloglik,matrix_lik=matrix_lik, prior=prior, control=list(maxiter=maxiter,tol=tol))#
  return(list(pihat = normalize(pmax(0,res$par)), B=res$value.objfn, #
              niter = res$iter, converged=res$convergence))#
}#
#
# helper functions used by mixEM#
normalize = function(x){return(x/sum(x))}#
#
fixpoint = function(pi, matrix_lik, prior){  #
  pi = normalize(pmax(0,pi)) #avoid occasional problems with negative pis due to rounding#
  m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
  m.rowsum = rowSums(m)#
  classprob = m/m.rowsum #an n by k matrix#
  pinew = normalize(colSums(classprob) + prior - 1)#
  return(pinew)#
}#
#
negpenloglik = function(pi,matrix_lik,prior){return(-penloglik(pi,matrix_lik,prior))}#
#
penloglik = function(pi, matrix_lik, prior){#
  pi = normalize(pmax(0,pi))#
  m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
  m.rowsum = rowSums(m)#
  loglik = sum(log(m.rowsum))#
  subset = (prior != 1.0)#
  priordens = sum((prior-1)[subset]*log(pi[subset]))#
  return(loglik+priordens)#
}#
#
#The kth element of this vector is the derivative #
#of the loglik for $\pi=(\pi_0,...,1-\pi_0,...)$ with respect to $\pi_0$ at $\pi_0=1$.#
gradient = function(matrix_lik){#
  n = nrow(matrix_lik)#
  grad = n - colSums(matrix_lik/matrix_lik[,1]) #
  return(grad)#
}#
#
# mixEM = function(matrix_lik, prior, pi.init = NULL,tol=0.0001, maxiter=5000){#
#   n=nrow(matrix_lik)#
#   k=ncol(matrix_lik)#
#   B = rep(0,maxiter)#
#   pi = pi.init#
#   if(is.null(pi.init)){#
#     pi = rep(1/k,k)# Use as starting point for pi#
#   } #
#   pi = ifelse(pi<1e-5,1e-5,pi) #set any estimates that are too small to be just very small#
#   pi = normalize(pi)#
#   #
#   loglik = rep(0,maxiter)#
#   priordens= rep(0,maxiter)#
#   m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
#   m.rowsum = rowSums(m)#
#   loglik[1] = sum(log(m.rowsum))#
#   priordens[1] = sum((prior-1)*log(pi)) #
#   classprob = m/m.rowsum #an n by k matrix#
#   i=1#
#   if(maxiter >= 2){#
#     for(i in 2:maxiter){  #
#       pi = colSums(classprob) + prior-1#
#       pi = ifelse(pi<1e-5,1e-5,pi) #set any estimates that are less than zero, which can happen with prior<1, to 0#
#       pi = normalize(pi)#
#         #
#       #Now re-estimate pi#
#       m  = t(pi * t(matrix_lik)) #
#       m.rowsum = rowSums(m)#
#       loglik[i] = sum(log(m.rowsum))#
#       priordens[i] = sum((prior-1)*log(pi)) #
#       classprob = m/m.rowsum#
#     #
#     #
#       if(abs(loglik[i]+priordens[i]-loglik[i-1]-priordens[i-1])<tol) break;#
#     }#
#   }#
#   converged=(abs(loglik[i]+priordens[i]-loglik[i-1]-priordens[i-1])<tol)#
#   if(!converged){#
#       warning("EM algorithm in function mixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
#   }#
#   return(list(pihat = pi, B=loglik[1:i], #
#               niter = i, converged=converged))#
# }#
#estimate mixture proportions of sigmaa by EM algorithm#
#prior gives the parameter of a Dirichlet prior on pi#
#(prior is used to encourage results towards smallest value of sigma when#
#likelihood is flat)#
#nullcheck indicates whether to check whether the loglike exceeds the null#
#(may not want to use if prior is used)#
#VB provides an approach to estimate the approximate posterior distribution#
#of mixture proportions of sigmaa by variational Bayes method#
#(use Dirichlet prior and approximate Dirichlet posterior)#
#if cxx TRUE use cpp version of R function mixEM#
EMest = function(betahat,sebetahat,g,prior,null.comp=1,nullcheck=TRUE,VB=FALSE, maxiter=5000, cxx=TRUE, df=NULL){ #
  pi.init = g$pi#
  k=ncomp(g)#
  n = length(betahat)#
  tol = min(0.1/n,1e-5) # set convergence criteria to be more stringent for larger samples#
  matrix_lik = t(compdens_conv(g,betahat,sebetahat,df))#
  #checks whether the gradient at pi0=1 is positive (suggesting that this is a fixed point)#
  #if(nullcheck){#
  #  if(all(gradient(matrix_lik)>=0)){#
  #    pi.init=rep(0,k)#
  #    pi.init[null.comp]=1 #this will make pi.init=(1,0,0...,0) which is a fixed point of the EM#
  #  }#
  #}#
  if(VB==TRUE){#
    EMfit=mixVBEM(matrix_lik,prior,maxiter=maxiter)}#
  else{#
    if (cxx==TRUE){#
      EMfit = cxxMixEM(matrix_lik,prior,pi.init,1e-5, maxiter) #currently use different convergence criteria for cxx version #
      if(!EMfit$converged){#
        warning("EM algorithm in function cxxMixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
      }#
    }#
    else{#
      EMfit = mixEM(matrix_lik,prior,pi.init,tol, maxiter)#
      if(!EMfit$converged & !(maxiter==1)){#
        warning("EM algorithm in function mixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
      }#
    }#
  }#
  pi = EMfit$pihat     #
  penloglik = EMfit$B #
  converged = EMfit$converged#
  niter = EMfit$niter#
  loglik.final =  penloglik(pi,matrix_lik,1) #compute penloglik without penalty#
  null.loglik = sum(log(matrix_lik[,null.comp]))  #
  if(nullcheck==TRUE & VB==FALSE){ #null check doesn't work with VB yet#
    pinull = rep(0,k)#
    pinull[null.comp]=1#
    null.penloglik = penloglik(pinull,matrix_lik,prior)#
    final.penloglik = penloglik(pi,matrix_lik,prior)#
    if(null.penloglik > final.penloglik){ #check whether exceeded "null" likelihood where everything is null#
      pi=pinull#
      loglik.final=penloglik(pi,matrix_lik,1)#
    }#
  }#
  g$pi=pi#
  return(list(loglik=loglik.final,null.loglik=null.loglik,#
              matrix_lik=matrix_lik,converged=converged,g=g))#
}#
#' @title Compute Posterior#
#'#
#' @description Return the posterior on beta given a prior (g) that is a mixture of normals (class normalmix) #
#' and observation betahat \sim N(beta,sebetahat)#
#'#
#' @details This can be used to obt#
#'#
#' @param g: a normalmix with components indicating the prior; works only if g has means 0#
#' @param betahat (n vector of observations) #
#' @param sebetahat (n vector of standard errors/deviations of observations)#
#' #
#' @return A list, (pi1,mu1,sigma1) whose components are each k by n matrices#
#' where k is number of mixture components in g, n is number of observations in betahat#
#' #
#' @export#
#' #
#' #
posterior_dist = function(g,betahat,sebetahat){#
  if(class(g)!="normalmix"){#
    stop("Error: posterior_dist implemented only for g of class normalmix")#
  }#
  pi0 = g$pi#
  mu0 = g$mean#
  sigma0 = g$sd  #
  k= length(pi0)#
  n= length(betahat)#
  if(!all.equal(g$mean,rep(0,k))) stop("Error: posterior_dist currently only implemented for zero-centered priors")#
  pi1 = pi0 * t(matrix_dens(betahat,sebetahat,sigma0))#
  pi1 = apply(pi1, 2, normalize) #pi1 is now an k by n matrix#
  #make k by n matrix versions of sigma0^2 and sebetahat^2#
  # and mu0 and betahat#
  s0m2 = matrix(sigma0^2,nrow=k,ncol=n,byrow=FALSE)#
  sebm2 = matrix(sebetahat^2,nrow=k,ncol=n, byrow=TRUE)#
  mu0m = matrix(mu0,nrow=k,ncol=n,byrow=FALSE)#
  bhatm = matrix(betahat,nrow=k,ncol=n,byrow=TRUE)#
  sigma1 = (s0m2*sebm2/(s0m2 + sebm2))^(0.5)  #
  w = sebm2/(s0m2 + sebm2)#
  mu1 = w*mu0m + (1-w)*bhatm#
  #WHERE DATA ARE MISSING, SET POSTERIOR = PRIOR#
  ismiss = (is.na(betahat) | is.na(sebetahat)) #
  pi1[,ismiss] = pi0#
  mu1[,ismiss] = mu0#
  sigma1[,ismiss] = sigma0#
  return(list(pi=pi1,mu=mu1,sigma=sigma1))#
}#
#
#return matrix of densities of observations (betahat) #
# assuming betahat_j \sim N(0, sebetahat_j^2 + sigmaavec_k^2)#
#normalized by maximum of each column#
#INPUT#
#betahat is n vector, #
#sebetahat is n vector, #
#sigmaavec is k vector#
#return is n by k matrix of the normal likelihoods, #
# with (j,k)th element the density of N(betahat_j; mean=0, var = sebetahat_j^2 + sigmaavec_k^2)#
#normalized to have maximum 1 in each column#
matrix_dens = function(betahat, sebetahat, sigmaavec){#
  k = length(sigmaavec)#
  n = length(betahat)#
  ldens = dnorm(betahat,0,sqrt(outer(sebetahat^2,sigmaavec^2,FUN="+")),log=TRUE)#
  maxldens = apply(ldens, 1, max)#
  ldens = ldens - maxldens#
  return(exp(ldens))#
}#
#
#return the "effective" estimate#
#that is the effect size betanew whose z score betanew/se#
#would give the same p value as betahat/se compared to a t with df#
effective.effect=function(betahat,se,df){#
  p = pt(betahat/se,df)#
  qnorm(p,sd=se)#
}#
#' @title Function to compute q values from local false discovery rates#
#'#
#' @description Computes q values from a vector of local fdr estimates#
#'#
#' @details The q value for a given lfdr is an estimate of the (tail) False Discovery Rate #
#' for all findings with a smaller lfdr, and is found by the average of the lfdr for#
#' all more significant findings. See Storey (2003), Annals of Statistics, for definition of q value.  #
#' #
#' #
#' @param lfdr, a vector of local fdr estimates#
#'#
#' @return vector of q values#
#' #
#' @export#
qval.from.lfdr = function(lfdr){#
  o = order(lfdr)#
  qvalue=rep(NA,length(lfdr))#
  qvalue[o] = (cumsum(sort(lfdr))/(1:sum(!is.na(lfdr))))#
  return(qvalue)#
}#
#
# try to select a default range for the sigmaa values#
# that should be used, based on the values of betahat and sebetahat#
# mult is the multiplier by which the sds differ across the grid#
autoselect.mixsd = function(betahat,sebetahat,mult){#
  sigmaamin = min(sebetahat)/10 #so that the minimum is small compared with measurement precision#
  if(all(betahat^2<sebetahat^2)){#
    sigmaamax = 8*sigmaamin #to deal with the occassional odd case where this could happen; 8 is arbitrary#
  } else {#
    sigmaamax = 2*sqrt(max(betahat^2-sebetahat^2)) #this computes a rough largest value you'd want to use, based on idea that sigmaamax^2 + sebetahat^2 should be at least betahat^2   #
  }#
  if(mult==0){#
    return(c(0,sigmaamax/2))#
  }else{#
    npoint = ceiling(log2(sigmaamax/sigmaamin)/log2(mult))#
    return(mult^((-npoint):0) * sigmaamax)#
  }#
}#
#return the KL-divergence between 2 dirichlet distributions#
#p,q are the vectors of dirichlet parameters of same lengths#
diriKL = function(p,q){#
  p.sum = sum(p)#
  q.sum = sum(q)#
  k = length(q)#
  KL = lgamma(q.sum)-lgamma(p.sum)+sum((q-p)*(digamma(q)-digamma(rep(q.sum,k))))+sum(lgamma(p)-lgamma(q))#
  return(KL)#
}#
#
#helper function for VBEM#
VB.update = function(matrix_lik, pipost){#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost * matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  B = sum(classprob*log(avgpipost*matrix_lik),na.rm=TRUE) - diriKL(prior,pipost) #negative free energy#
  return(list(classprob=classprob,B=B))#
}
test.ash=ash(test$betahat,test$betahatsd,method="shrink",nonzeromean=TRUE)
test.ash
mean(test$betahat)
simdata = function(n, betahatsd,nullproportion=1, altmean=1, altsd=1) {#
    nnull=round(n*nullproportion,0)#
    null = c(rep(1, nnull), rep(0, n - nnull))#
    beta = c(rep(0, nnull), rnorm(n - nnull, altmean, altsd))#
    betahat = rnorm(n, beta, betahatsd)#
    return(list(null = null, beta = beta, betahat = betahat, betahatsd = betahatsd))#
  }
set.seed(200)#
test=simdata(100,betahatsd=1,nullproportion=0.5,altmean=2,altsd=1)#
print(test$beta)#
test.ash1=ash(test$betahat,test$betahatsd,method="shrink",nonzeromean=FALSE)#
test.ash2=ash(test$betahat,test$betahatsd,method="shrink",nonzeromean=TRUE)
library(knitr)#
library(devtools)#
library(roxygen2)#
library(testthat)#
library(SQUAREM)#
library(truncnorm)
set.seed(200)#
test=simdata(100,betahatsd=1,nullproportion=0.5,altmean=2,altsd=1)#
print(test$beta)#
test.ash1=ash(test$betahat,test$betahatsd,method="shrink",nonzeromean=FALSE)#
test.ash2=ash(test$betahat,test$betahatsd,method="shrink",nonzeromean=TRUE)
ASH UTILITY FUNCTIONS #############################
#
#' @title Summary method for ash object#
#'#
#' @description Print summary of fitted ash object#
#'#
#' @details See readme for more details#
#' #
#' @export#
#' #
summary.ash=function(a){#
  print(a$fitted.g)#
  print(tail(a$fit$loglik,1),digits=10)#
  print(a$fit$converged)#
}#
#
#' @title Print method for ash object#
#'#
#' @description Print the fitted distribution of beta values in the EB hierarchical model#
#'#
#' @details None#
#' #
#' @export#
#' #
print.ash =function(a){#
  print(a$fitted.g)#
}#
#
#' @title Plot method for ash object#
#'#
#' @description Plot the density of the underlying fitted distribution#
#'#
#' @details None#
#' #
#' @export#
#' #
plot.ash = function(a,xmin,xmax,...){#
  x = seq(xmin,xmax,length=1000)#
  y = density(a,x)#
  plot(y,type="l",...)#
}#
#
#compute the predictive density of an observation#
#given the fitted ash object a and the vector se of standard errors#
#not implemented yet#
predictive=function(a,se){#
}#
#' @title Get fitted loglikelihood for ash object#
#'#
#' @description Return the log-likelihood of the data under the fitted distribution#
#'#
#' @param a the fitted ash object#
#'#
#' @details None#
#' #
#' @export#
#' #
#'#
get_loglik = function(a){#
  return(tail(a$fit$loglik,1))#
}#
#
#' @title Get pi0 estimate for ash object#
#'#
#' @description Return estimate of the null proportion, pi0#
#'#
#' @param a the fitted ash object#
#'#
#' @details Extracts the estimate of the null proportion, pi0, from the object a#
#' #
#' @export#
#' #
get_pi0 = function(a){#
  null.comp = comp_sd(a$fitted.g)==0#
  return(sum(a$fitted.g$pi[null.comp]))#
}#
#
#' @title Compute loglikelihood for data from ash fit#
#'#
#' @description Return the log-likelihood of the data betahat, with standard errors betahatsd, #
#' under the fitted distribution in the ash object. #
#' #
#'#
#' @param a the fitted ash object#
#' @param betahat the data#
#' @param betahatsd the observed standard errors#
#' @param zscores indicates whether ash object was originally fit to z scores #
#' @details None#
#' #
#' @export#
#' #
#'#
loglik.ash = function(a,betahat,betahatsd,zscores=FALSE){#
  g=a$fitted.g#
  FUN="+"#
  if(zscores==TRUE){#
    g$sd = sqrt(g$sd^2+1) #
    FUN="*"#
  }#
  return(loglik_conv(g,betahat, betahatsd,FUN))#
}#
#
#' @title Density method for ash object#
#'#
#' @description Return the density of the underlying fitted distribution#
#'#
#' @param a the fitted ash object#
#' @param x the vector of locations at which density is to be computed#
#'#
#' @details None#
#' #
#' @export#
#' #
#'#
density.ash=function(a,x){list(x=x,y=dens(a$fitted.g,x))}#
#
#' @title cdf method for ash object#
#'#
#' @description Computed the cdf of the underlying fitted distribution#
#'#
#' @param a the fitted ash object#
#' @param x the vector of locations at which cdf is to be computed#
#' @param lower.tail (default=TRUE) whether to compute the lower or upper tail#
#'#
#' @details None#
#' #
#' @export#
#' #
#'#
cdf.ash=function(a,x,lower.tail=TRUE){#
  return(list(x=x,y=mixcdf(a$fitted.g,x,lower.tail)))#
}#
#' @title Credible Interval Computation for the ash object#
#'#
#' @description Given the ash object return by the main function ash, this function computes the corresponding credible interval of the mixture model.#
#'#
#' @details Uses default optimization function and perform component-wise credible interval computation. The computation cost is linear of the length of betahat.#
#'#
#' @param a the fitted ash object #
#' @param levels, the level for the credible interval, (default=0.95)#
#' @param tol, the tolerance for convergence of log-likelihood.#
#' #
#' @return A matrix, with first column being the posterior mean, second and third column being the lower bound and upper bound for the credible interval. #
#'  #
#' @export#
#' #
#' #
ashci = function (a,level=0.95){#
  x=a$data$betahat#
  s=a$data$sebetahat#
  m=a$fitted.g#
  lower=min(x)-qnorm(level)*(max(m$sd)+max(s))#
  upper=max(x)+qnorm(level)*(max(m$sd)+max(s))#
  CImatrix=matrix(NA,nrow=length(x),ncol=3)	#
  colnames(CImatrix)=c("Posterior Mean",(1-level)/2,(1+level)/2)#
  CImatrix[,1]=a$PosteriorMean#
  if( class(a$fitted.g) == "normalmix" | class(a$fitted.g) == "unimix" ){#
    for(i in 1:length(x)){#
	  CImatrix[i,2]=optim(par=a$PosteriorMean[i],f=ci.lower,m=m,x=x[i],s=s[i],level=level,df=df,method="Brent",lower=lower,upper=upper)$par#
	  CImatrix[i,3]=optim(par=a$PosteriorMean[i],f=ci.upper,m=m,x=x[i],s=s[i],level=level,df=df,method="Brent",lower=lower,upper=upper)$par#
	}#
  }#
  else{stop(paste("Invalid class",class(m)))}#
  return(CImatrix)#
}#
#
ci.lower=function(z,m,x,s,level,df){#
	tailprob=cdf_post(m,z,x,s,df)#
	return(abs(tailprob-(1-level)/2))#
}#
#
ci.upper=function(z,m,x,s,level,df){#
	tailprob=1-cdf_post(m,z,x,s,df)#
	return(abs(tailprob-(1-level)/2))#
}
' @useDynLib ashr#
#todo#
##
#' @title Main Adaptive SHrinkage function#
#'#
#' @description Takes vectors of estimates (betahat) and their standard errors (sebetahat), and applies#
#' shrinkage to them, using Empirical Bayes methods, to compute shrunk estimates for beta.#
#'#
#' @details See readme for more details#
#' #
#' @param betahat, a p vector of estimates #
#' @param sebetahat, a p vector of corresponding standard errors#
#' @param method: specifies how ash is to be run. Can be "shrinkage" (if main aim is shrinkage) or "fdr" (if main aim is to assess fdr or fsr)#
#' This is simply a convenient way to specify certain combinations of parameters: "shrinkage" sets pointmass=FALSE and prior="uniform";#
#' "fdr" sets pointmass=TRUE and prior="nullbiased".#
#' @param mixcompdist: distribution of components in mixture ("normal", "uniform" or "halfuniform")#
#'#
#' @param lambda1: multiplicative "inflation factor" for standard errors (like Genomic Control)#
#' @param lambda2: additive "inflation factor" for standard errors (like Genomic Control)#
#' @param nullcheck: whether to check that any fitted model exceeds the "null" likelihood#
#' in which all weight is on the first component#
#' @param df: appropriate degrees of freedom for (t) distribution of betahat/sebetahat#
#' @param randomstart: bool, indicating whether to initialize EM randomly. If FALSE, then initializes to prior mean (for EM algorithm) or prior (for VBEM)#
#' @param nullweight: scalar, the weight put on the prior of null under "fdr" method#
#' @param nonzeromean: bool, indicating whether to use a nonzero mean unimodal mixture(defaults to "FALSE")#
#' @param pointmass: bool, indicating whether to use a point mass at zero as one of components for a mixture distribution#
#' @param onlylogLR: bool, indicating whether to use this function to get logLR. Skip posterior prob, posterior mean, lfdr...#
#' @param prior: string, or numeric vector indicating Dirichlet prior on mixture proportions (defaults to "uniform", or 1,1...,1; also can be "nullbiased" 1,1/k-1,...,1/k-1 to put more weight on first component)#
#' @param mixsd: vector of sds for underlying mixture components #
#' @param VB: whether to use Variational Bayes to estimate mixture proportions (instead of EM to find MAP estimate)#
#' @param gridmult: the multiplier by which the default grid values for mixsd differ by one another. (Smaller values produce finer grids)#
#' @param minimal_output: if TRUE, just outputs the fitted g and the lfsr (useful for very big data sets where memory is an issue) #
#' @param g: the prior distribution for beta (usually estimated from the data; this is used primarily in simulated data to do computations with the "true" g)#
#' @param maxiter: maximum number of iterations of the EM algorithm#
#' @param cxx: flag to indicate whether to use the c++ (Rcpp) version#
#' #
#'#
#' @return a list with elements fitted.g is fitted mixture#
#' logLR : logP(D|mle(pi)) - logP(D|null)#
#' #
#' @export#
#' #
#' @examples #
#' beta = c(rep(0,100),rnorm(100))#
#' sebetahat = abs(rnorm(200,0,1))#
#' betahat = rnorm(200,beta,sebetahat)#
#' beta.ash = ash(betahat, sebetahat)#
#' summary(beta.ash)#
#' plot(betahat,beta.ash$PosteriorMean,xlim=c(-4,4),ylim=c(-4,4))#
#' #
#' #
#Things to do:#
# check sampling routine#
# check number of iterations#
ash = function(betahat,sebetahat,method = c("shrink","fdr"), #
               mixcompdist = c("normal","uniform","halfuniform"),#
               lambda1=1,lambda2=0,nullcheck=TRUE,df=NULL,randomstart=FALSE,#
               nullweight=10,nonzeromean=FALSE, #
               pointmass = FALSE, #
               onlylogLR = FALSE, #
               prior=c("uniform","nullbiased"), #
               mixsd=NULL, VB=FALSE,gridmult=sqrt(2),#
               minimaloutput=FALSE,#
               multiseqoutput=FALSE,#
               g=NULL,#
               maxiter = 5000,#
               cxx=FALSE){#
  #method provides a convenient interface to set a particular combinations of parameters for prior an#
  #If method is supplied, use it to set up specific values for these parameters; provide warning if values#
  #are also specified by user#
  #If method is not supplied use the user-supplied values (or defaults if user does not specify them)#
  if(!missing(method)){#
    method = match.arg(method) #
    if(method=="shrink"){#
      if(missing(prior)){#
        prior = "uniform"#
      } else {#
        warning("Specification of prior overrides default for method shrink")#
      }#
      if(missing(pointmass)){#
        pointmass=FALSE#
      } else {#
        warning("Specification of pointmass overrides default for method shrink")#
      }#
    }#
    if(method=="fdr"){#
      if(missing(prior)){#
        prior = "nullbiased"#
      } else {#
        warning("Specification of prior overrides default for method fdr")#
      }#
      if(missing(pointmass)){#
        pointmass=TRUE#
      } else {#
        warning("Specification of pointmass overrides default for method fdr")#
      }#
    }  #
  }#
  if(gridmult<=1&multiseqoutput!=TRUE)#
    stop("gridmult must be > 1")#
  mixcompdist = match.arg(mixcompdist)#
  # if(mixcompdist=="uniform" & pointmass==TRUE){#
  #    stop("point mass not yet implemented for uniform or half-uniform")#
  #  }#
  #  if(mixcompdist=="halfuniform" & pointmass==TRUE){#
  #    stop("point mass not yet implemented for uniform or half-uniform")#
  #  }#
  if(!is.numeric(prior)){#
    prior = match.arg(prior)#
  }  #
  if(length(sebetahat)==1){#
    sebetahat = rep(sebetahat,length(betahat))#
  }#
  if(length(sebetahat) != length(betahat)){#
    stop("Error: sebetahat must have length 1, or same length as betahat")#
  }#
  completeobs = (!is.na(betahat) & !is.na(sebetahat))#
  n=sum(completeobs)#
  if(n==0){#
    if(onlylogLR){#
      return(list(pi=NULL, logLR = 0))#
    }#
    else{#
      stop("Error: all input values are missing")#
    }#
  }  #
  if(!is.null(g)){#
    maxiter = 1 # if g is specified, don't iterate the EM#
    prior = rep(1,ncomp(g)) #prior is not actually used if g specified, but required to make sure EM doesn't produce warning#
    null.comp=1 #null.comp also not used, but required #
  } else {#
    if(is.null(mixsd)){#
      mixsd = autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)#
    }#
    if(pointmass){#
      mixsd = c(0,mixsd)#
    }#
    if(nonzeromean & is.null(df)){#
		nonzeromean.fit=nonzeromeanEM(betahat[completeobs], sebetahat[completeobs], mixsd, maxiter=maxiter)#
		betahat[completeobs]= betahat[completeobs] - nonzeromean.fit$nonzeromean#
	}#
	else if(nonzeromean & !is.null(df)){#
		stop("Error: Nonzero mean only implemented for df=NULL")#
	}#
    null.comp = which.min(mixsd) #which component is the "null"#
    k = length(mixsd)#
    if(!is.numeric(prior)){#
      if(prior=="nullbiased"){ # set up prior to favour "null"#
        prior = rep(1,k)#
        prior[null.comp] = nullweight #prior 10-1 in favour of null by default#
      }else if(prior=="uniform"){#
        prior = rep(1,k)#
      }#
    }#
    if(length(prior)!=k | !is.numeric(prior)){#
      stop("invalid prior specification")#
    }#
    if(randomstart){#
      pi = rgamma(k,1,1)#
    } else {#
      if(k<n){#
        pi=rep(1,k)/n #default initialization strongly favours null; puts weight 1/n on everything except null#
        pi[null.comp] = (n-k+1)/n #the motivation is data can quickly drive away from null, but tend to drive only slowly toward null.#
      } else {#
        pi=rep(1,k)/k#
      }#
    }#
    pi=normalize(pi)#
    if(!is.element(mixcompdist,c("normal","uniform","halfuniform"))) stop("Error: invalid type of mixcompdist")#
    if(mixcompdist=="normal") g=normalmix(pi,rep(0,k),mixsd)#
    if(mixcompdist=="uniform") g=unimix(pi,-mixsd,mixsd)#
    if(mixcompdist=="halfuniform"){#
      g = unimix(c(pi,pi)/2,c(-mixsd,rep(0,k)),c(rep(0,k),mixsd))#
      prior = rep(prior, 2)#
      pi = rep(pi, 2)#
    }#
  }#
  pi.fit=EMest(betahat[completeobs],lambda1*sebetahat[completeobs]+lambda2,g,prior,null.comp=null.comp,nullcheck=nullcheck,VB=VB,maxiter = maxiter, cxx=cxx, df=df)  #
  if (!onlylogLR){#
      n=length(betahat)#
      if (!multiseqoutput){#
          ZeroProb = rep(0,length=n)#
          NegativeProb = rep(0,length=n)#
      }#
      if (!minimaloutput){#
          PosteriorMean = rep(0,length=n)#
          PosteriorSD = rep(0,length=n)#
      }#
      if (!multiseqoutput){#
          ZeroProb[completeobs] = colSums(comppostprob(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)[comp_sd(pi.fit$g)==0,,drop=FALSE])#
          NegativeProb[completeobs] = cdf_post(pi.fit$g, 0, betahat[completeobs],sebetahat[completeobs],df) - ZeroProb[completeobs]#
      }#
      if (!minimaloutput){#
          PosteriorMean[completeobs] = postmean(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
          PosteriorSD[completeobs] = postsd(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
      }#
                                        #FOR MISSING OBSERVATIONS, USE THE PRIOR INSTEAD OF THE POSTERIOR#
      if (!multiseqoutput){#
          ZeroProb[!completeobs] = sum(mixprop(pi.fit$g)[comp_sd(pi.fit$g)==0])#
          NegativeProb[!completeobs] = mixcdf(pi.fit$g,0)#
          lfsr = compute_lfsr(NegativeProb,ZeroProb)#
      }#
      if (!minimaloutput){#
          PosteriorMean[!completeobs] = mixmean(pi.fit$g)#
          PosteriorSD[!completeobs] = mixsd(pi.fit$g)#
      }#
      if (!minimaloutput & !multiseqoutput){#
          PositiveProb = 1- NegativeProb-ZeroProb#
          lfsra = compute_lfsra(PositiveProb,NegativeProb,ZeroProb) #
          lfdr = ZeroProb#
          qvalue = qval.from.lfdr(lfdr)#
      }#
  }#
  if (!minimaloutput)#
      logLR = tail(pi.fit$loglik,1) - pi.fit$null.loglik#
  if(nonzeromean & is.null(df)){#
      #Adding back the nonzero mean#
      betahat[completeobs]= betahat[completeobs]+nonzeromean.fit$nonzeromean#
      pi.fit$g$mean =nonzeromean.fit$nonzeromean#
      PosteriorMean= PosteriorMean + nonzeromean.fit$nonzeromean      #
  }	   #
  if (onlylogLR)#
      return(list(fitted.g=pi.fit$g, logLR = logLR, df=df))#
  else if (minimaloutput)#
      return(list(fitted.g = pi.fit$g, lfsr = lfsr, fit = pi.fit,df=df))#
  else if (multiseqoutput)#
      return(list(fitted.g = pi.fit$g, logLR = logLR, PosteriorMean = PosteriorMean, PosteriorSD = PosteriorSD, call= match.call(),df=df))#
  else{#
      result = list(fitted.g = pi.fit$g, logLR = logLR, PosteriorMean = PosteriorMean, PosteriorSD = PosteriorSD, PositiveProb = PositiveProb, NegativeProb = NegativeProb, ZeroProb = ZeroProb, lfsr = lfsr,lfsra = lfsra, lfdr = lfdr, qvalue = qvalue, fit = pi.fit, lambda1 = lambda1, lambda2 = lambda2, call = match.call(), data = list(betahat = betahat, sebetahat=sebetahat),df=df)#
      class(result) = "ash"#
      return(result)#
  }#
}#
  #if(nsamp>0){#
  #  sample = posterior_sample(post,nsamp)#
  #}#
#
# #' @title Faster version of function ash#
# #'#
# #' @description This function has similar functionality as ash, but only returns some of the outputs.#
# #'#
# #' @param betahat, a p vector of estimates#
# #' @param sebetahat, a p vector of corresponding standard errors#
# #' @param nullcheck: whether to check that any fitted model exceeds the "null" likelihood in which all weight is on the first component#
# #' @param randomstart: bool, indicating whether to initialize EM randomly. If FALSE, then initializes to prior mean (for EM algorithm) or prior (for VBEM)#
# #' @param pointmass: bool, indicating whether to use a point mass at zero as one of components for a mixture distribution#
# #' @param onlylogLR: bool, indicating whether to use this function to get logLR. Skip posterior prob, posterior mean, lfdr...#
# #' @param prior: string, or numeric vector indicating Dirichlet prior on mixture proportions (defaults to "uniform", or 1,1...,1; also can be "nullbiased" 1,1/k-1,...,1/k-1 to put more weight on first component)#
# #' @param mixsd: vector of sds for underlying mixture components#
# #' @param VB: whether to use Variational Bayes to estimate mixture proportions (instead of EM to find MAP estimate)#
# #' @param gridmult: the multiplier by which the default grid values for mixsd differ by one another. (Smaller values produce finer grids)#
# #' @param g: the prior distribution for beta (usually estimated from the data; this is used primarily in simulated data to do computations with the "true" g)#
# #' @param cxx: flag to indicate whether to use the c++ (Rcpp) version#
# #'#
# #' @return a list with elements fitted.g is fitted mixture#
# #' logLR : logP(D|mle(pi)) - logP(D|null)#
# #'#
# #' @export#
# fast.ash = function(betahat,sebetahat, #
#                     nullcheck=TRUE,randomstart=FALSE, #
#                     pointmass = TRUE,    #
#                     prior=c("nullbiased","uniform"), #
#                     mixsd=NULL, VB=FALSE,gridmult=4,#
#                     g=NULL, cxx=TRUE,#
#                     onlylogLR = FALSE,df=NULL){#
#   #
#   if(onlylogLR){#
#     pointmass <- TRUE  #
#   }#
#   #
#   #If method is supplied, use it to set up defaults; provide warning if these default values#
#   #are also specified by user#
#   if(!is.numeric(prior)){#
#     prior = match.arg(prior)#
#   }#
#   #
#   if(length(sebetahat)==1){#
#     sebetahat = rep(sebetahat,length(betahat))#
#   }#
#   if(length(sebetahat) != length(betahat)){#
#     stop("Error: sebetahat must have length 1, or same length as betahat")#
#   }#
#   #
#   completeobs = (!is.na(betahat) & !is.na(sebetahat))#
#   if(sum(completeobs)==0){#
#     if(onlylogLR){#
#       return(list(pi=NULL, logLR = 0))#
#     }else{#
#       stop("Error: all input values are missing")#
#     }#
#   }  #
#   #
#   if(is.null(mixsd)){#
#     mixsd= autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)#
#   }#
#   if(pointmass){#
#     mixsd = c(0,mixsd)#
#   }#
#   #
#   k=length(mixsd)  #
#   null.comp = which.min(mixsd) #which component is the "null"#
#   #
#   if(!is.numeric(prior)){#
#     if(prior=="nullbiased"){ # set up prior to favour "null"#
#       prior = rep(1,k)#
#       prior[null.comp] = 10 #prior 10-1 in favour of null#
#     }else if(prior=="uniform"){#
#       prior = rep(1,k)#
#     }#
#   }#
#   #
#   if(length(prior)!=k | !is.numeric(prior)){#
#     stop("invalid prior specification")#
#   }#
#   #
#   if(missing(g)){#
#     pi = prior^2 #default is to initialize pi at prior (mean)#
#     if(randomstart){pi=rgamma(k,1,1)}#
#     pi=normalize(pi)#
#     g=normalmix(pi,rep(0,k),mixsd)#
#     maxiter = 5000#
#   } else {#
#     maxiter = 1; # if g is specified, don't iterate the EM #
#   }#
#   #
#   pi.fit=EMest(betahat[completeobs],sebetahat[completeobs],g,prior,null.comp=null.comp,nullcheck=nullcheck,VB=VB,maxiter = maxiter, cxx=cxx, df=df)  #
#   #
#   if(onlylogLR){#
#     logLR = tail(pi.fit$loglik,1) - pi.fit$null.loglik#
#     return(list(pi=pi.fit$pi, logLR = logLR))#
#   }else{#
#     #
#     n=length(betahat)#
#     PosteriorMean = rep(0,length=n)#
#     PosteriorSD=rep(0,length=n)#
#     #
#     if(is.null(df)){#
#       PosteriorMean[completeobs] = postmean(pi.fit$g,betahat[completeobs],sebetahat[completeobs])#
#       PosteriorSD[completeobs] =postsd(pi.fit$g,betahat[completeobs],sebetahat[completeobs]) #
#     }#
#     else{#
#       PosteriorMean[completeobs] = postmean_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
#       PosteriorSD[completeobs] =postsd_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
#     }#
#     #FOR MISSING OBSERVATIONS, USE THE PRIOR INSTEAD OF THE POSTERIOR#
#     PosteriorMean[!completeobs] = mixmean(pi.fit$g)#
#     PosteriorSD[!completeobs] =mixsd(pi.fit$g)  #
#     #
#     result = list(fitted.g=pi.fit$g,PosteriorMean = PosteriorMean,PosteriorSD=PosteriorSD,call=match.call(),data=list(betahat = betahat, sebetahat=sebetahat))#
#     return(result)#
#   }#
#   #if(nsamp>0){#
#   #  sample = posterior_sample(post,nsamp)#
#   #}#
# }#
compute_lfsr = function(NegativeProb,ZeroProb){#
  ifelse(NegativeProb> 0.5*(1-ZeroProb),1-NegativeProb,NegativeProb+ZeroProb)#
}#
#
compute_lfsra = function(PositiveProb, NegativeProb,ZeroProb){#
  ifelse(PositiveProb<NegativeProb,2*PositiveProb+ZeroProb,2*NegativeProb+ZeroProb)  #
}  #
#
#' @title Estimate unimodal nonzero mean of a mixture model by EM algorithm#
#'#
#' @description Given the data, standard error of the data and standard deviations of the Gaussian mixture model, estimate the mean of a unimodal Gaussian mixture by an EM algorithm.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates unimodal mean \eqn{\mu} by EM algorithm. Uses the SQUAREM package to accelerate convergence of EM. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' #
#' @param betahat, a p vector of estimates #
#' @param sebetahat, a p vector of corresponding standard errors#
#' @param mixsd: vector of sds for underlying mixture components #
#' @param pi.init, the initial value of \eqn{\pi} to use. If not specified defaults to (1/k,...,1/k).#
#' @param tol, the tolerance for convergence of log-likelihood.#
#' @param maxiter the maximum number of iterations performed#
#' #
#' @return A list, including the estimates (\eqn{\mu}) and (\eqn{\pi}), the log likelihood for each iteration (NQ)#
#' and a flag to indicate convergence#
#'  #
#' @export#
#' #
#' #
nonzeromeanEM = function(betahat, sebetahat, mixsd, pi.init=NULL,tol=1e-7,maxiter=5000){#
  if(is.null(pi.init)){#
    pi.init = rep(1/length(mixsd),length(mixsd))# Use as starting point for pi#
  }#
  mupi=c(mean(betahat),pi.init)#
  res=squarem(par=mupi,fixptfn=nonzeromeanEMfixpoint,objfn=nonzeromeanEMobj,betahat=betahat,sebetahat=sebetahat,mixsd=mixsd,control=list(maxiter=maxiter,tol=tol))#
  return(list(nonzeromean=res$par[1],pi=res$par[-1],NQ=-res$value.objfn,niter = res$iter, converged=res$convergence,post=res$par))#
}#
#
nonzeromeanEMfixpoint = function(mupi,betahat,sebetahat,mixsd){#
	#omegamatrix=matrix(NA,nrow=length(betahat),ncol=length(mixsd))#
	mu=mupi[1]#
	pimean=mupi[-1]#
	sdmat = sqrt(outer(sebetahat ^2,mixsd^2,"+")) #
	xmat=matrix(rep(betahat,length(mixsd)),ncol=length(mixsd))#
	omegamatrix=t(t(dnorm(xmat,mean=mu,sd=sdmat))*pimean)#
	omegamatrix=omegamatrix /rowSums(omegamatrix)#
	pinew=normalize(colSums(omegamatrix))#
	munew=sum(omegamatrix*xmat/(sdmat^2))/sum(omegamatrix/(sdmat^2))#
	mupi=c(munew,pinew)#
	return(mupi)#
}#
#
nonzeromeanEMobj = function(mupi,betahat,sebetahat,mixsd){#
	mu=mupi[1]#
	pimean=mupi[-1]#
	sdmat = sqrt(outer(sebetahat ^2,mixsd^2,"+")) #
	xmat=matrix(rep(betahat,length(mixsd)),ncol=length(mixsd))#
	omegamatrix=t(t(dnorm(xmat,mean=mu,sd=sdmat))*pimean)#
	omegamatrix=omegamatrix /rowSums(omegamatrix)#
	NegativeQ=-sum(omegamatrix*dnorm(xmat,mean=mu,sd=sdmat,log=TRUE))#
	return(NegativeQ)#
}#
#' @title Estimate posterior distribution on mixture proportions of a mixture model by a Variational Bayes EM algorithm#
#'#
#' @description Given the individual component likelihoods for a mixture model, estimates the posterior on #
#' the mixture proportions by an VBEM algorithm. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates posterior on mixture proportions \eqn{\pi} by Variational Bayes, #
#' with a Dirichlet prior on \eqn{\pi}. #
#' Algorithm adapted from Bishop (2009), Pattern Recognition and Machine Learning, Chapter 10.#
#' #
#' @param matrix_lik: a n by k matrix with (j,k)th element equal to \eqn{f_k(x_j)}.#
#' @param prior: a k vector of the parameters of the Dirichlet prior on \eqn{\pi}. Recommended to be rep(1,k)#
#' @param post.init: the initial value of the posterior parameters. If not specified defaults to the prior parameters.#
#' @param tol: the tolerance for convergence of log-likelihood bound.#
#' @param maxiter: the maximum number of iterations performed#
#' #
#' @return A list, whose components include point estimates (pihat), #
#' the parameters of the fitted posterior on \eqn{\pi} (pipost),#
#' the bound on the log likelihood for each iteration (B)#
#' and a flag to indicate convergence (converged).#
#'  #
#' @export#
#' #
mixVBEM = function(matrix_lik, prior, pi.init = NULL,tol=1e-7, maxiter=5000){#
  k=ncol(matrix_lik)#
  if(is.null(pi.init)){#
    pi.init = rep(1,k)# Use as starting point for pi#
  } #
  res = squarem(par=pi.init,fixptfn=VBfixpoint, objfn=VBnegpenloglik,matrix_lik=matrix_lik, prior=prior, control=list(maxiter=maxiter,tol=tol))#
  return(list(pihat = res$par/sum(res$par), B=res$value.objfn, niter = res$iter, converged=res$convergence,post=res$par))#
}#
VBfixpoint = function(pipost, matrix_lik, prior){  #
  n=nrow(matrix_lik)#
  k=ncol(matrix_lik)#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost*matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  pipostnew = colSums(classprob) + prior#
  return(pipostnew)#
}#
#
VBnegpenloglik=function(pipost,matrix_lik,prior){#
  return(-VBpenloglik(pipost,matrix_lik,prior))#
}#
#
VBpenloglik = function(pipost, matrix_lik, prior){#
  n=nrow(matrix_lik)#
  k=ncol(matrix_lik)#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost*matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  B= sum(classprob*log(avgpipost*matrix_lik),na.rm=TRUE) - diriKL(prior,pipost) - sum(classprob*log(classprob)) #
  return(B)#
}#
#' @title Estimate mixture proportions of a mixture model by EM algorithm#
#'#
#' @description Given the individual component likelihoods for a mixture model, estimates the mixture proportions by an EM algorithm.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates mixture proportions \eqn{\pi} by maximum likelihood, or by maximum a posteriori (MAP) estimation for a Dirichlet prior on $\pi$ #
#' (if a prior is specified).  Uses the SQUAREM package to accelerate convergence of EM. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' #
#' @param matrix_lik, a n by k matrix with (j,k)th element equal to \eqn{f_k(x_j)}.#
#' @param prior, a k vector of the parameters of the Dirichlet prior on \eqn{\pi}. Recommended to be rep(1,k)#
#' @param pi.init, the initial value of \eqn{\pi} to use. If not specified defaults to (1/k,...,1/k).#
#' @param tol, the tolerance for convergence of log-likelihood.#
#' @param maxiter the maximum number of iterations performed#
#' #
#' @return A list, including the estimates (pihat), the log likelihood for each interation (B)#
#' and a flag to indicate convergence#
#'  #
#' @export#
#' #
#' #
mixEM = function(matrix_lik, prior, pi.init = NULL,tol=1e-7, maxiter=5000){#
  if(is.null(pi.init)){#
    pi.init = rep(1/k,k)# Use as starting point for pi#
  } #
  res = squarem(par=pi.init,fixptfn=fixpoint, objfn=negpenloglik,matrix_lik=matrix_lik, prior=prior, control=list(maxiter=maxiter,tol=tol))#
  return(list(pihat = normalize(pmax(0,res$par)), B=res$value.objfn, #
              niter = res$iter, converged=res$convergence))#
}#
#
# helper functions used by mixEM#
normalize = function(x){return(x/sum(x))}#
#
fixpoint = function(pi, matrix_lik, prior){  #
  pi = normalize(pmax(0,pi)) #avoid occasional problems with negative pis due to rounding#
  m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
  m.rowsum = rowSums(m)#
  classprob = m/m.rowsum #an n by k matrix#
  pinew = normalize(colSums(classprob) + prior - 1)#
  return(pinew)#
}#
#
negpenloglik = function(pi,matrix_lik,prior){return(-penloglik(pi,matrix_lik,prior))}#
#
penloglik = function(pi, matrix_lik, prior){#
  pi = normalize(pmax(0,pi))#
  m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
  m.rowsum = rowSums(m)#
  loglik = sum(log(m.rowsum))#
  subset = (prior != 1.0)#
  priordens = sum((prior-1)[subset]*log(pi[subset]))#
  return(loglik+priordens)#
}#
#
#The kth element of this vector is the derivative #
#of the loglik for $\pi=(\pi_0,...,1-\pi_0,...)$ with respect to $\pi_0$ at $\pi_0=1$.#
gradient = function(matrix_lik){#
  n = nrow(matrix_lik)#
  grad = n - colSums(matrix_lik/matrix_lik[,1]) #
  return(grad)#
}#
#
# mixEM = function(matrix_lik, prior, pi.init = NULL,tol=0.0001, maxiter=5000){#
#   n=nrow(matrix_lik)#
#   k=ncol(matrix_lik)#
#   B = rep(0,maxiter)#
#   pi = pi.init#
#   if(is.null(pi.init)){#
#     pi = rep(1/k,k)# Use as starting point for pi#
#   } #
#   pi = ifelse(pi<1e-5,1e-5,pi) #set any estimates that are too small to be just very small#
#   pi = normalize(pi)#
#   #
#   loglik = rep(0,maxiter)#
#   priordens= rep(0,maxiter)#
#   m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
#   m.rowsum = rowSums(m)#
#   loglik[1] = sum(log(m.rowsum))#
#   priordens[1] = sum((prior-1)*log(pi)) #
#   classprob = m/m.rowsum #an n by k matrix#
#   i=1#
#   if(maxiter >= 2){#
#     for(i in 2:maxiter){  #
#       pi = colSums(classprob) + prior-1#
#       pi = ifelse(pi<1e-5,1e-5,pi) #set any estimates that are less than zero, which can happen with prior<1, to 0#
#       pi = normalize(pi)#
#         #
#       #Now re-estimate pi#
#       m  = t(pi * t(matrix_lik)) #
#       m.rowsum = rowSums(m)#
#       loglik[i] = sum(log(m.rowsum))#
#       priordens[i] = sum((prior-1)*log(pi)) #
#       classprob = m/m.rowsum#
#     #
#     #
#       if(abs(loglik[i]+priordens[i]-loglik[i-1]-priordens[i-1])<tol) break;#
#     }#
#   }#
#   converged=(abs(loglik[i]+priordens[i]-loglik[i-1]-priordens[i-1])<tol)#
#   if(!converged){#
#       warning("EM algorithm in function mixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
#   }#
#   return(list(pihat = pi, B=loglik[1:i], #
#               niter = i, converged=converged))#
# }#
#estimate mixture proportions of sigmaa by EM algorithm#
#prior gives the parameter of a Dirichlet prior on pi#
#(prior is used to encourage results towards smallest value of sigma when#
#likelihood is flat)#
#nullcheck indicates whether to check whether the loglike exceeds the null#
#(may not want to use if prior is used)#
#VB provides an approach to estimate the approximate posterior distribution#
#of mixture proportions of sigmaa by variational Bayes method#
#(use Dirichlet prior and approximate Dirichlet posterior)#
#if cxx TRUE use cpp version of R function mixEM#
EMest = function(betahat,sebetahat,g,prior,null.comp=1,nullcheck=TRUE,VB=FALSE, maxiter=5000, cxx=TRUE, df=NULL){ #
  pi.init = g$pi#
  k=ncomp(g)#
  n = length(betahat)#
  tol = min(0.1/n,1e-5) # set convergence criteria to be more stringent for larger samples#
  matrix_lik = t(compdens_conv(g,betahat,sebetahat,df))#
  #checks whether the gradient at pi0=1 is positive (suggesting that this is a fixed point)#
  #if(nullcheck){#
  #  if(all(gradient(matrix_lik)>=0)){#
  #    pi.init=rep(0,k)#
  #    pi.init[null.comp]=1 #this will make pi.init=(1,0,0...,0) which is a fixed point of the EM#
  #  }#
  #}#
  if(VB==TRUE){#
    EMfit=mixVBEM(matrix_lik,prior,maxiter=maxiter)}#
  else{#
    if (cxx==TRUE){#
      EMfit = cxxMixEM(matrix_lik,prior,pi.init,1e-5, maxiter) #currently use different convergence criteria for cxx version #
      if(!EMfit$converged){#
        warning("EM algorithm in function cxxMixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
      }#
    }#
    else{#
      EMfit = mixEM(matrix_lik,prior,pi.init,tol, maxiter)#
      if(!EMfit$converged & !(maxiter==1)){#
        warning("EM algorithm in function mixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
      }#
    }#
  }#
  pi = EMfit$pihat     #
  penloglik = EMfit$B #
  converged = EMfit$converged#
  niter = EMfit$niter#
  loglik.final =  penloglik(pi,matrix_lik,1) #compute penloglik without penalty#
  null.loglik = sum(log(matrix_lik[,null.comp]))  #
  if(nullcheck==TRUE & VB==FALSE){ #null check doesn't work with VB yet#
    pinull = rep(0,k)#
    pinull[null.comp]=1#
    null.penloglik = penloglik(pinull,matrix_lik,prior)#
    final.penloglik = penloglik(pi,matrix_lik,prior)#
    if(null.penloglik > final.penloglik){ #check whether exceeded "null" likelihood where everything is null#
      pi=pinull#
      loglik.final=penloglik(pi,matrix_lik,1)#
    }#
  }#
  g$pi=pi#
  return(list(loglik=loglik.final,null.loglik=null.loglik,#
              matrix_lik=matrix_lik,converged=converged,g=g))#
}#
#' @title Compute Posterior#
#'#
#' @description Return the posterior on beta given a prior (g) that is a mixture of normals (class normalmix) #
#' and observation betahat \sim N(beta,sebetahat)#
#'#
#' @details This can be used to obt#
#'#
#' @param g: a normalmix with components indicating the prior; works only if g has means 0#
#' @param betahat (n vector of observations) #
#' @param sebetahat (n vector of standard errors/deviations of observations)#
#' #
#' @return A list, (pi1,mu1,sigma1) whose components are each k by n matrices#
#' where k is number of mixture components in g, n is number of observations in betahat#
#' #
#' @export#
#' #
#' #
posterior_dist = function(g,betahat,sebetahat){#
  if(class(g)!="normalmix"){#
    stop("Error: posterior_dist implemented only for g of class normalmix")#
  }#
  pi0 = g$pi#
  mu0 = g$mean#
  sigma0 = g$sd  #
  k= length(pi0)#
  n= length(betahat)#
  if(!all.equal(g$mean,rep(0,k))) stop("Error: posterior_dist currently only implemented for zero-centered priors")#
  pi1 = pi0 * t(matrix_dens(betahat,sebetahat,sigma0))#
  pi1 = apply(pi1, 2, normalize) #pi1 is now an k by n matrix#
  #make k by n matrix versions of sigma0^2 and sebetahat^2#
  # and mu0 and betahat#
  s0m2 = matrix(sigma0^2,nrow=k,ncol=n,byrow=FALSE)#
  sebm2 = matrix(sebetahat^2,nrow=k,ncol=n, byrow=TRUE)#
  mu0m = matrix(mu0,nrow=k,ncol=n,byrow=FALSE)#
  bhatm = matrix(betahat,nrow=k,ncol=n,byrow=TRUE)#
  sigma1 = (s0m2*sebm2/(s0m2 + sebm2))^(0.5)  #
  w = sebm2/(s0m2 + sebm2)#
  mu1 = w*mu0m + (1-w)*bhatm#
  #WHERE DATA ARE MISSING, SET POSTERIOR = PRIOR#
  ismiss = (is.na(betahat) | is.na(sebetahat)) #
  pi1[,ismiss] = pi0#
  mu1[,ismiss] = mu0#
  sigma1[,ismiss] = sigma0#
  return(list(pi=pi1,mu=mu1,sigma=sigma1))#
}#
#
#return matrix of densities of observations (betahat) #
# assuming betahat_j \sim N(0, sebetahat_j^2 + sigmaavec_k^2)#
#normalized by maximum of each column#
#INPUT#
#betahat is n vector, #
#sebetahat is n vector, #
#sigmaavec is k vector#
#return is n by k matrix of the normal likelihoods, #
# with (j,k)th element the density of N(betahat_j; mean=0, var = sebetahat_j^2 + sigmaavec_k^2)#
#normalized to have maximum 1 in each column#
matrix_dens = function(betahat, sebetahat, sigmaavec){#
  k = length(sigmaavec)#
  n = length(betahat)#
  ldens = dnorm(betahat,0,sqrt(outer(sebetahat^2,sigmaavec^2,FUN="+")),log=TRUE)#
  maxldens = apply(ldens, 1, max)#
  ldens = ldens - maxldens#
  return(exp(ldens))#
}#
#
#return the "effective" estimate#
#that is the effect size betanew whose z score betanew/se#
#would give the same p value as betahat/se compared to a t with df#
effective.effect=function(betahat,se,df){#
  p = pt(betahat/se,df)#
  qnorm(p,sd=se)#
}#
#' @title Function to compute q values from local false discovery rates#
#'#
#' @description Computes q values from a vector of local fdr estimates#
#'#
#' @details The q value for a given lfdr is an estimate of the (tail) False Discovery Rate #
#' for all findings with a smaller lfdr, and is found by the average of the lfdr for#
#' all more significant findings. See Storey (2003), Annals of Statistics, for definition of q value.  #
#' #
#' #
#' @param lfdr, a vector of local fdr estimates#
#'#
#' @return vector of q values#
#' #
#' @export#
qval.from.lfdr = function(lfdr){#
  o = order(lfdr)#
  qvalue=rep(NA,length(lfdr))#
  qvalue[o] = (cumsum(sort(lfdr))/(1:sum(!is.na(lfdr))))#
  return(qvalue)#
}#
#
# try to select a default range for the sigmaa values#
# that should be used, based on the values of betahat and sebetahat#
# mult is the multiplier by which the sds differ across the grid#
autoselect.mixsd = function(betahat,sebetahat,mult){#
  sigmaamin = min(sebetahat)/10 #so that the minimum is small compared with measurement precision#
  if(all(betahat^2<sebetahat^2)){#
    sigmaamax = 8*sigmaamin #to deal with the occassional odd case where this could happen; 8 is arbitrary#
  } else {#
    sigmaamax = 2*sqrt(max(betahat^2-sebetahat^2)) #this computes a rough largest value you'd want to use, based on idea that sigmaamax^2 + sebetahat^2 should be at least betahat^2   #
  }#
  if(mult==0){#
    return(c(0,sigmaamax/2))#
  }else{#
    npoint = ceiling(log2(sigmaamax/sigmaamin)/log2(mult))#
    return(mult^((-npoint):0) * sigmaamax)#
  }#
}#
#return the KL-divergence between 2 dirichlet distributions#
#p,q are the vectors of dirichlet parameters of same lengths#
diriKL = function(p,q){#
  p.sum = sum(p)#
  q.sum = sum(q)#
  k = length(q)#
  KL = lgamma(q.sum)-lgamma(p.sum)+sum((q-p)*(digamma(q)-digamma(rep(q.sum,k))))+sum(lgamma(p)-lgamma(q))#
  return(KL)#
}#
#
#helper function for VBEM#
VB.update = function(matrix_lik, pipost){#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost * matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  B = sum(classprob*log(avgpipost*matrix_lik),na.rm=TRUE) - diriKL(prior,pipost) #negative free energy#
  return(list(classprob=classprob,B=B))#
}
GENERIC FUNCTIONS #############################
# find matrix of densities at y, for each component of the mixture#
# INPUT y is an n-vector#
# OUTPUT k by n matrix of densities#
compdens = function(x,y,log=FALSE){#
  UseMethod("compdens")#
}#
compdens.default = function(x,y,log=FALSE){#
  stop(paste("Invalid class", class(m), "for first argument in",  match.call()))  #
}#
#
#standard deviations#
comp_sd = function(m){#
  UseMethod("comp_sd")#
}#
comp_sd.default = function(m){#
  stop("method comp_sd not written for this class")#
}#
#
#second moments#
comp_mean2 = function(m){#
  UseMethod("comp_mean2")#
}#
comp_mean2.default = function(m){#
  comp_sd(m)^2 + comp_mean(m)^2#
}#
#return the overall mean of the mixture#
mixmean = function(m){#
  UseMethod("mixmean")#
}#
mixmean.default = function(m){#
  sum(m$pi * comp_mean(m))#
}#
#
#return the overall second moment of the mixture#
mixmean2 = function(m){#
  UseMethod("mixmean2")#
}#
mixmean2.default = function(m){#
  sum(m$pi * comp_mean2(m))#
}#
#
#return the overall sd of the mixture#
mixsd = function(m){#
  UseMethod("mixsd")#
}#
mixsd.default = function(m){#
  sqrt(mixmean2(m)-mixmean(m)^2)#
}#
#
#means#
comp_mean = function(m){#
  UseMethod("comp_mean")#
}#
comp_mean.default = function(m){#
  stop("method comp_mean not written for this class")#
}#
#
#number of components#
ncomp = function(m){#
  UseMethod("ncomp")#
}#
ncomp.default = function(m){#
  return(length(m$pi))#
}#
#
#return mixture proportions, a generic function#
mixprop = function(m){#
  UseMethod("mixprop")#
}#
mixprop.default = function(m){#
  m$pi#
}#
#
#' @title mixcdf#
#'#
#' @description Returns cdf for a mixture (generic function)#
#' #
#' @details None#
#' #
#' @param x a mixture (eg of type normalmix or unimix)#
#' @param y locations at which cdf to be computed#
#' @param lower.tail: boolean indicating whether to report lower tail#
#' #
#' @return an object of class normalmix#
#' #
#' @export#
#' #
#' @examples mixcdf(normalmix(c(0.5,0.5),c(0,0),c(1,2)),seq(-4,4,length=100))#
#' #
mixcdf = function(x,y,lower.tail=TRUE){#
  UseMethod("mixcdf")#
}#
#' @title mixcdf.default#
#' @export#
#' #
mixcdf.default = function(x,y,lower.tail=TRUE){#
  x$pi %*% comp_cdf(x,y,lower.tail)#
}#
#
#find cdf for each component, a generic function#
comp_cdf = function(x,y,lower.tail=TRUE){#
  UseMethod("comp_cdf")#
}#
comp_cdf.default = function(x,y,lower.tail=TRUE){#
  stop("comp_cdf not implemented for this class")#
}#
#find density at y, a generic function#
dens = function(x,y){#
  UseMethod("dens")#
}#
dens.default = function(x,y){#
  return (x$pi %*% compdens(x, y))#
}#
#
#find log likelihood of data in x (a vector) for mixture in m#
loglik = function(m,x){#
  UseMethod("loglik")#
}#
loglik.default = function(m,x){#
  sum(log(dens(m,x)))#
}#
#
#find log likelihood of data in betahat, when #
#the mixture m is convolved with a normal with sd betahatsd#
#betahatsd is an n vector#
#betahat is an n vector#
#v is the degree of freedom#
#' @title loglik_conv#
#' #
#' @export#
#' #
loglik_conv = function(m,betahat,betahatsd,v,FUN="+"){#
  UseMethod("loglik_conv")#
}#
#' @title loglik_conv.default#
#' #
#' @export#
#' #
loglik_conv.default = function(m,betahat,betahatsd,v,FUN="+"){#
  sum(log(dens_conv(m,betahat,betahatsd,v,FUN)))#
}#
#
#compute the density of the components of the mixture m#
#when convoluted with a normal with standard deviation s#
#or a scaled (se) student.t with df v#
#the density is evaluated at x#
#x and s are n-vectors#
#m is a mixture with k components#
#output is a k by n matrix of densities#
compdens_conv = function(m,x,s,v,FUN="+"){#
  UseMethod("compdens_conv")#
}#
compdens_conv.default = function(m,x,s,v,FUN="+"){#
  stop(paste("Invalid class", class(m), "for first argument in",  match.call()))  #
}#
#
#compute density of mixture m convoluted with normal of sd (s) or student t with df v#
#at locations x#
#m is a mixture#
#x is an n vector#
#s is an n vector or integer#
dens_conv = function(m,x,s,v,FUN="+"){#
  UseMethod("dens_conv")#
}#
dens_conv.default = function(m,x,s,v,FUN="+"){#
  colSums(m$pi * compdens_conv(m,x,s,v,FUN))#
}#
#
#compute the posterior prob that each observation#
#came from each component of the mixture m#
#output a k by n vector of probabilities#
#computed by weighting the component densities by pi#
#and then normalizing#
comppostprob=function(m,x,s,v){#
 UseMethod("comppostprob") #
}#
comppostprob.default = function(m,x,s,v){#
  tmp= (t(m$pi * compdens_conv(m,x,s,v))/dens_conv(m,x,s,v))#
  ismissing = (is.na(x) | is.na(s))#
  tmp[ismissing,]=m$pi#
  t(tmp)#
}#
# evaluate cdf of posterior distribution of beta at c#
# m is the prior on beta, a mixture#
# c is location of evaluation#
# assumption is betahat | beta \sim N(beta,sebetahat)#
# m is a mixture with k components#
# c a scalar#
# betahat, sebetahat are n vectors #
# output is a k by n matrix#
compcdf_post=function(m,c,betahat,sebetahat,v){#
  UseMethod("compcdf_post")#
}#
compcdf_post.default=function(m,c,betahat,sebetahat,v){#
  stop("method compcdf_post not written for this class")#
}#
cdf_post = function(m,c,betahat,sebetahat,v){#
  UseMethod("cdf_post")#
}#
cdf_post.default=function(m,c,betahat,sebetahat,v){#
  colSums(comppostprob(m,betahat,sebetahat,v)*compcdf_post(m,c,betahat,sebetahat,v))#
}#
#
#output posterior mean for beta for prior mixture m,#
#given observations betahat, sebetahat, df v#
postmean = function(m, betahat,sebetahat,v){#
  UseMethod("postmean")#
}#
postmean.default = function(m,betahat,sebetahat,v){#
  colSums(comppostprob(m,betahat,sebetahat,v) * comp_postmean(m,betahat,sebetahat,v))#
}#
#output posterior mean-squared value for beta for prior mixture m,#
#given observations betahat, sebetahat, df v#
postmean2 = function(m, betahat,sebetahat,v){#
  UseMethod("postmean2")#
}#
postmean2.default = function(m,betahat,sebetahat,v){#
  colSums(comppostprob(m,betahat,sebetahat,v) * comp_postmean2(m,betahat,sebetahat,v))#
}#
#
#output posterior sd for beta for prior mixture m,#
#given observations betahat, sebetahat, df v#
postsd = function(m,betahat,sebetahat,v){#
  UseMethod("postsd")#
}#
postsd.default = function(m,betahat,sebetahat,v){#
  sqrt(postmean2(m,betahat,sebetahat,v)-postmean(m,betahat,sebetahat,v)^2)#
}#
#
#output posterior mean-squared value for beta for prior mixture m,#
#given observations betahat, sebetahat, df v#
comp_postmean2 = function(m,betahat,sebetahat,v){#
  UseMethod("comp_postmean2")#
}#
comp_postmean2.default = function(m,betahat,sebetahat,v){#
  comp_postsd(m,betahat,sebetahat,v)^2 + comp_postmean(m,betahat,sebetahat,v)^2#
}#
#output posterior mean for beta for each component of prior mixture m,#
#given observations betahat, sebetahat, df v#
comp_postmean = function(m, betahat,sebetahat,v){#
  UseMethod("comp_postmean")#
}#
comp_postmean.default = function(m,betahat,sebetahat,v){#
  stop("method comp_postmean not written for this class")#
}#
#output posterior sd for beta for each component of prior mixture m,#
#given observations betahat, sebetahat, df v#
comp_postsd = function(m, betahat,sebetahat,v){#
  UseMethod("comp_postsd")#
}#
comp_postsd.default = function(m,betahat,sebetahat,v){#
  stop("method comp_postsd not written for this class")#
}#
#
#find nice limits of mixture m for plotting#
min_lim = function(m){#
  UseMethod("min_lim")#
}#
min_lim.default=function(m){#
  -5#
}#
#
max_lim = function(m){#
  UseMethod("max_lim")#
}#
max_lim.default=function(m){#
  5#
}#
#plot density of mixture#
plot_dens = function(m,npoints=100,...){#
  UseMethod("plot_dens")#
}#
plot_dens.default = function(m,npoints=100,...){#
  x = seq(min_lim(m),max_lim(m),length=npoints)#
  plot(x,dens(m,x),type="l",xlab="density",ylab="x",...)#
}#
#
plot_post_cdf = function(m,betahat,sebetahat,v,npoints=100,...){#
  UseMethod("plot_post_cdf")#
}#
plot_post_cdf.default = function(m,betahat,sebetahat,v,npoints=100,...){#
  x = seq(min_lim(m),max_lim(m),length=npoints)#
  x_cdf = vapply(x,cdf_post,FUN.VALUE=betahat,m=m,betahat=betahat,sebetahat=sebetahat,v=v)#
  plot(x,x_cdf,type="l",xlab="x",ylab="cdf",...)#
 # for(i in 2:nrow(x_cdf)){#
 #   lines(x,x_cdf[i,],col=i)#
 # }#
}#
#
############################### METHODS FOR normalmix class ############################
#
#' @title Constructor for normalmix class#
#'#
#' @description Creates an object of class normalmix (finite mixture of univariate normals)#
#' #
#' @details None#
#' #
#' @param pi vector of mixture proportions#
#' @param mean vector of means#
#' @param sd: vector of standard deviations#
#' #
#' @return an object of class normalmix#
#' #
#' @export#
#' #
#' @examples normalmix(c(0.5,0.5),c(0,0),c(1,2))#
#' #
normalmix = function(pi,mean,sd){#
  structure(data.frame(pi,mean,sd),class="normalmix")#
}#
#
comp_sd.normalmix = function(m){#
  m$sd#
}#
#
comp_mean.normalmix = function(m){#
  m$mean#
}#
#
compdens.normalmix = function(x,y,log=FALSE){#
  k=ncomp(x)#
  n=length(y)#
  d = matrix(rep(y,rep(k,n)),nrow=k)#
  return(matrix(dnorm(d, x$mean, x$sd, log),nrow=k))  #
}#
#
#density of convolution of each component of a normal mixture with N(0,s^2) or s*t(v) at x#
# x an n-vector at which density is to be evaluated#
#return a k by n matrix#
#Note that convolution of two normals is normal, so it works that way#
compdens_conv.normalmix = function(m,x,s,v,FUN="+"){#
  if(!is.null(v)){#
  	stop("method comp_postsd of normal mixture not written for df!null")#
  }#
  if(length(s)==1){s=rep(s,length(x))}#
  sdmat = sqrt(outer(s^2,m$sd^2,FUN)) #n by k matrix of standard deviations of convolutions#
  return(t(dnorm(outer(x,m$mean,FUN="-")/sdmat)/sdmat))#
}#
comp_cdf.normalmix = function(x,y,lower.tail=TRUE){#
  vapply(y,pnorm,x$mean,x$mean,x$sd,lower.tail)#
}#
#
#c is a scalar#
#m a mixture with k components#
#betahat a vector of n observations#
#sebetahat an n vector of standard errors#
#return a k by n matrix of the posterior cdf#
compcdf_post.normalmix=function(m,c,betahat,sebetahat,v){#
  if(!is.null(v)){#
  	stop("Error: normal mixture for student-t likelihood is not yet implemented")#
  }  #
  k = length(m$pi)#
  n=length(betahat)#
  #compute posterior standard deviation (s1) and posterior mean (m1)#
  s1 = sqrt(outer(sebetahat^2,m$sd^2,FUN="*")/outer(sebetahat^2,m$sd^2,FUN="+"))#
  ismissing = (is.na(betahat) | is.na(sebetahat))#
  s1[ismissing,]=m$sd#
  m1 = t(comp_postmean(m,betahat,sebetahat,v))#
  t(pnorm(c,mean=m1,sd=s1))#
}#
#
#return posterior mean for each component of prior m, given observations betahat and sebetahat#
#input, m is a mixture with k components#
#betahat, sebetahat are n vectors#
#output is a k by n matrix#
comp_postmean.normalmix = function(m,betahat,sebetahat,v){#
  if(!is.null(v)){#
  	stop("method comp_postmean of normal mixture not written for df!=NULL")#
  }#
  tmp=(outer(sebetahat^2,m$mean, FUN="*") + outer(betahat,m$sd^2, FUN="*"))/outer(sebetahat^2,m$sd^2,FUN="+")#
  ismissing = (is.na(betahat) | is.na(sebetahat))#
  tmp[ismissing,]=m$mean #return prior mean when missing data#
  t(tmp)#
}#
#return posterior standard deviation for each component of prior m, given observations betahat and sebetahat#
#input, m is a mixture with k components#
#betahat, sebetahat are n vectors#
#output is a k by n matrix#
comp_postsd.normalmix = function(m,betahat,sebetahat,v){#
  if(!is.null(v)){#
  	stop("method comp_postsd of normal mixture not written for df!=NULL")#
  }#
  t(sqrt(outer(sebetahat^2,m$sd^2,FUN="*")/outer(sebetahat^2,m$sd^2,FUN="+")))#
}#
############################### METHODS FOR unimix class ############################
#
#constructor; pi, a and b are vectors; kth component is Uniform(a[k],b[k])#
unimix = function(pi,a,b){#
  structure(data.frame(pi,a,b),class="unimix")#
}#
#
comp_cdf.unimix = function(m,y,lower.tail=TRUE){#
  vapply(y,punif,m$a,min=m$a,max=m$b,lower.tail)#
}#
#
comp_sd.unimix = function(m){#
  (m$b-m$a)/sqrt(12)#
}#
#
comp_mean.unimix = function(m){#
  (m$a+m$b)/2#
}#
compdens.unimix = function(x,y,log=FALSE){#
  k=ncomp(x)#
  n=length(y)#
  d = matrix(rep(y,rep(k,n)),nrow=k)#
  return(matrix(dunif(d, x$a, x$b, log),nrow=k))  #
}#
#
#density of convolution of each component of a unif mixture with N(0,s) at x#
# x an n-vector#
#return a k by n matrix#
compdens_conv.unimix = function(m,x,s,v, FUN="+"){#
  if(FUN!="+") stop("Error; compdens_conv not implemented for uniform with FUN!=+")#
  if(is.null(v)){#
    compdens= t(pnorm(outer(x,m$a,FUN="-")/s)-pnorm(outer(x,m$b,FUN="-")/s))/(m$b-m$a)#
    compdens[m$a==m$b,]=t(dnorm(outer(x,m$a,FUN="-")/s)/s)[m$a==m$b,]#
  }#
  else{#
    compdens= t(pt(outer(x,m$a,FUN="-")/s,df=v)-pt(outer(x,m$b,FUN="-")/s,df=v))/(m$b-m$a)#
    compdens[m$a==m$b,]=t(dt(outer(x,m$a,FUN="-")/s,df=v)/s)[m$a==m$b,]#
  }#
  return(compdens)#
}#
#c is a scalar#
#m a mixture with k components#
#betahat a vector of n observations#
#sebetahat an n vector of standard errors#
#return a k by n matrix of the posterior cdf#
compcdf_post.unimix=function(m,c,betahat,sebetahat,v){#
  k = length(m$pi)#
  n=length(betahat)#
  tmp = matrix(1,nrow=k,ncol=n)#
  tmp[m$a > c,] = 0#
  subset = m$a<=c & m$b>c # subset of components (1..k) with nontrivial cdf#
  if(sum(subset)>0){#
  	if(is.null(v)){#
      pna = pnorm(outer(betahat,m$a[subset],FUN="-")/sebetahat)#
      pnc = pnorm(outer(betahat,rep(c,sum(subset)),FUN="-")/sebetahat)#
      pnb = pnorm(outer(betahat,m$b[subset],FUN="-")/sebetahat)#
    }#
    else{#
      pna = pt(outer(betahat,m$a[subset],FUN="-")/sebetahat, df=v)#
      pnc = pt(outer(betahat,rep(c,sum(subset)),FUN="-")/sebetahat, df=v)#
      pnb = pt(outer(betahat,m$b[subset],FUN="-")/sebetahat, df=v)#
    }#
    tmp[subset,] = t((pnc-pna)/(pnb-pna))#
  }#
  subset = (m$a == m$b) #subset of components with trivial cdf#
  tmp[subset,]= rep(m$a[subset] <= c,n)#
  tmp#
}#
#
my_etruncnorm= function(a,b,mean=0,sd=1){#
  alpha = (a-mean)/sd#
  beta =  (b-mean)/sd#
 #Flip the onese where both are positive, as the computations are more stable#
  #when both negative#
  flip = (alpha>0 & beta>0)#
  flip[is.na(flip)]=FALSE #deal with NAs#
  alpha[flip]= -alpha[flip]#
  beta[flip]=-beta[flip]#
  tmp= (-1)^flip * (mean+sd*etruncnorm(alpha,beta,0,1))#
  max_alphabeta = ifelse(alpha<beta, beta,alpha)#
  max_ab = ifelse(alpha<beta,b,a)#
  toobig = max_alphabeta<(-30)#
  toobig[is.na(toobig)]=FALSE #
  tmp[toobig] = max_ab[toobig]#
  tmp#
}#
#return posterior mean for each component of prior m, given observations betahat and sebetahat#
#input, m is a mixture with k components#
#betahat, sebetahat are n vectors#
#output is a k by n matrix#
#note that with uniform prior, posterior is truncated normal, so#
#this is computed using formula for mean of truncated normal #
comp_postmean.unimix = function(m,betahat,sebetahat,v){#
#   k= ncomp(m)#
#   n=length(betahat)#
#   a = matrix(m$a,nrow=n,ncol=k,byrow=TRUE)#
#   b = matrix(m$b,nrow=n,ncol=k,byrow=TRUE)#
#   matrix(etruncnorm(a,b,betahat,sebetahat),nrow=k,byrow=TRUE)#
  #note: etruncnorm is more stable for a and b negative than positive#
  #so maybe use this, and standardize to make the whole more stable.#
  alpha = outer(-betahat, m$a,FUN="+")/sebetahat#
  beta = outer(-betahat, m$b, FUN="+")/sebetahat#
  if(is.null(v)){#
    tmp = betahat + sebetahat*my_etruncnorm(alpha,beta,0,1)#
  }#
  else{#
  	tmp = betahat + sebetahat*my_etrunct(alpha,beta,v)#
  }#
  ismissing = is.na(betahat) | is.na(sebetahat)#
  tmp[ismissing,]= (m$a+m$b)/2#
  t(tmp)#
#   t(#
#     betahat + sebetahat* #
#       exp(dnorm(alpha,log=TRUE)- pnorm(alpha,log=TRUE))#
#    * #
#       (-expm1(dnorm(beta,log=TRUE)-dnorm(alpha,log=TRUE)))#
#     /#
#       (expm1(pnorm(beta,log=TRUE)-pnorm(alpha,log=TRUE)))#
#   )#
}#
#
#not yet implemented!#
#just returns 0s for now#
comp_postsd.unimix = function(m,betahat,sebetahat,v){#
  print("Warning: Posterior SDs not yet implemented for uniform components")#
  k= ncomp(m)#
  n=length(betahat)#
  return(matrix(NA,nrow=k,ncol=n)) #
}#
#
# the mean of a truncated student.t#
# the result is from the paper 'Moments of truncated Student-t distribution' by H.-J Kim #
#
my_etrunct= function(a,b,v){#
  A = v+a^2#
  B = v+b^2#
  F_a = pt(a,df=v)#
  F_b = pt(b,df=v)#
  G = gamma((v-1)/2)*v^(v/2)/(2*(F_b-F_a)*gamma(v/2)*gamma(1/2))#
  tmp = ifelse(a==b,a,G*(A^(-(v-1)/2)-B^(-(v-1)/2)))#
  tmp#
}
set.seed(200)#
test=simdata(100,betahatsd=1,nullproportion=0.5,altmean=2,altsd=1)#
print(test$beta)#
test.ash2=ash(test$betahat,test$betahatsd,method="shrink",nonzeromean=TRUE)
test.ash1=ash(test$betahat,test$betahatsd,method="shrink",nonzeromean=TRUE)#
test.ash2=ash(test$betahat,test$betahatsd,method="shrink",nonzeromean=TRUE)
test.ash1
test.ash2
test.ash1=ash(test$betahat,test$betahatsd,method="shrink",nonzeromean=FALSE)#
test.ash2=ash(test$betahat,test$betahatsd,method="shrink",nonzeromean=TRUE)
test.ash1
test.ash2
attributes(test.ash1)
test.ash1$lfdr
test.ash2$lfdr
test.ash1$lfsr
test.ash2$lfsr
test.ash2$lfsr-test.ash1$lfsr
roxygen
roxygen2
library(roxygen2)
help roxygen2
R code to run some simulations#
#
require(ashr)#
require(qvalue)#
require(fdrtool)#
require(mixfdr)#
require(locfdr)#
require(ggplot2)#
#bsd gives standard deviation of beta#
#pi0 is simulated to be uniform on [minpi0,1]#
basicsim=function(mixsd,mixpi_alt,bsd=1,minpi0=0,seedval = 100,nsamp=1000,niter=50){  #
  set.seed(seedval)  #
  beta =list()#
  betahatsd=list()#
  betahat = list()#
  zscore = list()#
  pval = list()#
  betahat.ash.n = list()#
  betahat.ash.u = list()#
  betahat.ash.ur = list()#
  betahat.ash.npm = list()#
  betahat.ash.true = list()#
  betahat.qval = list()#
  betahat.fdrtool = list()#
  betahat.locfdr = list()#
  betahat.mixfdr = list()#
  pi0 = rep(0,niter)#
  for(i in 1:niter){#
    pi0[i]=runif(1,minpi0,1)#
    mixpi = c(pi0[i],(1-pi0[i])*mixpi_alt)#
    sd = sample(mixsd,nsamp,prob=mixpi,replace=TRUE )#
    beta[[i]] = rnorm(nsamp,0,sd)#
    betahatsd[[i]] = bsd#
    betahat[[i]] = beta[[i]]+rnorm(nsamp,0,betahatsd[[i]])#
    zscore[[i]] = betahat[[i]]/betahatsd[[i]]#
    pval[[i]] = pchisq(zscore[[i]]^2,df=1,lower.tail=F)#
    betahat.ash.n[[i]] = ash(betahat[[i]],betahatsd[[i]],pointmass=TRUE,prior="nullbiased",gridmult=2)#
    betahat.ash.u[[i]] = ash(betahat[[i]],betahatsd[[i]],pointmass=TRUE,prior="uniform",gridmult=2)#
    betahat.ash.ur[[i]] = ash(betahat[[i]],betahatsd[[i]],pointmass=TRUE,prior="uniform",randomstart=TRUE,gridmult=2)#
    betahat.ash.npm[[i]] = ash(betahat[[i]],betahatsd[[i]],pointmass=FALSE,prior="uniform",gridmult=2)#
    betahat.ash.true[[i]] = ash(betahat[[i]],betahatsd[[i]],g=normalmix(mixpi,rep(0,length(mixpi)),mixsd))#
    cat("applying q value\n")#
    betahat.qval[[i]] = qvalue(pval[[i]])#
    cat("applying fdrtool\n")#
    betahat.fdrtool[[i]] = fdrtool(pval[[i]],statistic="pvalue",plot=FALSE)#
    cat("applying locfdr\n")#
    betahat.locfdr[[i]] = try(locfdr(zscore[[i]],nulltype=0,plot=0))#
    cat("applying mixfdr\n")#
    betahat.mixfdr[[i]] = mixFdr(zscore[[i]],noiseSD=1,theonull=TRUE,plot=FALSE)#
  }#
  return(list(beta =beta,#
              betahatsd=betahatsd,#
              betahat = betahat,#
              zscore = zscore,#
              pval = pval,#
              betahat.ash.n = betahat.ash.n,#
              betahat.ash.u = betahat.ash.u,#
              betahat.ash.npm = betahat.ash.npm,#
              betahat.ash.true=betahat.ash.true,#
              betahat.qval = betahat.qval,#
              betahat.fdrtool = betahat.fdrtool,#
              betahat.locfdr = betahat.locfdr,#
              betahat.mixfdr = betahat.mixfdr,#
              pi0=pi0))#
}    #
#
mixsd = c(0,0.25,0.5,1,2)#
mixpi_alt = c(0.4,0.2,0.2,0.2) #mixture proportions under the alternative#
#
set.seed(100)#
#these are the simulations for scenarios 1a, 1b and 2 from the paper#
simres1a = basicsim(mixsd,mixpi_alt,niter=200,nsamp=1000)#
simres1b = basicsim(mixsd,mixpi_alt,niter=200,nsamp=10000)#
simres2= basicsim(c(0,4),c(1))#
#
#these do a situation where precision varies across measurements#
simres3=basicsim(c(0,2),c(1),bsd=c(rep(1,500),rep(10,500)))#
simres4=basicsim(c(0,2),c(1),bsd=c(rep(1,500),rep(10,500)),minpi0=0.9,seed=200)#
#
save.image(file="sim1.RData")
require(ashr)#
require(qvalue)#
require(fdrtool)#
require(mixfdr)#
require(locfdr)#
require(ggplot2)#
#bsd gives standard deviation of beta#
#pi0 is simulated to be uniform on [minpi0,1]#
basicsim=function(mixsd,mixpi_alt,bsd=1,minpi0=0,seedval = 100,nsamp=1000,niter=50){  #
  set.seed(seedval)  #
  beta =list()#
  betahatsd=list()#
  betahat = list()#
  zscore = list()#
  pval = list()#
  betahat.ash.n = list()#
  betahat.ash.u = list()#
  betahat.ash.ur = list()#
  betahat.ash.npm = list()#
  betahat.ash.true = list()#
  betahat.qval = list()#
  betahat.fdrtool = list()#
  betahat.locfdr = list()#
  betahat.mixfdr = list()#
  pi0 = rep(0,niter)#
  for(i in 1:niter){#
    pi0[i]=runif(1,minpi0,1)#
    mixpi = c(pi0[i],(1-pi0[i])*mixpi_alt)#
    sd = sample(mixsd,nsamp,prob=mixpi,replace=TRUE )#
    beta[[i]] = rnorm(nsamp,0,sd)#
    betahatsd[[i]] = bsd#
    betahat[[i]] = beta[[i]]+rnorm(nsamp,0,betahatsd[[i]])#
    zscore[[i]] = betahat[[i]]/betahatsd[[i]]#
    pval[[i]] = pchisq(zscore[[i]]^2,df=1,lower.tail=F)#
    betahat.ash.n[[i]] = ash(betahat[[i]],betahatsd[[i]],pointmass=TRUE,prior="nullbiased",gridmult=2)#
    betahat.ash.u[[i]] = ash(betahat[[i]],betahatsd[[i]],pointmass=TRUE,prior="uniform",gridmult=2)#
    betahat.ash.ur[[i]] = ash(betahat[[i]],betahatsd[[i]],pointmass=TRUE,prior="uniform",randomstart=TRUE,gridmult=2)#
    betahat.ash.npm[[i]] = ash(betahat[[i]],betahatsd[[i]],pointmass=FALSE,prior="uniform",gridmult=2)#
    betahat.ash.true[[i]] = ash(betahat[[i]],betahatsd[[i]],g=normalmix(mixpi,rep(0,length(mixpi)),mixsd))#
    cat("applying q value\n")#
    betahat.qval[[i]] = qvalue(pval[[i]])#
    cat("applying fdrtool\n")#
    betahat.fdrtool[[i]] = fdrtool(pval[[i]],statistic="pvalue",plot=FALSE)#
    cat("applying locfdr\n")#
    betahat.locfdr[[i]] = try(locfdr(zscore[[i]],nulltype=0,plot=0))#
    cat("applying mixfdr\n")#
    betahat.mixfdr[[i]] = mixFdr(zscore[[i]],noiseSD=1,theonull=TRUE,plot=FALSE)#
  }#
  return(list(beta =beta,#
              betahatsd=betahatsd,#
              betahat = betahat,#
              zscore = zscore,#
              pval = pval,#
              betahat.ash.n = betahat.ash.n,#
              betahat.ash.u = betahat.ash.u,#
              betahat.ash.npm = betahat.ash.npm,#
              betahat.ash.true=betahat.ash.true,#
              betahat.qval = betahat.qval,#
              betahat.fdrtool = betahat.fdrtool,#
              betahat.locfdr = betahat.locfdr,#
              betahat.mixfdr = betahat.mixfdr,#
              pi0=pi0))#
}
ls()
rm(list = ls())
ls()
getwd()
install.packages("/Users/daichaoxing/ash/package/ashr.no.cxx.tar.gz",repos=NULL,type="source")#
library("ashr")
help(ash)
beta = c(rep(0,100),rnorm(100))#
sebetahat = abs(rnorm(200,0,1))#
betahat = rnorm(200,beta,sebetahat)#
beta.ash = ash(betahat, sebetahat)#
summary(beta.ash)#
plot(betahat,beta.ash$PosteriorMean,xlim=c(-4,4),ylim=c(-4,4))#
#
betahat=betahat+1000#
beta.ash = ash(betahat, sebetahat)#
summary(beta.ash)#
plot(betahat,beta.ash$PosteriorMean,xlim=c(-4,4),ylim=c(-4,4))
plot(betahat,beta.ash$PosteriorMean)
remove.packages("ashr")
remove.packages("ashr")#
install.packages("/Users/daichaoxing/ash/package/ashr.no.cxx.tar.gz",repos=NULL,type="source")#
library("ashr")
help(ash)
ash
source*
source()
rm(list = ls())#
remove.packages("ashr")#
install.packages("/Users/daichaoxing/ash/package/ashr.no.cxx.tar.gz",repos=NULL,type="source")#
library("ashr")
help(ash)
rm(list = ls())#
remove.packages("ashr")#
install.packages("/Users/daichaoxing/ash/package/ashr.no.cxx.tar.gz",repos=NULL,type="source")#
library("ashr")
help(ash)
rm(list = ls())#
remove.packages("ashr")#
install.packages("/Users/daichaoxing/ash/package/ashr.no.cxx.tar.gz",repos=NULL,type="source")#
library("ashr")#
#library("ashr",lib.loc="/Users/daichaoxing/ash/package")
help(ash)
rm(list = ls())
install.packages("/Users/daichaoxing/ash/package/ashr.no.cxx.tar.gz",repos=NULL,type="source")#
library("ashr")
help(ash)
rm(list = ls())#
remove.packages("ashr")#
install.packages("/Users/daichaoxing/ash/package/ashr.no.cxx.tar.gz",repos=NULL,type="source")#
library("ashr")
help(ash)
rm(list = ls())
remove.packages("ashr")#
install.packages("/Users/daichaoxing/ash/package/ashr.no.cxx.tar.gz",repos=NULL,type="source")#
library("ashr")
help(ash)
require(roxygen2)#
require(devtools)#
setwd("/Users/daichaoxing/ash/package/ashr")#
#devtools::document()#
roxygenise()#
#
##Terminal Createpackage no cxx version#
cd#
cd ash/package#
sh create.ashr.no.cxx.sh#
tar -pczf ashr.tar.gz ashr#
sh create.ashr.no.cxx.sh
rm(list = ls())#
remove.packages("ashr")#
install.packages("/Users/daichaoxing/ash/package/ashr.no.cxx.tar.gz",repos=NULL,type="source")#
library("ashr")
simdata = function(n, betahatsd,nullproportion=1, altmean=1, altsd=1) {#
    nnull=round(n*nullproportion,0)#
    null = c(rep(1, nnull), rep(0, n - nnull))#
    beta = c(rep(0, nnull), rnorm(n - nnull, altmean, altsd))#
    betahat = rnorm(n, beta, betahatsd)#
    return(list(null = null, beta = beta, betahat = betahat, betahatsd = betahatsd))#
}#
## Testing the nonzeromean feature#
#
N=1000#
miu=500#
set.seed(miu)#
test=simdata(N,betahatsd=1,nullproportion=0.5,altmean=2,altsd=1)#
test.ash=ash(test$betahat,test$betahatsd,method="shrink")#
print(test.ash$fitted.g$mean)#
test$betahat=test$betahat+miu#
test.ashn=ash(test$betahat,test$betahatsd,method="shrink",nonzeromean=TRUE)#
print(test.ashn$fitted.g$mean)
N=1000#
miu=500#
set.seed(miu)#
test=simdata(N,betahatsd=1,nullproportion=0.5,altmean=2,altsd=1)#
test.ash=ash(test$betahat,test$betahatsd,method="shrink",mixcompdist="uniform")#
print(test.ash$fitted.g$a)#
print(test.ash$fitted.g$b)#
#
test$betahat=test$betahat+miu#
test.ash1=ash(test$betahat,test$betahatsd,method="shrink",nonzeromean=TRUE,mixcompdist="uniform")#
test.ash2=ash(test$betahat,test$betahatsd,method="shrink",nonzeromean=FALSE,mixcompdist="uniform")#
print(test.ash1)#
print(test.ash2)
simdata = function(n, betahatsd,nullproportion=1, altmean=1, altsd=1) {#
    nnull=round(n*nullproportion,0)#
    null = c(rep(1, nnull), rep(0, n - nnull))#
    beta = c(rep(0, nnull), rnorm(n - nnull, altmean, altsd))#
    betahat = rnorm(n, beta, betahatsd)#
    return(list(null = null, beta = beta, betahat = betahat, betahatsd = betahatsd))#
}#
#
N=1000#
miu=500#
set.seed(miu)#
test=simdata(N,betahatsd=1,nullproportion=0.5,altmean=2,altsd=1)#
test$betahat=test$betahat+miu#
betahat=test$betahat#
sebetahat=test$betahatsd
' @useDynLib ashr#
#todo#
#2014Sep10#
#' @title Main Adaptive SHrinkage function#
#'#
#' @description Takes vectors of estimates (betahat) and their standard errors (sebetahat), and applies#
#' shrinkage to them, using Empirical Bayes methods, to compute shrunk estimates for beta.#
#'#
#' @details See readme for more details#
#' #
#' @param betahat  a p vector of estimates #
#' @param sebetahat a p vector of corresponding standard errors#
#' @param method specifies how ash is to be run. Can be "shrinkage" (if main aim is shrinkage) or "fdr" (if main aim is to assess fdr or fsr)#
#' This is simply a convenient way to specify certain combinations of parameters: "shrinkage" sets pointmass=FALSE and prior="uniform";#
#' "fdr" sets pointmass=TRUE and prior="nullbiased".#
#' @param mixcompdist distribution of components in mixture ("normal", "uniform" or "halfuniform")#
#'#
#' @param lambda1  multiplicative "inflation factor" for standard errors (like Genomic Control)#
#' @param lambda2  additive "inflation factor" for standard errors (like Genomic Control)#
#' @param nullcheck  whether to check that any fitted model exceeds the "null" likelihood#
#' in which all weight is on the first component#
#' @param df appropriate degrees of freedom for (t) distribution of betahat/sebetahat, default is NULL(Gaussian)#
#' @param nullweight scalar, the weight put on the prior of null under "fdr" method#
#' @param randomstart logical, indicating whether to initialize EM randomly. If FALSE, then initializes to prior mean (for EM algorithm) or prior (for VBEM)#
#' @param nonzeromean logical, indicating whether to use a nonzero mean unimodal mixture(defaults to "FALSE")#
#' @param pointmass logical, indicating whether to use a point mass at zero as one of components for a mixture distribution#
#' @param onlylogLR logical, indicating whether to use this function to get logLR. Skip posterior prob, posterior mean, lfdr...#
#' @param prior string, or numeric vector indicating Dirichlet prior on mixture proportions (defaults to "uniform", or 1,1...,1; also can be "nullbiased" 1,1/k-1,...,1/k-1 to put more weight on first component)#
#' @param mixsd vector of sds for underlying mixture components #
#' @param VB whether to use Variational Bayes to estimate mixture proportions (instead of EM to find MAP estimate)#
#' @param gridmult the multiplier by which the default grid values for mixsd differ by one another. (Smaller values produce finer grids)#
#' @param minimal_output if TRUE, just outputs the fitted g and the lfsr (useful for very big data sets where memory is an issue) #
#' @param g the prior distribution for beta (usually estimated from the data; this is used primarily in simulated data to do computations with the "true" g)#
#' @param maxiter maximum number of iterations of the EM algorithm#
#' @param cxx flag to indicate whether to use the c++ (Rcpp) version#
#'#
#' @return ash returns an object of \code{\link[base]{class}} "ash", a list with the following elements(or a  simplified list, if \eqn{onlylogLR=TRUE}, \eqn{minimaloutput=TRUE}   or \eqn{multiseqoutput=TRUE} \cr#
#' \item{fitted.g}{fitted mixture, either a normalmix or unimix}#
#' \item{logLR}{logP(D|mle(pi)) - logP(D|null)}#
#' \item{PosteriorMean}{A vector consisting the posterior mean of beta from the mixture}#
#' \item{PosteriorSD}{A vector consisting the corresponding posterior standard deviation}#
#' \item{PositiveProb}{A vector of posterior probability that beta is positive}#
#' \item{NegativeProb}{A vector of posterior probability that beta is negative}#
#' \item{ZeroProb}{A vector of posterior probability that beta is zero}#
#' \item{lfsr}{The local false sign rate}#
#' \item{lfsra}{The local false sign rate(adjusted)}#
#' \item{lfdr}{A vector of estimated local false discovery rate}#
#' \item{qvalue}{A vector of q values given estimated local false discovery rates, and estimate of (tail) False Discovery Rate}#
#' \item{fit}{The fitted object return by \code{\link{EMest}}}#
#' \item{lambda1}{multiplicative "inflation factor"}#
#' \item{lambda2}{additive "inflation factor"}#
#' \item{call}{a call in which all of the specified arguments are specified by their full names}#
#' \item{data}{a list consisting the input betahat and sebetahat}#
#' \item{df}{the specified degrees of freedom for (t) distribution of betahat/sebetahat}#
#'#
#' @export#
#' @examples #
#' beta = c(rep(0,100),rnorm(100))#
#' sebetahat = abs(rnorm(200,0,1))#
#' betahat = rnorm(200,beta,sebetahat)#
#' beta.ash = ash(betahat, sebetahat)#
#' summary(beta.ash)#
#' plot(betahat,beta.ash$PosteriorMean,xlim=c(-4,4),ylim=c(-4,4))#
#' #
#' betahat=betahat+1000#
#' beta.ash = ash(betahat, sebetahat)#
#' summary(beta.ash)#
#' plot(betahat,beta.ash$PosteriorMean)#
#Things to do:#
# check sampling routine#
# check number of iterations#
ash = function(betahat,sebetahat,method = c("shrink","fdr"), #
               mixcompdist = c("normal","uniform","halfuniform"),#
               lambda1=1,lambda2=0,nullcheck=TRUE,df=NULL,randomstart=FALSE,#
               nullweight=10,nonzeromean=FALSE, #
               pointmass = FALSE, #
               onlylogLR = FALSE, #
               prior=c("uniform","nullbiased"), #
               mixsd=NULL, VB=FALSE,gridmult=sqrt(2),#
               minimaloutput=FALSE,#
               multiseqoutput=FALSE,#
               g=NULL,#
               maxiter = 5000,#
               cxx=FALSE){#
  #method provides a convenient interface to set a particular combinations of parameters for prior an#
  #If method is supplied, use it to set up specific values for these parameters; provide warning if values#
  #are also specified by user#
  #If method is not supplied use the user-supplied values (or defaults if user does not specify them)#
  if(!missing(method)){#
    method = match.arg(method) #
    if(method=="shrink"){#
      if(missing(prior)){#
        prior = "uniform"#
      } else {#
        warning("Specification of prior overrides default for method shrink")#
      }#
      if(missing(pointmass)){#
        pointmass=FALSE#
      } else {#
        warning("Specification of pointmass overrides default for method shrink")#
      }#
    }#
    if(method=="fdr"){#
      if(missing(prior)){#
        prior = "nullbiased"#
      } else {#
        warning("Specification of prior overrides default for method fdr")#
      }#
      if(missing(pointmass)){#
        pointmass=TRUE#
      } else {#
        warning("Specification of pointmass overrides default for method fdr")#
      }#
    }  #
  }#
  if(gridmult<=1&multiseqoutput!=TRUE)#
    stop("gridmult must be > 1")#
  mixcompdist = match.arg(mixcompdist)#
  # if(mixcompdist=="uniform" & pointmass==TRUE){#
  #    stop("point mass not yet implemented for uniform or half-uniform")#
  #  }#
  #  if(mixcompdist=="halfuniform" & pointmass==TRUE){#
  #    stop("point mass not yet implemented for uniform or half-uniform")#
  #  }#
  if(!is.numeric(prior)){#
    prior = match.arg(prior)#
  }  #
  if(length(sebetahat)==1){#
    sebetahat = rep(sebetahat,length(betahat))#
  }#
  if(length(sebetahat) != length(betahat)){#
    stop("Error: sebetahat must have length 1, or same length as betahat")#
  }#
  completeobs = (!is.na(betahat) & !is.na(sebetahat))#
  n=sum(completeobs)#
  if(n==0){#
    if(onlylogLR){#
      return(list(pi=NULL, logLR = 0))#
    }#
    else{#
      stop("Error: all input values are missing")#
    }#
  }  #
  if(!is.null(g)){#
    maxiter = 1 # if g is specified, don't iterate the EM#
    prior = rep(1,ncomp(g)) #prior is not actually used if g specified, but required to make sure EM doesn't produce warning#
    null.comp=1 #null.comp also not used, but required #
  } else {#
    if(is.null(mixsd)){#
      if(nonzeromean & is.null(df)){#
        mixsd = autoselect.mixsd(betahat[completeobs]-mean(betahat[completeobs]),sebetahat[completeobs],gridmult)#
        if(pointmass){ mixsd = c(0,mixsd) }#
        nonzeromean.fit=nonzeromeanEM(betahat[completeobs], sebetahat[completeobs], mixsd=mixsd, mixcompdist=mixcompdist,df=df,maxiter=maxiter)#
        betahat[completeobs]= betahat[completeobs] - nonzeromean.fit$nonzeromean#
      	}#
      else if(nonzeromean & !is.null(df)){#
        stop("Error: Nonzero mean only implemented for df=NULL")#
      }#
        mixsd = autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)#
    }#
    if(pointmass){#
      mixsd = c(0,mixsd)#
    }#
    null.comp = which.min(mixsd) #which component is the "null"#
    k = length(mixsd)#
    if(!is.numeric(prior)){#
      if(prior=="nullbiased"){ # set up prior to favour "null"#
        prior = rep(1,k)#
        prior[null.comp] = nullweight #prior 10-1 in favour of null by default#
      }else if(prior=="uniform"){#
        prior = rep(1,k)#
      }#
    }#
    if(length(prior)!=k | !is.numeric(prior)){#
      stop("invalid prior specification")#
    }#
    if(randomstart){#
      pi = rgamma(k,1,1)#
    } else {#
      if(k<n){#
        pi=rep(1,k)/n #default initialization strongly favours null; puts weight 1/n on everything except null#
        pi[null.comp] = (n-k+1)/n #the motivation is data can quickly drive away from null, but tend to drive only slowly toward null.#
      } else {#
        pi=rep(1,k)/k#
      }#
    }    #
    pi=normalize(pi)#
    if(!is.element(mixcompdist,c("normal","uniform","halfuniform"))) stop("Error: invalid type of mixcompdist")#
    if(mixcompdist=="normal") g=normalmix(pi,rep(0,k),mixsd)#
    if(mixcompdist=="uniform") g=unimix(pi,-mixsd,mixsd)#
    if(mixcompdist=="halfuniform"){#
      g = unimix(c(pi,pi)/2,c(-mixsd,rep(0,k)),c(rep(0,k),mixsd))#
      prior = rep(prior, 2)#
      pi = rep(pi, 2)#
    }#
  }#
  pi.fit=EMest(betahat[completeobs],lambda1*sebetahat[completeobs]+lambda2,g,prior,null.comp=null.comp,nullcheck=nullcheck,VB=VB,maxiter = maxiter, cxx=cxx, df=df)  #
  if (!onlylogLR){#
      n=length(betahat)#
      if (!multiseqoutput){#
          ZeroProb = rep(0,length=n)#
          NegativeProb = rep(0,length=n)#
      }#
      if (!minimaloutput){#
          PosteriorMean = rep(0,length=n)#
          PosteriorSD = rep(0,length=n)#
      }#
      if (!multiseqoutput){#
          ZeroProb[completeobs] = colSums(comppostprob(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)[comp_sd(pi.fit$g)==0,,drop=FALSE])#
          NegativeProb[completeobs] = cdf_post(pi.fit$g, 0, betahat[completeobs],sebetahat[completeobs],df) - ZeroProb[completeobs]#
      }#
      if (!minimaloutput){#
          PosteriorMean[completeobs] = postmean(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
          PosteriorSD[completeobs] = postsd(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
      }#
                                        #FOR MISSING OBSERVATIONS, USE THE PRIOR INSTEAD OF THE POSTERIOR#
      if (!multiseqoutput){#
          ZeroProb[!completeobs] = sum(mixprop(pi.fit$g)[comp_sd(pi.fit$g)==0])#
          NegativeProb[!completeobs] = mixcdf(pi.fit$g,0)#
          lfsr = compute_lfsr(NegativeProb,ZeroProb)#
      }#
      if (!minimaloutput){#
          PosteriorMean[!completeobs] = mixmean(pi.fit$g)#
          PosteriorSD[!completeobs] = mixsd(pi.fit$g)#
      }#
      if (!minimaloutput & !multiseqoutput){#
          PositiveProb = 1- NegativeProb-ZeroProb#
          lfsra = compute_lfsra(PositiveProb,NegativeProb,ZeroProb) #
          lfdr = ZeroProb#
          qvalue = qval.from.lfdr(lfdr)#
      }#
  }#
  if (!minimaloutput)#
      logLR = tail(pi.fit$loglik,1) - pi.fit$null.loglik#
  if(nonzeromean){#
      #Adding back the nonzero mean#
      betahat[completeobs]= betahat[completeobs]+nonzeromean.fit$nonzeromean#
      if(mixcompdist=="normal"){#
      	pi.fit$g$mean = rep(nonzeromean.fit$nonzeromean,length(pi.fit$g$pi))#
      }#
      else if(mixcompdist=="uniform"|mixcompdist=="halfuniform"){#
      	pi.fit$g$a = pi.fit$g$a + nonzeromean.fit$nonzeromean#
      	pi.fit$g$b = pi.fit$g$b + nonzeromean.fit$nonzeromean#
      }#
      PosteriorMean = PosteriorMean + nonzeromean.fit$nonzeromean      #
  }	   #
  if (onlylogLR)#
      return(list(fitted.g=pi.fit$g, logLR = logLR, df=df))#
  else if (minimaloutput)#
      return(list(fitted.g = pi.fit$g, lfsr = lfsr, fit = pi.fit,df=df))#
  else if (multiseqoutput)#
      return(list(fitted.g = pi.fit$g, logLR = logLR, PosteriorMean = PosteriorMean, PosteriorSD = PosteriorSD, call= match.call(),df=df))#
  else{#
      result = list(fitted.g = pi.fit$g, logLR = logLR, PosteriorMean = PosteriorMean, PosteriorSD = PosteriorSD, PositiveProb = PositiveProb, NegativeProb = NegativeProb, ZeroProb = ZeroProb, lfsr = lfsr,lfsra = lfsra, lfdr = lfdr, qvalue = qvalue, fit = pi.fit, lambda1 = lambda1, lambda2 = lambda2, call = match.call(), data = list(betahat = betahat, sebetahat=sebetahat),df=df)#
      class(result) = "ash"#
      return(result)#
  }#
}#
  #if(nsamp>0){#
  #  sample = posterior_sample(post,nsamp)#
  #}#
#
# #' @title Faster version of function ash#
# #'#
# #' @description This function has similar functionality as ash, but only returns some of the outputs.#
# #'#
# #' @param betahat, a p vector of estimates#
# #' @param sebetahat, a p vector of corresponding standard errors#
# #' @param nullcheck: whether to check that any fitted model exceeds the "null" likelihood in which all weight is on the first component#
# #' @param randomstart: logical, indicating whether to initialize EM randomly. If FALSE, then initializes to prior mean (for EM algorithm) or prior (for VBEM)#
# #' @param pointmass: logical, indicating whether to use a point mass at zero as one of components for a mixture distribution#
# #' @param onlylogLR: logical, indicating whether to use this function to get logLR. Skip posterior prob, posterior mean, lfdr...#
# #' @param prior: string, or numeric vector indicating Dirichlet prior on mixture proportions (defaults to "uniform", or 1,1...,1; also can be "nullbiased" 1,1/k-1,...,1/k-1 to put more weight on first component)#
# #' @param mixsd: vector of sds for underlying mixture components#
# #' @param VB: whether to use Variational Bayes to estimate mixture proportions (instead of EM to find MAP estimate)#
# #' @param gridmult: the multiplier by which the default grid values for mixsd differ by one another. (Smaller values produce finer grids)#
# #' @param g: the prior distribution for beta (usually estimated from the data; this is used primarily in simulated data to do computations with the "true" g)#
# #' @param cxx: flag to indicate whether to use the c++ (Rcpp) version#
# #'#
# #' @return a list with elements fitted.g is fitted mixture#
# #' logLR : logP(D|mle(pi)) - logP(D|null)#
# #'#
# #' @export#
# fast.ash = function(betahat,sebetahat, #
#                     nullcheck=TRUE,randomstart=FALSE, #
#                     pointmass = TRUE,    #
#                     prior=c("nullbiased","uniform"), #
#                     mixsd=NULL, VB=FALSE,gridmult=4,#
#                     g=NULL, cxx=TRUE,#
#                     onlylogLR = FALSE,df=NULL){#
#   #
#   if(onlylogLR){#
#     pointmass <- TRUE  #
#   }#
#   #
#   #If method is supplied, use it to set up defaults; provide warning if these default values#
#   #are also specified by user#
#   if(!is.numeric(prior)){#
#     prior = match.arg(prior)#
#   }#
#   #
#   if(length(sebetahat)==1){#
#     sebetahat = rep(sebetahat,length(betahat))#
#   }#
#   if(length(sebetahat) != length(betahat)){#
#     stop("Error: sebetahat must have length 1, or same length as betahat")#
#   }#
#   #
#   completeobs = (!is.na(betahat) & !is.na(sebetahat))#
#   if(sum(completeobs)==0){#
#     if(onlylogLR){#
#       return(list(pi=NULL, logLR = 0))#
#     }else{#
#       stop("Error: all input values are missing")#
#     }#
#   }  #
#   #
#   if(is.null(mixsd)){#
#     mixsd= autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)#
#   }#
#   if(pointmass){#
#     mixsd = c(0,mixsd)#
#   }#
#   #
#   k=length(mixsd)  #
#   null.comp = which.min(mixsd) #which component is the "null"#
#   #
#   if(!is.numeric(prior)){#
#     if(prior=="nullbiased"){ # set up prior to favour "null"#
#       prior = rep(1,k)#
#       prior[null.comp] = 10 #prior 10-1 in favour of null#
#     }else if(prior=="uniform"){#
#       prior = rep(1,k)#
#     }#
#   }#
#   #
#   if(length(prior)!=k | !is.numeric(prior)){#
#     stop("invalid prior specification")#
#   }#
#   #
#   if(missing(g)){#
#     pi = prior^2 #default is to initialize pi at prior (mean)#
#     if(randomstart){pi=rgamma(k,1,1)}#
#     pi=normalize(pi)#
#     g=normalmix(pi,rep(0,k),mixsd)#
#     maxiter = 5000#
#   } else {#
#     maxiter = 1; # if g is specified, don't iterate the EM #
#   }#
#   #
#   pi.fit=EMest(betahat[completeobs],sebetahat[completeobs],g,prior,null.comp=null.comp,nullcheck=nullcheck,VB=VB,maxiter = maxiter, cxx=cxx, df=df)  #
#   #
#   if(onlylogLR){#
#     logLR = tail(pi.fit$loglik,1) - pi.fit$null.loglik#
#     return(list(pi=pi.fit$pi, logLR = logLR))#
#   }else{#
#     #
#     n=length(betahat)#
#     PosteriorMean = rep(0,length=n)#
#     PosteriorSD=rep(0,length=n)#
#     #
#     if(is.null(df)){#
#       PosteriorMean[completeobs] = postmean(pi.fit$g,betahat[completeobs],sebetahat[completeobs])#
#       PosteriorSD[completeobs] =postsd(pi.fit$g,betahat[completeobs],sebetahat[completeobs]) #
#     }#
#     else{#
#       PosteriorMean[completeobs] = postmean_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
#       PosteriorSD[completeobs] =postsd_t(pi.fit$g,betahat[completeobs],sebetahat[completeobs],df)#
#     }#
#     #FOR MISSING OBSERVATIONS, USE THE PRIOR INSTEAD OF THE POSTERIOR#
#     PosteriorMean[!completeobs] = mixmean(pi.fit$g)#
#     PosteriorSD[!completeobs] =mixsd(pi.fit$g)  #
#     #
#     result = list(fitted.g=pi.fit$g,PosteriorMean = PosteriorMean,PosteriorSD=PosteriorSD,call=match.call(),data=list(betahat = betahat, sebetahat=sebetahat))#
#     return(result)#
#   }#
#   #if(nsamp>0){#
#   #  sample = posterior_sample(post,nsamp)#
#   #}#
# }#
compute_lfsr = function(NegativeProb,ZeroProb){#
  ifelse(NegativeProb> 0.5*(1-ZeroProb),1-NegativeProb,NegativeProb+ZeroProb)#
}#
#
compute_lfsra = function(PositiveProb, NegativeProb,ZeroProb){#
  ifelse(PositiveProb<NegativeProb,2*PositiveProb+ZeroProb,2*NegativeProb+ZeroProb)  #
}  #
#
#' @title Estimate unimodal nonzero mean of a mixture model by EM algorithm#
#'#
#' @description Given the data, standard error of the data and standard deviations of the Gaussian mixture model, estimate the mean of a unimodal Gaussian mixture by an EM algorithm.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates unimodal mean \eqn{\mu} by EM algorithm. Uses the SQUAREM package to accelerate convergence of EM. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#' Currently works for mixcompdist="normal", while "uniform" abd "halfuniform" would only return the naive mean#
#' #
#' @param betahat, a p vector of estimates #
#' @param sebetahat, a p vector of corresponding standard errors#
#' @param mixsd: vector of sds for underlying mixture components #
#' @param pi.init, the initial value of \eqn{\pi} to use. If not specified defaults to (1/k,...,1/k).#
#' @param tol, the tolerance for convergence of log-likelihood.#
#' @param maxiter the maximum number of iterations performed#
#' #
#' @return A list, including the estimates (\eqn{\mu}) and (\eqn{\pi}), the log likelihood for each iteration (NQ)#
#' and a flag to indicate convergence#
#' #
#' @export#
#' #
#' #
nonzeromeanEM = function(betahat, sebetahat, mixsd, mixcompdist, df=NULL, pi.init=NULL,tol=1e-7,maxiter=5000){#
  if(is.null(pi.init)){#
    pi.init = rep(1/length(mixsd),length(mixsd))# Use as starting point for pi#
  }#
  pi=pi.init#
  if(!is.element(mixcompdist,c("normal","uniform","halfuniform"))) stop("Error: invalid type of mixcompdist")#
  if(mixcompdist=="normal") {g=normalmix(pi,rep(0,length(mixsd)),mixsd)}#
  if(mixcompdist=="uniform") {g=unimix(pi,-mixsd,mixsd)}#
  if(mixcompdist=="halfuniform"){g=unimix(c(pi,pi)/2,c(-mixsd,rep(0,length(mixsd))),c(rep(0,length(mixsd)),mixsd))}#
  if(mixcompdist=="normal" & is.null(df)){#
  	mupi=c(mean(betahat),pi.init)#
    res=squarem(par=mupi,fixptfn=nonzeromeanEMfixpoint,objfn=nonzeromeanEMobj,betahat=betahat,sebetahat=sebetahat,mixsd=mixsd,control=list(maxiter=maxiter,tol=tol))#
  }#
  else if(mixcompdist=="normal" & !is.null(df)){#
  	stop("method comp_postsd of normal mixture not yet written for t likelihood")#
  	mupi=c(mean(betahat),pi.init)#
    res=squarem(par=mupi,fixptfn= nonzeromeanEMoptimfixpoint,objfn= nonzeromeanEMoptimobj,betahat=betahat,sebetahat=sebetahat,g=g,df=df,control=list(maxiter=maxiter,tol=tol))#
  }#
  else if(mixcompdist=="uniform"){#
    #return(list(nonzeromean=mean(betahat)))#
  	stop("method not yet completed")#
    mupi=c(mean(betahat),pi.init)    #
    res=squarem(par=mupi,fixptfn=nonzeromeanEMoptimfixpoint,objfn=nonzeromeanEMoptimobj,betahat=betahat,sebetahat=sebetahat,g=g,df=df,control=list(maxiter=maxiter,tol=tol))#
  }#
  else if(mixcompdist=="halfuniform"){#
  	stop("method not yet completed")#
    mupi=c(mean(betahat),pi.init/2,pi.init/2)#
    res=squarem(par=mupi,fixptfn= nonzeromeanEMoptimfixpoint,objfn= nonzeromeanEMoptimobj,betahat=betahat,sebetahat=sebetahat,g=g,df=df,control=list(maxiter=maxiter,tol=tol))#
  }#
#
  return(list(nonzeromean=res$par[1],pi=res$par[-1],NQ=-res$value.objfn,niter = res$iter, converged=res$convergence,post=res$par))#
}#
nonzeromeanEMoptimfixpoint = function(mupi,betahat,sebetahat,g,df){#
	#omegamatrix=matrix(NA,nrow=length(betahat),ncol=length(mixsd))#
	mu=mupi[1]#
	pimean=mupi[-1]#
	matrix_lik=t(compdens_conv(g,betahat-mu,sebetahat,df))#
    m=t(pimean * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
    m.rowsum=rowSums(m)#
    classprob=m/m.rowsum #an n by k matrix	#
    pinew=normalize(colSums(classprob))#
#
	munew=optim(par=mean(betahat),f= nonzeromeanEMoptim,pinew=pinew,betahat=betahat,sebetahat=sebetahat,g=g,df=df,method="Brent",lower=min(betahat),upper=max(betahat))$par#
	mupi=c(munew,pinew)#
	return(mupi)#
}#
nonzeromeanEMoptimobj = function(mupi,betahat,sebetahat,g,df){#
	mu=mupi[1]#
	pimean=mupi[-1]#
	matrix_lik = t(compdens_conv(g,betahat-mu,sebetahat,df))#
	m = t(pimean * t(matrix_lik))#
	m.rowsum = rowSums(m)#
	loglik = sum(log(m.rowsum))#
	return(-loglik)#
}#
#
nonzeromeanEMoptim = function(mu,pinew,betahat,sebetahat,g,df){#
	matrix_lik = t(compdens_conv(g,betahat-mu,sebetahat,df))#
	m = t(pinew * t(matrix_lik))#
	m.rowsum = rowSums(m)#
	loglik = sum(log(m.rowsum))#
	return(-loglik)#
}#
nonzeromeanEMfixpoint = function(mupi,betahat,sebetahat,mixsd){#
	#omegamatrix=matrix(NA,nrow=length(betahat),ncol=length(mixsd))#
	mu=mupi[1]#
	pimean=mupi[-1]#
	sdmat = sqrt(outer(sebetahat ^2,mixsd^2,"+")) #
	xmat=matrix(rep(betahat,length(mixsd)),ncol=length(mixsd))#
	omegamatrix=t(t(dnorm(xmat,mean=mu,sd=sdmat))*pimean)#
	omegamatrix=omegamatrix /rowSums(omegamatrix)#
	pinew=normalize(colSums(omegamatrix))#
	munew=sum(omegamatrix*xmat/(sdmat^2))/sum(omegamatrix/(sdmat^2))#
	mupi=c(munew,pinew)#
	return(mupi)#
}#
#
nonzeromeanEMobj = function(mupi,betahat,sebetahat,mixsd){#
	mu=mupi[1]#
	pimean=mupi[-1]#
	sdmat = sqrt(outer(sebetahat ^2,mixsd^2,"+")) #
	xmat=matrix(rep(betahat,length(mixsd)),ncol=length(mixsd))#
	omegamatrix=t(t(dnorm(xmat,mean=mu,sd=sdmat))*pimean)#
	omegamatrix=omegamatrix /rowSums(omegamatrix)#
	NegativeQ=-sum(omegamatrix*dnorm(xmat,mean=mu,sd=sdmat,log=TRUE))#
	return(NegativeQ)#
}#
#' @title Estimate posterior distribution on mixture proportions of a mixture model by a Variational Bayes EM algorithm#
#'#
#' @description Given the individual component likelihoods for a mixture model, estimates the posterior on #
#' the mixture proportions by an VBEM algorithm. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates posterior on mixture proportions \eqn{\pi} by Variational Bayes, #
#' with a Dirichlet prior on \eqn{\pi}. #
#' Algorithm adapted from Bishop (2009), Pattern Recognition and Machine Learning, Chapter 10.#
#' #
#' @param matrix_lik: a n by k matrix with (j,k)th element equal to \eqn{f_k(x_j)}.#
#' @param prior: a k vector of the parameters of the Dirichlet prior on \eqn{\pi}. Recommended to be rep(1,k)#
#' @param post.init: the initial value of the posterior parameters. If not specified defaults to the prior parameters.#
#' @param tol: the tolerance for convergence of log-likelihood bound.#
#' @param maxiter: the maximum number of iterations performed#
#' #
#' @return A list, whose components include point estimates (pihat), #
#' the parameters of the fitted posterior on \eqn{\pi} (pipost),#
#' the bound on the log likelihood for each iteration (B)#
#' and a flag to indicate convergence (converged).#
#'  #
#' @export#
#' #
mixVBEM = function(matrix_lik, prior, pi.init = NULL,tol=1e-7, maxiter=5000){#
  k=ncol(matrix_lik)#
  if(is.null(pi.init)){#
    pi.init = rep(1,k)# Use as starting point for pi#
  } #
  res = squarem(par=pi.init,fixptfn=VBfixpoint, objfn=VBnegpenloglik,matrix_lik=matrix_lik, prior=prior, control=list(maxiter=maxiter,tol=tol))#
  return(list(pihat = res$par/sum(res$par), B=res$value.objfn, niter = res$iter, converged=res$convergence,post=res$par))#
}#
VBfixpoint = function(pipost, matrix_lik, prior){  #
  n=nrow(matrix_lik)#
  k=ncol(matrix_lik)#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost*matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  pipostnew = colSums(classprob) + prior#
  return(pipostnew)#
}#
#
VBnegpenloglik=function(pipost,matrix_lik,prior){#
  return(-VBpenloglik(pipost,matrix_lik,prior))#
}#
#
VBpenloglik = function(pipost, matrix_lik, prior){#
  n=nrow(matrix_lik)#
  k=ncol(matrix_lik)#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost*matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  B= sum(classprob*log(avgpipost*matrix_lik),na.rm=TRUE) - diriKL(prior,pipost) - sum(classprob*log(classprob)) #
  return(B)#
}#
#' @title Estimate mixture proportions of a mixture model by EM algorithm#
#'#
#' @description Given the individual component likelihoods for a mixture model, estimates the mixture proportions by an EM algorithm.#
#'#
#' @details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent#
#' and identically distributed data \eqn{x_1,\dots,x_n}. #
#' Estimates mixture proportions \eqn{\pi} by maximum likelihood, or by maximum a posteriori (MAP) estimation for a Dirichlet prior on \eqn{\pi} #
#' (if a prior is specified).  Uses the SQUAREM package to accelerate convergence of EM. Used by the ash main function; there is no need for a user to call this #
#' function separately, but it is exported for convenience.#
#'#
#' #
#' @param matrix_lik, a n by k matrix with (j,k)th element equal to \eqn{f_k(x_j)}.#
#' @param prior, a k vector of the parameters of the Dirichlet prior on \eqn{\pi}. Recommended to be rep(1,k)#
#' @param pi.init, the initial value of \eqn{\pi} to use. If not specified defaults to (1/k,...,1/k).#
#' @param tol, the tolerance for convergence of log-likelihood.#
#' @param maxiter the maximum number of iterations performed#
#' #
#' @return A list, including the estimates (pihat), the log likelihood for each interation (B)#
#' and a flag to indicate convergence#
#'  #
#' @export#
#' #
#' #
mixEM = function(matrix_lik, prior, pi.init = NULL,tol=1e-7, maxiter=5000){#
  if(is.null(pi.init)){#
    pi.init = rep(1/k,k)# Use as starting point for pi#
  } #
  res = squarem(par=pi.init,fixptfn=fixpoint, objfn=negpenloglik,matrix_lik=matrix_lik, prior=prior, control=list(maxiter=maxiter,tol=tol))#
  return(list(pihat = normalize(pmax(0,res$par)), B=res$value.objfn, #
              niter = res$iter, converged=res$convergence))#
}#
#
# helper functions used by mixEM#
normalize = function(x){return(x/sum(x))}#
#
fixpoint = function(pi, matrix_lik, prior){  #
  pi = normalize(pmax(0,pi)) #avoid occasional problems with negative pis due to rounding#
  m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
  m.rowsum = rowSums(m)#
  classprob = m/m.rowsum #an n by k matrix#
  pinew = normalize(colSums(classprob) + prior - 1)#
  return(pinew)#
}#
#
negpenloglik = function(pi,matrix_lik,prior){return(-penloglik(pi,matrix_lik,prior))}#
#
penloglik = function(pi, matrix_lik, prior){#
  pi = normalize(pmax(0,pi))#
  m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
  m.rowsum = rowSums(m)#
  loglik = sum(log(m.rowsum))#
  subset = (prior != 1.0)#
  priordens = sum((prior-1)[subset]*log(pi[subset]))#
  return(loglik+priordens)#
}#
#
#The kth element of this vector is the derivative #
#of the loglik for $\pi=(\pi_0,...,1-\pi_0,...)$ with respect to $\pi_0$ at $\pi_0=1$.#
gradient = function(matrix_lik){#
  n = nrow(matrix_lik)#
  grad = n - colSums(matrix_lik/matrix_lik[,1]) #
  return(grad)#
}#
#
# mixEM = function(matrix_lik, prior, pi.init = NULL,tol=0.0001, maxiter=5000){#
#   n=nrow(matrix_lik)#
#   k=ncol(matrix_lik)#
#   B = rep(0,maxiter)#
#   pi = pi.init#
#   if(is.null(pi.init)){#
#     pi = rep(1/k,k)# Use as starting point for pi#
#   } #
#   pi = ifelse(pi<1e-5,1e-5,pi) #set any estimates that are too small to be just very small#
#   pi = normalize(pi)#
#   #
#   loglik = rep(0,maxiter)#
#   priordens= rep(0,maxiter)#
#   m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
#   m.rowsum = rowSums(m)#
#   loglik[1] = sum(log(m.rowsum))#
#   priordens[1] = sum((prior-1)*log(pi)) #
#   classprob = m/m.rowsum #an n by k matrix#
#   i=1#
#   if(maxiter >= 2){#
#     for(i in 2:maxiter){  #
#       pi = colSums(classprob) + prior-1#
#       pi = ifelse(pi<1e-5,1e-5,pi) #set any estimates that are less than zero, which can happen with prior<1, to 0#
#       pi = normalize(pi)#
#         #
#       #Now re-estimate pi#
#       m  = t(pi * t(matrix_lik)) #
#       m.rowsum = rowSums(m)#
#       loglik[i] = sum(log(m.rowsum))#
#       priordens[i] = sum((prior-1)*log(pi)) #
#       classprob = m/m.rowsum#
#     #
#     #
#       if(abs(loglik[i]+priordens[i]-loglik[i-1]-priordens[i-1])<tol) break;#
#     }#
#   }#
#   converged=(abs(loglik[i]+priordens[i]-loglik[i-1]-priordens[i-1])<tol)#
#   if(!converged){#
#       warning("EM algorithm in function mixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
#   }#
#   return(list(pihat = pi, B=loglik[1:i], #
#               niter = i, converged=converged))#
# }#
#estimate mixture proportions of sigmaa by EM algorithm#
#prior gives the parameter of a Dirichlet prior on pi#
#(prior is used to encourage results towards smallest value of sigma when#
#likelihood is flat)#
#nullcheck indicates whether to check whether the loglike exceeds the null#
#(may not want to use if prior is used)#
#VB provides an approach to estimate the approximate posterior distribution#
#of mixture proportions of sigmaa by variational Bayes method#
#(use Dirichlet prior and approximate Dirichlet posterior)#
#if cxx TRUE use cpp version of R function mixEM#
EMest = function(betahat,sebetahat,g,prior,null.comp=1,nullcheck=TRUE,VB=FALSE, maxiter=5000, cxx=TRUE, df=NULL){ #
  pi.init = g$pi#
  k=ncomp(g)#
  n = length(betahat)#
  tol = min(0.1/n,1e-5) # set convergence criteria to be more stringent for larger samples#
  matrix_lik = t(compdens_conv(g,betahat,sebetahat,df))#
  #checks whether the gradient at pi0=1 is positive (suggesting that this is a fixed point)#
  #if(nullcheck){#
  #  if(all(gradient(matrix_lik)>=0)){#
  #    pi.init=rep(0,k)#
  #    pi.init[null.comp]=1 #this will make pi.init=(1,0,0...,0) which is a fixed point of the EM#
  #  }#
  #}#
  if(VB==TRUE){#
    EMfit=mixVBEM(matrix_lik,prior,maxiter=maxiter)}#
  else{#
    if (cxx==TRUE){#
      EMfit = cxxMixEM(matrix_lik,prior,pi.init,1e-5, maxiter) #currently use different convergence criteria for cxx version #
      if(!EMfit$converged){#
        warning("EM algorithm in function cxxMixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
      }#
    }#
    else{#
      EMfit = mixEM(matrix_lik,prior,pi.init,tol, maxiter)#
      if(!EMfit$converged & !(maxiter==1)){#
        warning("EM algorithm in function mixEM failed to converge. Results may be unreliable. Try increasing maxiter and rerunning.")#
      }#
    }#
  }#
  pi = EMfit$pihat     #
  penloglik = EMfit$B #
  converged = EMfit$converged#
  niter = EMfit$niter#
  loglik.final =  penloglik(pi,matrix_lik,1) #compute penloglik without penalty#
  null.loglik = sum(log(matrix_lik[,null.comp]))  #
  if(nullcheck==TRUE & VB==FALSE){ #null check doesn't work with VB yet#
    pinull = rep(0,k)#
    pinull[null.comp]=1#
    null.penloglik = penloglik(pinull,matrix_lik,prior)#
    final.penloglik = penloglik(pi,matrix_lik,prior)#
    if(null.penloglik > final.penloglik){ #check whether exceeded "null" likelihood where everything is null#
      pi=pinull#
      loglik.final=penloglik(pi,matrix_lik,1)#
    }#
  }#
  g$pi=pi#
  return(list(loglik=loglik.final,null.loglik=null.loglik,#
              matrix_lik=matrix_lik,converged=converged,g=g))#
}#
#' @title Compute Posterior#
#'#
#' @description Return the posterior on beta given a prior (g) that is a mixture of normals (class normalmix) #
#' and observation \eqn{betahat ~ N(beta,sebetahat)}#
#'#
#' @details This can be used to obt#
#'#
#' @param g: a normalmix with components indicating the prior; works only if g has means 0#
#' @param betahat (n vector of observations) #
#' @param sebetahat (n vector of standard errors/deviations of observations)#
#' #
#' @return A list, (pi1,mu1,sigma1) whose components are each k by n matrices#
#' where k is number of mixture components in g, n is number of observations in betahat#
#' #
#' @export#
#' #
#' #
posterior_dist = function(g,betahat,sebetahat){#
  if(class(g)!="normalmix"){#
    stop("Error: posterior_dist implemented only for g of class normalmix")#
  }#
  pi0 = g$pi#
  mu0 = g$mean#
  sigma0 = g$sd  #
  k= length(pi0)#
  n= length(betahat)#
  if(!all.equal(g$mean,rep(0,k))) stop("Error: posterior_dist currently only implemented for zero-centered priors")#
  pi1 = pi0 * t(matrix_dens(betahat,sebetahat,sigma0))#
  pi1 = apply(pi1, 2, normalize) #pi1 is now an k by n matrix#
  #make k by n matrix versions of sigma0^2 and sebetahat^2#
  # and mu0 and betahat#
  s0m2 = matrix(sigma0^2,nrow=k,ncol=n,byrow=FALSE)#
  sebm2 = matrix(sebetahat^2,nrow=k,ncol=n, byrow=TRUE)#
  mu0m = matrix(mu0,nrow=k,ncol=n,byrow=FALSE)#
  bhatm = matrix(betahat,nrow=k,ncol=n,byrow=TRUE)#
  sigma1 = (s0m2*sebm2/(s0m2 + sebm2))^(0.5)  #
  w = sebm2/(s0m2 + sebm2)#
  mu1 = w*mu0m + (1-w)*bhatm#
  #WHERE DATA ARE MISSING, SET POSTERIOR = PRIOR#
  ismiss = (is.na(betahat) | is.na(sebetahat)) #
  pi1[,ismiss] = pi0#
  mu1[,ismiss] = mu0#
  sigma1[,ismiss] = sigma0#
  return(list(pi=pi1,mu=mu1,sigma=sigma1))#
}#
#
#return matrix of densities of observations (betahat) #
# assuming betahat_j \sim N(0, sebetahat_j^2 + sigmaavec_k^2)#
#normalized by maximum of each column#
#INPUT#
#betahat is n vector, #
#sebetahat is n vector, #
#sigmaavec is k vector#
#return is n by k matrix of the normal likelihoods, #
# with (j,k)th element the density of N(betahat_j; mean=0, var = sebetahat_j^2 + sigmaavec_k^2)#
#normalized to have maximum 1 in each column#
matrix_dens = function(betahat, sebetahat, sigmaavec){#
  k = length(sigmaavec)#
  n = length(betahat)#
  ldens = dnorm(betahat,0,sqrt(outer(sebetahat^2,sigmaavec^2,FUN="+")),log=TRUE)#
  maxldens = apply(ldens, 1, max)#
  ldens = ldens - maxldens#
  return(exp(ldens))#
}#
#
#return the "effective" estimate#
#that is the effect size betanew whose z score betanew/se#
#would give the same p value as betahat/se compared to a t with df#
effective.effect=function(betahat,se,df){#
  p = pt(betahat/se,df)#
  qnorm(p,sd=se)#
}#
#' @title Function to compute q values from local false discovery rates#
#'#
#' @description Computes q values from a vector of local fdr estimates#
#'#
#' @details The q value for a given lfdr is an estimate of the (tail) False Discovery Rate #
#' for all findings with a smaller lfdr, and is found by the average of the lfdr for#
#' all more significant findings. See Storey (2003), Annals of Statistics, for definition of q value.  #
#' #
#' #
#' @param lfdr, a vector of local fdr estimates#
#'#
#' @return vector of q values#
#' #
#' @export#
qval.from.lfdr = function(lfdr){#
  o = order(lfdr)#
  qvalue=rep(NA,length(lfdr))#
  qvalue[o] = (cumsum(sort(lfdr))/(1:sum(!is.na(lfdr))))#
  return(qvalue)#
}#
#
# try to select a default range for the sigmaa values#
# that should be used, based on the values of betahat and sebetahat#
# mult is the multiplier by which the sds differ across the grid#
autoselect.mixsd = function(betahat,sebetahat,mult){#
  sigmaamin = min(sebetahat)/10 #so that the minimum is small compared with measurement precision#
  if(all(betahat^2<sebetahat^2)){#
    sigmaamax = 8*sigmaamin #to deal with the occassional odd case where this could happen; 8 is arbitrary#
  } else {#
    sigmaamax = 2*sqrt(max(betahat^2-sebetahat^2)) #this computes a rough largest value you'd want to use, based on idea that sigmaamax^2 + sebetahat^2 should be at least betahat^2   #
  }#
  if(mult==0){#
    return(c(0,sigmaamax/2))#
  }else{#
    npoint = ceiling(log2(sigmaamax/sigmaamin)/log2(mult))#
    return(mult^((-npoint):0) * sigmaamax)#
  }#
}#
#return the KL-divergence between 2 dirichlet distributions#
#p,q are the vectors of dirichlet parameters of same lengths#
diriKL = function(p,q){#
  p.sum = sum(p)#
  q.sum = sum(q)#
  k = length(q)#
  KL = lgamma(q.sum)-lgamma(p.sum)+sum((q-p)*(digamma(q)-digamma(rep(q.sum,k))))+sum(lgamma(p)-lgamma(q))#
  return(KL)#
}#
#
#helper function for VBEM#
VB.update = function(matrix_lik, pipost){#
  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)#
  classprob = avgpipost * matrix_lik#
  classprob = classprob/rowSums(classprob) # n by k matrix#
  B = sum(classprob*log(avgpipost*matrix_lik),na.rm=TRUE) - diriKL(prior,pipost) #negative free energy#
  return(list(classprob=classprob,B=B))#
}
GENERIC FUNCTIONS #############################
# find matrix of densities at y, for each component of the mixture#
# INPUT y is an n-vector#
# OUTPUT k by n matrix of densities#
compdens = function(x,y,log=FALSE){#
  UseMethod("compdens")#
}#
compdens.default = function(x,y,log=FALSE){#
  stop(paste("Invalid class", class(m), "for first argument in",  match.call()))  #
}#
#
#standard deviations#
comp_sd = function(m){#
  UseMethod("comp_sd")#
}#
comp_sd.default = function(m){#
  stop("method comp_sd not written for this class")#
}#
#
#second moments#
comp_mean2 = function(m){#
  UseMethod("comp_mean2")#
}#
comp_mean2.default = function(m){#
  comp_sd(m)^2 + comp_mean(m)^2#
}#
#return the overall mean of the mixture#
mixmean = function(m){#
  UseMethod("mixmean")#
}#
mixmean.default = function(m){#
  sum(m$pi * comp_mean(m))#
}#
#
#return the overall second moment of the mixture#
mixmean2 = function(m){#
  UseMethod("mixmean2")#
}#
mixmean2.default = function(m){#
  sum(m$pi * comp_mean2(m))#
}#
#
#return the overall sd of the mixture#
mixsd = function(m){#
  UseMethod("mixsd")#
}#
mixsd.default = function(m){#
  sqrt(mixmean2(m)-mixmean(m)^2)#
}#
#
#means#
comp_mean = function(m){#
  UseMethod("comp_mean")#
}#
comp_mean.default = function(m){#
  stop("method comp_mean not written for this class")#
}#
#
#number of components#
ncomp = function(m){#
  UseMethod("ncomp")#
}#
ncomp.default = function(m){#
  return(length(m$pi))#
}#
#
#return mixture proportions, a generic function#
mixprop = function(m){#
  UseMethod("mixprop")#
}#
mixprop.default = function(m){#
  m$pi#
}#
#
#' @title mixcdf#
#'#
#' @description Returns cdf for a mixture (generic function)#
#' #
#' @details None#
#' #
#' @param x a mixture (eg of type normalmix or unimix)#
#' @param y locations at which cdf to be computed#
#' @param lower.tail: boolean indicating whether to report lower tail#
#' #
#' @return an object of class normalmix#
#' #
#' @export#
#' #
#' @examples mixcdf(normalmix(c(0.5,0.5),c(0,0),c(1,2)),seq(-4,4,length=100))#
#' #
mixcdf = function(x,y,lower.tail=TRUE){#
  UseMethod("mixcdf")#
}#
#' @title mixcdf.default#
#' @export#
#' #
mixcdf.default = function(x,y,lower.tail=TRUE){#
  x$pi %*% comp_cdf(x,y,lower.tail)#
}#
#
#find cdf for each component, a generic function#
comp_cdf = function(x,y,lower.tail=TRUE){#
  UseMethod("comp_cdf")#
}#
comp_cdf.default = function(x,y,lower.tail=TRUE){#
  stop("comp_cdf not implemented for this class")#
}#
#find density at y, a generic function#
dens = function(x,y){#
  UseMethod("dens")#
}#
dens.default = function(x,y){#
  return (x$pi %*% compdens(x, y))#
}#
#
#find log likelihood of data in x (a vector) for mixture in m#
loglik = function(m,x){#
  UseMethod("loglik")#
}#
loglik.default = function(m,x){#
  sum(log(dens(m,x)))#
}#
#
#find log likelihood of data in betahat, when #
#the mixture m is convolved with a normal with sd betahatsd#
#betahatsd is an n vector#
#betahat is an n vector#
#v is the degree of freedom#
#' @title loglik_conv#
#' #
#' @export#
#' #
loglik_conv = function(m,betahat,betahatsd,v,FUN="+"){#
  UseMethod("loglik_conv")#
}#
#' @title loglik_conv.default#
#' #
#' @export#
#' #
loglik_conv.default = function(m,betahat,betahatsd,v,FUN="+"){#
  sum(log(dens_conv(m,betahat,betahatsd,v,FUN)))#
}#
#
#compute the density of the components of the mixture m#
#when convoluted with a normal with standard deviation s#
#or a scaled (se) student.t with df v#
#the density is evaluated at x#
#x and s are n-vectors#
#m is a mixture with k components#
#output is a k by n matrix of densities#
compdens_conv = function(m,x,s,v,FUN="+"){#
  UseMethod("compdens_conv")#
}#
compdens_conv.default = function(m,x,s,v,FUN="+"){#
  stop(paste("Invalid class", class(m), "for first argument in",  match.call()))  #
}#
#
#compute density of mixture m convoluted with normal of sd (s) or student t with df v#
#at locations x#
#m is a mixture#
#x is an n vector#
#s is an n vector or integer#
dens_conv = function(m,x,s,v,FUN="+"){#
  UseMethod("dens_conv")#
}#
dens_conv.default = function(m,x,s,v,FUN="+"){#
  colSums(m$pi * compdens_conv(m,x,s,v,FUN))#
}#
#
#compute the posterior prob that each observation#
#came from each component of the mixture m#
#output a k by n vector of probabilities#
#computed by weighting the component densities by pi#
#and then normalizing#
comppostprob=function(m,x,s,v){#
 UseMethod("comppostprob") #
}#
comppostprob.default = function(m,x,s,v){#
  tmp= (t(m$pi * compdens_conv(m,x,s,v))/dens_conv(m,x,s,v))#
  ismissing = (is.na(x) | is.na(s))#
  tmp[ismissing,]=m$pi#
  t(tmp)#
}#
# evaluate cdf of posterior distribution of beta at c#
# m is the prior on beta, a mixture#
# c is location of evaluation#
# assumption is betahat | beta \sim N(beta,sebetahat)#
# m is a mixture with k components#
# c a scalar#
# betahat, sebetahat are n vectors #
# output is a k by n matrix#
compcdf_post=function(m,c,betahat,sebetahat,v){#
  UseMethod("compcdf_post")#
}#
compcdf_post.default=function(m,c,betahat,sebetahat,v){#
  stop("method compcdf_post not written for this class")#
}#
cdf_post = function(m,c,betahat,sebetahat,v){#
  UseMethod("cdf_post")#
}#
cdf_post.default=function(m,c,betahat,sebetahat,v){#
  colSums(comppostprob(m,betahat,sebetahat,v)*compcdf_post(m,c,betahat,sebetahat,v))#
}#
#
#output posterior mean for beta for prior mixture m,#
#given observations betahat, sebetahat, df v#
postmean = function(m, betahat,sebetahat,v){#
  UseMethod("postmean")#
}#
postmean.default = function(m,betahat,sebetahat,v){#
  colSums(comppostprob(m,betahat,sebetahat,v) * comp_postmean(m,betahat,sebetahat,v))#
}#
#output posterior mean-squared value for beta for prior mixture m,#
#given observations betahat, sebetahat, df v#
postmean2 = function(m, betahat,sebetahat,v){#
  UseMethod("postmean2")#
}#
postmean2.default = function(m,betahat,sebetahat,v){#
  colSums(comppostprob(m,betahat,sebetahat,v) * comp_postmean2(m,betahat,sebetahat,v))#
}#
#
#output posterior sd for beta for prior mixture m,#
#given observations betahat, sebetahat, df v#
postsd = function(m,betahat,sebetahat,v){#
  UseMethod("postsd")#
}#
postsd.default = function(m,betahat,sebetahat,v){#
  sqrt(postmean2(m,betahat,sebetahat,v)-postmean(m,betahat,sebetahat,v)^2)#
}#
#
#output posterior mean-squared value for beta for prior mixture m,#
#given observations betahat, sebetahat, df v#
comp_postmean2 = function(m,betahat,sebetahat,v){#
  UseMethod("comp_postmean2")#
}#
comp_postmean2.default = function(m,betahat,sebetahat,v){#
  comp_postsd(m,betahat,sebetahat,v)^2 + comp_postmean(m,betahat,sebetahat,v)^2#
}#
#output posterior mean for beta for each component of prior mixture m,#
#given observations betahat, sebetahat, df v#
comp_postmean = function(m, betahat,sebetahat,v){#
  UseMethod("comp_postmean")#
}#
comp_postmean.default = function(m,betahat,sebetahat,v){#
  stop("method comp_postmean not written for this class")#
}#
#output posterior sd for beta for each component of prior mixture m,#
#given observations betahat, sebetahat, df v#
comp_postsd = function(m, betahat,sebetahat,v){#
  UseMethod("comp_postsd")#
}#
comp_postsd.default = function(m,betahat,sebetahat,v){#
  stop("method comp_postsd not written for this class")#
}#
#
#find nice limits of mixture m for plotting#
min_lim = function(m){#
  UseMethod("min_lim")#
}#
min_lim.default=function(m){#
  -5#
}#
#
max_lim = function(m){#
  UseMethod("max_lim")#
}#
max_lim.default=function(m){#
  5#
}#
#plot density of mixture#
plot_dens = function(m,npoints=100,...){#
  UseMethod("plot_dens")#
}#
plot_dens.default = function(m,npoints=100,...){#
  x = seq(min_lim(m),max_lim(m),length=npoints)#
  plot(x,dens(m,x),type="l",xlab="density",ylab="x",...)#
}#
#
plot_post_cdf = function(m,betahat,sebetahat,v,npoints=100,...){#
  UseMethod("plot_post_cdf")#
}#
plot_post_cdf.default = function(m,betahat,sebetahat,v,npoints=100,...){#
  x = seq(min_lim(m),max_lim(m),length=npoints)#
  x_cdf = vapply(x,cdf_post,FUN.VALUE=betahat,m=m,betahat=betahat,sebetahat=sebetahat,v=v)#
  plot(x,x_cdf,type="l",xlab="x",ylab="cdf",...)#
 # for(i in 2:nrow(x_cdf)){#
 #   lines(x,x_cdf[i,],col=i)#
 # }#
}#
#
############################### METHODS FOR normalmix class ############################
#
#' @title Constructor for normalmix class#
#'#
#' @description Creates an object of class normalmix (finite mixture of univariate normals)#
#' #
#' @details None#
#' #
#' @param pi vector of mixture proportions#
#' @param mean vector of means#
#' @param sd: vector of standard deviations#
#' #
#' @return an object of class normalmix#
#' #
#' @export#
#' #
#' @examples normalmix(c(0.5,0.5),c(0,0),c(1,2))#
#' #
normalmix = function(pi,mean,sd){#
  structure(data.frame(pi,mean,sd),class="normalmix")#
}#
#
comp_sd.normalmix = function(m){#
  m$sd#
}#
#
comp_mean.normalmix = function(m){#
  m$mean#
}#
#
compdens.normalmix = function(x,y,log=FALSE){#
  k=ncomp(x)#
  n=length(y)#
  d = matrix(rep(y,rep(k,n)),nrow=k)#
  return(matrix(dnorm(d, x$mean, x$sd, log),nrow=k))  #
}#
#
#density of convolution of each component of a normal mixture with N(0,s^2) or s*t(v) at x#
# x an n-vector at which density is to be evaluated#
#return a k by n matrix#
#Note that convolution of two normals is normal, so it works that way#
compdens_conv.normalmix = function(m,x,s,v,FUN="+"){#
  if(!is.null(v)){#
  	stop("method comp_postsd of normal mixture not written for df!=NULL")#
  }#
  if(length(s)==1){s=rep(s,length(x))}#
  sdmat = sqrt(outer(s^2,m$sd^2,FUN)) #n by k matrix of standard deviations of convolutions#
  return(t(dnorm(outer(x,m$mean,FUN="-")/sdmat)/sdmat))#
}#
comp_cdf.normalmix = function(x,y,lower.tail=TRUE){#
  vapply(y,pnorm,x$mean,x$mean,x$sd,lower.tail)#
}#
#
#c is a scalar#
#m a mixture with k components#
#betahat a vector of n observations#
#sebetahat an n vector of standard errors#
#return a k by n matrix of the posterior cdf#
compcdf_post.normalmix=function(m,c,betahat,sebetahat,v){#
  if(!is.null(v)){#
  	stop("Error: normal mixture for student-t likelihood is not yet implemented")#
  }  #
  k = length(m$pi)#
  n=length(betahat)#
  #compute posterior standard deviation (s1) and posterior mean (m1)#
  s1 = sqrt(outer(sebetahat^2,m$sd^2,FUN="*")/outer(sebetahat^2,m$sd^2,FUN="+"))#
  ismissing = (is.na(betahat) | is.na(sebetahat))#
  s1[ismissing,]=m$sd#
  m1 = t(comp_postmean(m,betahat,sebetahat,v))#
  t(pnorm(c,mean=m1,sd=s1))#
}#
#
#return posterior mean for each component of prior m, given observations betahat and sebetahat#
#input, m is a mixture with k components#
#betahat, sebetahat are n vectors#
#output is a k by n matrix#
comp_postmean.normalmix = function(m,betahat,sebetahat,v){#
  if(!is.null(v)){#
  	stop("method comp_postmean of normal mixture not written for df!=NULL")#
  }#
  tmp=(outer(sebetahat^2,m$mean, FUN="*") + outer(betahat,m$sd^2, FUN="*"))/outer(sebetahat^2,m$sd^2,FUN="+")#
  ismissing = (is.na(betahat) | is.na(sebetahat))#
  tmp[ismissing,]=m$mean #return prior mean when missing data#
  t(tmp)#
}#
#return posterior standard deviation for each component of prior m, given observations betahat and sebetahat#
#input, m is a mixture with k components#
#betahat, sebetahat are n vectors#
#output is a k by n matrix#
comp_postsd.normalmix = function(m,betahat,sebetahat,v){#
  if(!is.null(v)){#
  	stop("method comp_postsd of normal mixture not written for df!=NULL")#
  }#
  t(sqrt(outer(sebetahat^2,m$sd^2,FUN="*")/outer(sebetahat^2,m$sd^2,FUN="+")))#
}#
############################### METHODS FOR unimix class ############################
#
#constructor; pi, a and b are vectors; kth component is Uniform(a[k],b[k])#
unimix = function(pi,a,b){#
  structure(data.frame(pi,a,b),class="unimix")#
}#
#
comp_cdf.unimix = function(m,y,lower.tail=TRUE){#
  vapply(y,punif,m$a,min=m$a,max=m$b,lower.tail)#
}#
#
comp_sd.unimix = function(m){#
  (m$b-m$a)/sqrt(12)#
}#
#
comp_mean.unimix = function(m){#
  (m$a+m$b)/2#
}#
compdens.unimix = function(x,y,log=FALSE){#
  k=ncomp(x)#
  n=length(y)#
  d = matrix(rep(y,rep(k,n)),nrow=k)#
  return(matrix(dunif(d, x$a, x$b, log),nrow=k))  #
}#
#
#density of convolution of each component of a unif mixture with N(0,s) at x#
# x an n-vector#
#return a k by n matrix#
compdens_conv.unimix = function(m,x,s,v, FUN="+"){#
  if(FUN!="+") stop("Error; compdens_conv not implemented for uniform with FUN!=+")#
  if(is.null(v)){#
    compdens= t(pnorm(outer(x,m$a,FUN="-")/s)-pnorm(outer(x,m$b,FUN="-")/s))/(m$b-m$a)#
    compdens[m$a==m$b,]=t(dnorm(outer(x,m$a,FUN="-")/s)/s)[m$a==m$b,]#
  }#
  else{#
    compdens= t(pt(outer(x,m$a,FUN="-")/s,df=v)-pt(outer(x,m$b,FUN="-")/s,df=v))/(m$b-m$a)#
    compdens[m$a==m$b,]=t(dt(outer(x,m$a,FUN="-")/s,df=v)/s)[m$a==m$b,]#
  }#
  return(compdens)#
}#
#c is a scalar#
#m a mixture with k components#
#betahat a vector of n observations#
#sebetahat an n vector of standard errors#
#return a k by n matrix of the posterior cdf#
compcdf_post.unimix=function(m,c,betahat,sebetahat,v){#
  k = length(m$pi)#
  n=length(betahat)#
  tmp = matrix(1,nrow=k,ncol=n)#
  tmp[m$a > c,] = 0#
  subset = m$a<=c & m$b>c # subset of components (1..k) with nontrivial cdf#
  if(sum(subset)>0){#
  	if(is.null(v)){#
      pna = pnorm(outer(betahat,m$a[subset],FUN="-")/sebetahat)#
      pnc = pnorm(outer(betahat,rep(c,sum(subset)),FUN="-")/sebetahat)#
      pnb = pnorm(outer(betahat,m$b[subset],FUN="-")/sebetahat)#
    }#
    else{#
      pna = pt(outer(betahat,m$a[subset],FUN="-")/sebetahat, df=v)#
      pnc = pt(outer(betahat,rep(c,sum(subset)),FUN="-")/sebetahat, df=v)#
      pnb = pt(outer(betahat,m$b[subset],FUN="-")/sebetahat, df=v)#
    }#
    tmp[subset,] = t((pnc-pna)/(pnb-pna))#
  }#
  subset = (m$a == m$b) #subset of components with trivial cdf#
  tmp[subset,]= rep(m$a[subset] <= c,n)#
  tmp#
}#
#
my_etruncnorm= function(a,b,mean=0,sd=1){#
  alpha = (a-mean)/sd#
  beta =  (b-mean)/sd#
 #Flip the onese where both are positive, as the computations are more stable#
  #when both negative#
  flip = (alpha>0 & beta>0)#
  flip[is.na(flip)]=FALSE #deal with NAs#
  alpha[flip]= -alpha[flip]#
  beta[flip]=-beta[flip]#
  tmp= (-1)^flip * (mean+sd*etruncnorm(alpha,beta,0,1))#
  max_alphabeta = ifelse(alpha<beta, beta,alpha)#
  max_ab = ifelse(alpha<beta,b,a)#
  toobig = max_alphabeta<(-30)#
  toobig[is.na(toobig)]=FALSE #
  tmp[toobig] = max_ab[toobig]#
  tmp#
}#
#return posterior mean for each component of prior m, given observations betahat and sebetahat#
#input, m is a mixture with k components#
#betahat, sebetahat are n vectors#
#output is a k by n matrix#
#note that with uniform prior, posterior is truncated normal, so#
#this is computed using formula for mean of truncated normal #
comp_postmean.unimix = function(m,betahat,sebetahat,v){#
#   k= ncomp(m)#
#   n=length(betahat)#
#   a = matrix(m$a,nrow=n,ncol=k,byrow=TRUE)#
#   b = matrix(m$b,nrow=n,ncol=k,byrow=TRUE)#
#   matrix(etruncnorm(a,b,betahat,sebetahat),nrow=k,byrow=TRUE)#
  #note: etruncnorm is more stable for a and b negative than positive#
  #so maybe use this, and standardize to make the whole more stable.#
  alpha = outer(-betahat, m$a,FUN="+")/sebetahat#
  beta = outer(-betahat, m$b, FUN="+")/sebetahat#
  if(is.null(v)){#
    tmp = betahat + sebetahat*my_etruncnorm(alpha,beta,0,1)#
  }#
  else{#
  	tmp = betahat + sebetahat*my_etrunct(alpha,beta,v)#
  }#
  ismissing = is.na(betahat) | is.na(sebetahat)#
  tmp[ismissing,]= (m$a+m$b)/2#
  t(tmp)#
#   t(#
#     betahat + sebetahat* #
#       exp(dnorm(alpha,log=TRUE)- pnorm(alpha,log=TRUE))#
#    * #
#       (-expm1(dnorm(beta,log=TRUE)-dnorm(alpha,log=TRUE)))#
#     /#
#       (expm1(pnorm(beta,log=TRUE)-pnorm(alpha,log=TRUE)))#
#   )#
}#
#
#not yet implemented!#
#just returns 0s for now#
comp_postsd.unimix = function(m,betahat,sebetahat,v){#
  print("Warning: Posterior SDs not yet implemented for uniform components")#
  k= ncomp(m)#
  n=length(betahat)#
  return(matrix(NA,nrow=k,ncol=n)) #
}#
#
# the mean of a truncated student.t#
# the result is from the paper 'Moments of truncated Student-t distribution' by H.-J Kim #
#
my_etrunct= function(a,b,v){#
  A = v+a^2#
  B = v+b^2#
  F_a = pt(a,df=v)#
  F_b = pt(b,df=v)#
  G = gamma((v-1)/2)*v^(v/2)/(2*(F_b-F_a)*gamma(v/2)*gamma(1/2))#
  tmp = ifelse(a==b,a,G*(A^(-(v-1)/2)-B^(-(v-1)/2)))#
  tmp#
}
g
mixcompdist=="uniform"
mixcompdist="uniform"
mixsd
mixsd1=c(1,2,3,4)
g=unimix(pi,-mixsd1,mixsd1)
g
pi.init=c(0.5,0.5)
pi=pi.init
pi
pi=rep(0.25,4)
mupi=c(1,pi)
mupi
betahat
sebetahat
sebetahat=rep(1,length(betahat))
sebetahat
df
df=NULL
df
g
nonzeromeanEMoptimfixpoint(mupi,betahat,sebetahat,g,df)
nonzeromeanEMoptimfixpoint(mupi,betahat,sebetahat,g=g,df=df)
g
g1=g
g1
nonzeromeanEMoptimfixpoint(mupi,betahat,sebetahat,g=g1,df=df)
a= nonzeromeanEMoptimfixpoint(mupi,betahat,sebetahat,g,df)
class(g1)
a= nonzeromeanEMoptimfixpoint(mupi,betahat,sebetahat,g1,df)
nonzeromeanEMoptimobj(mupi,betahat,sebetahat,g1,df)
unimix
nonzeromeanEMoptimfixpoint = function(mupi,betahat,sebetahat,g,df){#
	#omegamatrix=matrix(NA,nrow=length(betahat),ncol=length(mixsd))#
	mu=mupi[1]#
	pimean=mupi[-1]#
	matrix_lik=matrix(NA,nrow=length(betahat),ncol=length(mixsd))#
	#matrix_lik=t(compdens_conv(g,betahat-mu,sebetahat,df))#
    m=t(pimean * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k#
    m.rowsum=rowSums(m)#
    classprob=m/m.rowsum #an n by k matrix	#
    pinew=normalize(colSums(classprob))#
#
	munew=optim(par=mean(betahat),f= nonzeromeanEMoptim,pinew=pinew,betahat=betahat,sebetahat=sebetahat,g=g,df=df,method="Brent",lower=min(betahat),upper=max(betahat))$par#
	mupi=c(munew,pinew)#
	return(mupi)#
}
nonzeromeanEMoptimobj(mupi,betahat,sebetahat,g1,df)
omegamatrix=matrix(NA,nrow=length(betahat),ncol=length(mixsd))#
	mu=mupi[1]#
	pimean=mupi[-1]
matrix_lik=t(compdens_conv(g,betahat-mu,sebetahat,df))
matrix_lik
mu=mupi[1]
mu
pimean=mupi[-1]
pimean
betahat
sebetahat
betahat=betahat-500
betahat
mu
matrix_lik=t(compdens_conv(g,betahat-mu,sebetahat,df))
matrix_lik
m=t(pimean * t(matrix_lik))
m
m.rowsum=rowSums(m)
classprob=m/m.rowsum #an n by k matrix	#
    pinew=normalize(colSums(classprob))
pinew
munew=optim(par=mean(betahat),f= nonzeromeanEMoptim,pinew=pinew,betahat=betahat,sebetahat=sebetahat,g=g,df=df,method="Brent",lower=min(betahat),upper=max(betahat))$par
g
g1
munew=optim(par=mean(betahat),f= nonzeromeanEMoptim,pinew=pinew,betahat=betahat,sebetahat=sebetahat,g=g1,df=df,method="Brent",lower=min(betahat),upper=max(betahat))$par
matrix_lik = t(compdens_conv(g,betahat-mu,sebetahat,df))
m = t(pinew * t(matrix_lik))
m.rowsum = rowSums(m)#
	loglik = sum(log(m.rowsum))#
	return(-loglik)
loglik
munew=optim(par=mean(betahat),f= nonzeromeanEMoptim,pinew=pinew,betahat=betahat,sebetahat=sebetahat,g=g,df=df,method="Brent",lower=min(betahat),upper=max(betahat))$par
rm(list = ls())#
remove.packages("ashr")#
install.packages("/Users/daichaoxing/ash/package/ashr.no.cxx.tar.gz",repos=NULL,type="source")#
library("ashr")
help(optim)
munew=optim(par=mean(betahat),fn= nonzeromeanEMoptim,pinew=pinew,betahat=betahat,sebetahat=sebetahat,g=g,df=df,method="Brent",lower=min(betahat),upper=max(betahat))$par
g
g1
