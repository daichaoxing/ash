{
    "contents" : "#' @useDynLib ashr\n#todo\n#\n#' @title Main Adaptive SHrinkage function\n#'\n#' @description Takes vectors of estimates (betahat) and their standard errors (sebetahat), and applies\n#' shrinkage to them, using Empirical Bayes methods, to compute shrunk estimates for beta.\n#'\n#' @details See readme for more details\n#' \n#' @param betahat, a p vector of estimates \n#' @param sebetahat, a p vector of corresponding standard errors\n#' @param method: specifies how ash is to be run. Can be \"shrinkage\" (if main aim is shrinkage) or \"fdr\" (if main aim is to assess fdr or fsr)\n#' This is simply a convenient way to specify certain combinations of parameters: \"shrinkage\" sets pointmass=FALSE and prior=\"uniform\";\n#' \"fdr\" sets pointmass=TRUE and prior=\"nullbiased\".\n#' @param mixcompdist: distribution of components in mixture (\"normal\", \"uniform\" or \"halfuniform\")\n#'\n#' @param lambda1: multiplicative \"inflation factor\" for standard errors (like Genomic Control)\n#' @param lambda2: additive \"inflation factor\" for standard errors (like Genomic Control)\n#' @param nullcheck: whether to check that any fitted model exceeds the \"null\" likelihood\n#' in which all weight is on the first component\n#' @param df: appropriate degrees of freedom for (t) distribution of betahat/sebetahat\n#' @param randomstart: bool, indicating whether to initialize EM randomly. If FALSE, then initializes to prior mean (for EM algorithm) or prior (for VBEM)\n#' @param pointmass: bool, indicating whether to use a point mass at zero as one of components for a mixture distribution\n#' @param onlylogLR: bool, indicating whether to use this function to get logLR. Skip posterior prob, posterior mean, lfdr...\n#' @param prior: string, or numeric vector indicating Dirichlet prior on mixture proportions (defaults to \"uniform\", or 1,1...,1; also can be \"nullbiased\" 1,1/k-1,...,1/k-1 to put more weight on first component)\n#' @param mixsd: vector of sds for underlying mixture components \n#' @param VB: whether to use Variational Bayes to estimate mixture proportions (instead of EM to find MAP estimate)\n#' @param gridmult: the multiplier by which the default grid values for mixsd differ by one another. (Smaller values produce finer grids)\n#' @param minimal_output: if TRUE, just outputs the fitted g and the lfsr (useful for very big data sets where memory is an issue) \n#' @param g: the prior distribution for beta (usually estimated from the data; this is used primarily in simulated data to do computations with the \"true\" g)\n#' \n#' \n#' @return a list with elements fitted.g is fitted mixture\n#' logLR : logP(D|mle(pi)) - logP(D|null)\n#' \n#' @export\n#' \n#' @examples \n#' beta = c(rep(0,100),rnorm(100))\n#' sebetahat = abs(rnorm(200,0,1))\n#' betahat = rnorm(200,beta,sebetahat)\n#' beta.ash = ash(betahat, sebetahat)\n#' summary(beta.ash)\n#' plot(betahat,beta.ash$PosteriorMean,xlim=c(-4,4),ylim=c(-4,4))\n#' \n#' \n#Things to do:\n# check sampling routine\n# check number of iterations\nash = function(betahat,sebetahat,method = c(\"shrink\",\"fdr\"), \n               mixcompdist = c(\"normal\",\"uniform\",\"halfuniform\"),\n               lambda1=1,lambda2=0,nullcheck=TRUE,df=NULL,randomstart=FALSE, \n               pointmass = FALSE, \n               onlylogLR = FALSE, \n               prior=c(\"uniform\",\"nullbiased\"), \n               mixsd=NULL, VB=FALSE,gridmult=sqrt(2),\n               minimaloutput=FALSE,\n               g=NULL,\n               cxx=TRUE){\n  \n    \n  #If method is supplied, use it to set up defaults; provide warning if these default values\n  #are also specified by user\n  if(!missing(method)){\n    method = match.arg(method) \n    if(method==\"shrink\"){\n      if(missing(prior)){\n        prior = \"uniform\"\n      } else {\n        cat(\"Warning: specification of prior overrides default for method shrink\")\n      }\n      if(missing(pointmass)){\n        pointmass=FALSE\n      } else {\n        cat(\"Warning: specification of pointmass overrides default for method shrink\")\n      }\n    }\n  \n    if(method==\"fdr\"){\n      if(missing(prior)){\n        prior = \"nullbiased\"\n      } else {\n        cat(\"Warning: specification of prior overrides default for method fdr\")\n      }\n      if(missing(pointmass)){\n        pointmass=TRUE\n      } else {\n        cat(\"Warning: specification of pointmass overrides default for method fdr\")\n      }\n    }  \n  }\n  \n    mixcompdist = match.arg(mixcompdist)\n    if(mixcompdist==\"uniform\" & pointmass==TRUE){\n      stop(\"point mass not yet implemented for uniform or half-uniform\")\n    }\n    if(mixcompdist==\"halfuniform\" & pointmass==TRUE){\n      stop(\"point mass not yet implemented for uniform or half-uniform\")\n    }\n    if(!is.numeric(prior)){\n      prior = match.arg(prior)\n    }\n  \n  \n  #if df specified, convert betahat so that bethata/sebetahat gives the same p value\n  #from a z test as the original effects would give under a t test with df=df\n  if(!is.null(df)){\n    betahat = effective.effect(betahat,sebetahat,df)\n  }  \n  \n  \n  \n  if(length(sebetahat)==1){\n    sebetahat = rep(sebetahat,length(betahat))\n  }\n  if(length(sebetahat) != length(betahat)){\n    stop(\"Error: sebetahat must have length 1, or same length as betahat\")\n  }\n  \n  completeobs = (!is.na(betahat) & !is.na(sebetahat))\n  if(sum(completeobs)==0){\n    if(onlylogLR){\n      return(list(pi=NULL, logLR = 0))\n    }\n    else{\n      stop(\"Error: all input values are missing\")\n    }\n  }  \n  \n  if(is.null(mixsd)){\n    mixsd= autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)\n  }\n  if(pointmass){\n    mixsd = c(0,mixsd)\n  }\n  \n  k=length(mixsd)  \n  null.comp = which.min(mixsd) #which component is the \"null\"\n  \n  if(!is.numeric(prior)){\n    if(prior==\"nullbiased\"){ # set up prior to favour \"null\"\n      prior = rep(1,k)\n      prior[null.comp] = 10 #prior 10-1 in favour of null\n    }else if(prior==\"uniform\"){\n      prior = rep(1,k)\n    }\n  }\n  \n  if(length(prior)!=k | !is.numeric(prior)){\n    stop(\"invalid prior specification\")\n  }\n  \n  if(missing(g)){\n    pi = prior #default is to initialize pi at prior (mean)\n    if(randomstart){pi=rgamma(k,1,1)}\n  \n    if(!is.element(mixcompdist,c(\"normal\",\"uniform\",\"halfuniform\"))) stop(\"Error: invalid type of mixcompdist\")\n    if(mixcompdist==\"normal\") g=normalmix(pi,rep(0,k),mixsd)\n    if(mixcompdist==\"uniform\") g=unimix(pi,-mixsd,mixsd)\n    if(mixcompdist==\"halfuniform\") g=unimix(c(pi,pi),c(-mixsd,rep(0,k)),c(rep(0,k),mixsd))\n    maxiter = 5000\n  } else {\n    maxiter = 1 # if g is specified, don't iterate the EM \n    prior = rep(1,ncomp(g)) #prior is not actually used if g specified, but required to make sure EM doesn't produce warning\n  }\n  \n  pi.fit=EMest(betahat[completeobs],lambda1*sebetahat[completeobs]+lambda2,g,prior,null.comp=null.comp,nullcheck=nullcheck,VB=VB,maxiter = maxiter, cxx=cxx)  \n  \n\n  if(onlylogLR){\n    logLR = tail(pi.fit$loglik,1) - pi.fit$null.loglik\n    return(list(pi=pi.fit$pi, logLR = logLR))\n  } else if(minimaloutput){\n    n=length(betahat)\n    ZeroProb = rep(0,length=n)\n    NegativeProb = rep(0,length=n)\n    \n    ZeroProb[completeobs] = colSums(comppostprob(pi.fit$g,betahat[completeobs],sebetahat[completeobs])[comp_sd(pi.fit$g)==0,,drop=FALSE])     \n    NegativeProb[completeobs] = cdf_post(pi.fit$g, 0, betahat[completeobs],sebetahat[completeobs]) - ZeroProb[completeobs]\n    ZeroProb[!completeobs] = sum(mixprop(pi.fit$g)[comp_sd(pi.fit$g)==0])\n    NegativeProb[!completeobs] = mixcdf(pi.fit$g,0) \n    \n    lfsr = compute_lfsr(NegativeProb,ZeroProb)\n    result = list(fitted.g=pi.fit$g,lfsr=lfsr,fit=pi.fit)\n    return(result)\n  } else{\n    \n    \n    #   \tpost = posterior_dist(pi.fit$g,betahat,sebetahat)\n    n=length(betahat)\n    ZeroProb = rep(0,length=n)\n    NegativeProb = rep(0,length=n)\n    PosteriorMean = rep(0,length=n)\n    PosteriorSD=rep(0,length=n)\n    \n    ZeroProb[completeobs] = colSums(comppostprob(pi.fit$g,betahat[completeobs],sebetahat[completeobs])[comp_sd(pi.fit$g)==0,,drop=FALSE])     \n    NegativeProb[completeobs] = cdf_post(pi.fit$g, 0, betahat[completeobs],sebetahat[completeobs]) - ZeroProb[completeobs]\n    PosteriorMean[completeobs] = postmean(pi.fit$g,betahat[completeobs],sebetahat[completeobs])\n    PosteriorSD[completeobs] =postsd(pi.fit$g,betahat[completeobs],sebetahat[completeobs]) \n    \n    #FOR MISSING OBSERVATIONS, USE THE PRIOR INSTEAD OF THE POSTERIOR\n    ZeroProb[!completeobs] = sum(mixprop(pi.fit$g)[comp_sd(pi.fit$g)==0])\n    NegativeProb[!completeobs] = mixcdf(pi.fit$g,0) \n    PosteriorMean[!completeobs] = mixmean(pi.fit$g)\n    PosteriorSD[!completeobs] =mixsd(pi.fit$g)  \n    PositiveProb =  1- NegativeProb-ZeroProb    \n    \n    lfsr = compute_lfsr(NegativeProb,ZeroProb)\n    lfsra =  compute_lfsra(PositiveProb,NegativeProb,ZeroProb) \n    \n    lfdr = ZeroProb\n    qvalue = qval.from.lfdr(lfdr)\n        \n    result = list(fitted.g=pi.fit$g,logLR =tail(pi.fit$loglik,1) - pi.fit$null.loglik,PosteriorMean = PosteriorMean,PosteriorSD=PosteriorSD,PositiveProb =PositiveProb,NegativeProb=NegativeProb, ZeroProb=ZeroProb,lfsr = lfsr,lfsra=lfsra, lfdr=lfdr,qvalue=qvalue,fit=pi.fit,lambda1=lambda1,lambda2=lambda2,call=match.call(),data=list(betahat = betahat, sebetahat=sebetahat))\n    class(result)= \"ash\"\n    return(result)\n    \n  }\n  #if(nsamp>0){\n  #  sample = posterior_sample(post,nsamp)\n  #}\n}\n\n\n#' @export\nfast.ash = function(betahat,sebetahat, \n               nullcheck=TRUE,randomstart=FALSE, \n               pointmass = TRUE,    \n               prior=c(\"nullbiased\",\"uniform\"), \n               mixsd=NULL, VB=FALSE,gridmult=4,\n               g=NULL, cxx=TRUE){\n  \n    \n  #If method is supplied, use it to set up defaults; provide warning if these default values\n  #are also specified by user\n    if(!is.numeric(prior)){\n      prior = match.arg(prior)\n    }\n  \n  if(length(sebetahat)==1){\n    sebetahat = rep(sebetahat,length(betahat))\n  }\n  if(length(sebetahat) != length(betahat)){\n    stop(\"Error: sebetahat must have length 1, or same length as betahat\")\n  }\n  \n  completeobs = (!is.na(betahat) & !is.na(sebetahat))\n  if(sum(completeobs)==0){\n    stop(\"Error: all input values are missing\")\n  }  \n  \n  if(is.null(mixsd)){\n    mixsd= autoselect.mixsd(betahat[completeobs],sebetahat[completeobs],gridmult)\n  }\n  if(pointmass){\n    mixsd = c(0,mixsd)\n  }\n  \n  k=length(mixsd)  \n  null.comp = which.min(mixsd) #which component is the \"null\"\n  \n  if(!is.numeric(prior)){\n    if(prior==\"nullbiased\"){ # set up prior to favour \"null\"\n      prior = rep(1,k)\n      prior[null.comp] = 10 #prior 10-1 in favour of null\n    }else if(prior==\"uniform\"){\n      prior = rep(1,k)\n    }\n  }\n  \n  if(length(prior)!=k | !is.numeric(prior)){\n    stop(\"invalid prior specification\")\n  }\n  \n  if(missing(g)){\n    pi = prior #default is to initialize pi at prior (mean)\n    if(randomstart){pi=rgamma(k,1,1)}\n  \n    g=normalmix(pi,rep(0,k),mixsd)\n    maxiter = 5000\n  } else {\n    maxiter = 1; # if g is specified, don't iterate the EM \n  }\n  \n  pi.fit=EMest(betahat[completeobs],sebetahat[completeobs],g,prior,null.comp=null.comp,nullcheck=nullcheck,VB=VB,maxiter = maxiter, cxx=cxx)  \n\n    n=length(betahat)\n    PosteriorMean = rep(0,length=n)\n    PosteriorSD=rep(0,length=n)\n    \n    PosteriorMean[completeobs] = postmean(pi.fit$g,betahat[completeobs],sebetahat[completeobs])\n    PosteriorSD[completeobs] =postsd(pi.fit$g,betahat[completeobs],sebetahat[completeobs]) \n    \n    #FOR MISSING OBSERVATIONS, USE THE PRIOR INSTEAD OF THE POSTERIOR\n    PosteriorMean[!completeobs] = mixmean(pi.fit$g)\n    PosteriorSD[!completeobs] =mixsd(pi.fit$g)  \n        \n    result = list(fitted.g=pi.fit$g,PosteriorMean = PosteriorMean,PosteriorSD=PosteriorSD,call=match.call(),data=list(betahat = betahat, sebetahat=sebetahat))\n    return(result)\n\n  #if(nsamp>0){\n  #  sample = posterior_sample(post,nsamp)\n  #}\n}\n\n\n\ncompute_lfsr = function(NegativeProb,ZeroProb){\n  ifelse(NegativeProb> 0.5*(1-ZeroProb),1-NegativeProb,NegativeProb+ZeroProb)\n}\n\ncompute_lfsra = function(PositiveProb, NegativeProb,ZeroProb){\n  ifelse(PositiveProb<NegativeProb,2*PositiveProb+ZeroProb,2*NegativeProb+ZeroProb)  \n}  \n#' @title Estimate posterior distribution on mixture proportions of a mixture model by a Variational Bayes EM algorithm\n#'\n#' @description Given the individual component likelihoods for a mixture model, estimates the posterior on \n#' the mixture proportions by an VBEM algorithm. Used by the ash main function; there is no need for a user to call this \n#' function separately, but it is exported for convenience.\n#'\n#' @details Fits a k component mixture model \\deqn{f(x|\\pi) = \\sum_k \\pi_k f_k(x)} to independent\n#' and identically distributed data \\eqn{x_1,\\dots,x_n}. \n#' Estimates posterior on mixture proportions \\eqn{\\pi} by Variational Bayes, \n#' with a Dirichlet prior on \\eqn{\\pi}. \n#' Algorithm adapted from Bishop (2009), Pattern Recognition and Machine Learning, Chapter 10.\n#' \n#' @param matrix_lik: a n by k matrix with (j,k)th element equal to \\eqn{f_k(x_j)}.\n#' @param prior: a k vector of the parameters of the Dirichlet prior on \\eqn{\\pi}. Recommended to be rep(1,k)\n#' @param post.init: the initial value of the posterior parameters. If not specified defaults to the prior parameters.\n#' @param tol: the tolerance for convergence of log-likelihood bound.\n#' @param maxiter: the maximum number of iterations performed\n#' \n#' @return A list, whose components include point estimates (pihat), \n#' the parameters of the fitted posterior on \\eqn{\\pi} (pipost),\n#' the bound on the log likelihood for each iteration (B)\n#' and a flag to indicate convergence (converged).\n#'  \n#' @export\n#' \nmixVBEM = function(matrix_lik, prior, post.init=NULL, tol=0.0001, maxiter=5000){\n  n=nrow(matrix_lik)\n  k=ncol(matrix_lik)\n  B = rep(0,maxiter)\n  pipost = post.init\n  if(is.null(post.init)){\n    pipost = prior # Dirichlet posterior on pi\n  }\n  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)\n  classprob = avgpipost * matrix_lik\n  classprob = classprob/rowSums(classprob) # n by k matrix  \n  B[1] = sum(classprob*log(avgpipost*matrix_lik),na.rm=TRUE) - diriKL(prior,pipost) #negative free energy\n  i=1\n  \n  if(maxiter>=2){\n    for(i in 2:maxiter){  \n      pipost = colSums(classprob) + prior\n    \n      #Now re-estimate pipost\n      avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)\n      classprob = avgpipost*matrix_lik\n      classprob = classprob/rowSums(classprob) # n by k matrix\n    \n      B[i] = sum(classprob*log(avgpipost*matrix_lik),na.rm=TRUE) - diriKL(prior,pipost)\n    \n      if(abs(B[i]-B[i-1])<tol) break;\n    } \n    if(i>maxiter){i=maxiter}\n  }\n  \n   \n  return(list(pihat = pipost/sum(pipost), B=B[1:i], niter = i, converged=(abs(B[i]-B[i-1])<tol),post=pipost))\n}\n  \n\n#' @title Estimate mixture proportions of a mixture model by EM algorithm\n#'\n#' @description Given the individual component likelihoods for a mixture model, estimates the mixture proportions by an EM algorithm.\n#'\n#' @details Fits a k component mixture model \\deqn{f(x|\\pi) = \\sum_k \\pi_k f_k(x)} to independent\n#' and identically distributed data \\eqn{x_1,\\dots,x_n}. \n#' Estimates mixture proportions \\eqn{\\pi} by maximum likelihood, or by maximum a posteriori (MAP) estimation for a Dirichlet prior on $\\pi$ \n#' (if a prior is specified).  Used by the ash main function; there is no need for a user to call this \n#' function separately, but it is exported for convenience.\n#'\n#' \n#' @param matrix_lik, a n by k matrix with (j,k)th element equal to \\eqn{f_k(x_j)}.\n#' @param prior, a k vector of the parameters of the Dirichlet prior on \\eqn{\\pi}. Recommended to be rep(1,k)\n#' @param pi.init, the initial value of \\eqn{\\pi} to use. If not specified defaults to (1/k,...,1/k).\n#' @param tol, the tolerance for convergence of log-likelihood.\n#' @param maxiter the maximum number of iterations performed\n#' \n#' @return A list, including the estimates (pihat), the log likelihood for each interation (B)\n#' and a flag to indicate convergence\n#'  \n#' @export\n#' \nmixEM = function(matrix_lik, prior, pi.init = NULL,tol=0.0001, maxiter=5000){\n  n=nrow(matrix_lik)\n  k=ncol(matrix_lik)\n  B = rep(0,maxiter)\n  pi = pi.init\n  if(is.null(pi.init)){\n    pi = rep(1/k,k)# Use as starting point for pi\n  } \n  pi = ifelse(pi<1e-5,1e-5,pi) #set any estimates that are too small to be just very small\n  pi = normalize(pi)\n  \n  loglik = rep(0,maxiter)\n  priordens= rep(0,maxiter)\n  m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k\n  m.rowsum = rowSums(m)\n  loglik[1] = sum(log(m.rowsum))\n  priordens[1] = sum((prior-1)*log(pi)) \n  classprob = m/m.rowsum #an n by k matrix\n  i=1\n  if(maxiter >= 2){\n    for(i in 2:maxiter){  \n      pi = colSums(classprob) + prior-1\n      pi = ifelse(pi<1e-5,1e-5,pi) #set any estimates that are less than zero, which can happen with prior<1, to 0\n      pi = normalize(pi)\n        \n      #Now re-estimate pi\n      m  = t(pi * t(matrix_lik)) \n      m.rowsum = rowSums(m)\n      loglik[i] = sum(log(m.rowsum))\n      priordens[i] = sum((prior-1)*log(pi)) \n      classprob = m/m.rowsum\n    \n    \n      if(abs(loglik[i]+priordens[i]-loglik[i-1]-priordens[i-1])<tol) break;\n    }\n  }\n  \n  return(list(pihat = pi, B=loglik[1:i], \n              niter = i, converged=(abs(loglik[i]+priordens[i]-loglik[i-1]-priordens[i-1])<tol)))\n}\n\n\n#estimate mixture proportions of sigmaa by EM algorithm\n#prior gives the parameter of a Dirichlet prior on pi\n#(prior is used to encourage results towards smallest value of sigma when\n#likelihood is flat)\n#nullcheck indicates whether to check whether the loglike exceeds the null\n#(may not want to use if prior is used)\n#VB provides an approach to estimate the approximate posterior distribution\n#of mixture proportions of sigmaa by variational Bayes method\n#(use Dirichlet prior and approximate Dirichlet posterior)\n#if cxx TRUE use cpp version of R function mixEM\nEMest = function(betahat,sebetahat,g,prior,null.comp=1,nullcheck=TRUE,VB=FALSE,ltol=0.0001, maxiter=5000, cxx=TRUE){ \n \n  pi.init = g$pi\n  k=ncomp(g)\n  n = length(betahat)\n  \n  matrix_lik = t(compdens_conv(g,betahat,sebetahat))\n    \n  if(VB==TRUE){\n    EMfit=mixVBEM(matrix_lik,prior,pi.init,ltol, maxiter)}\n  else{\n    if (cxx==TRUE){\n        EMfit = cxxMixEM(matrix_lik,prior,pi.init,ltol, maxiter)}\n    else{\n        EMfit = mixEM(matrix_lik,prior,pi.init,ltol, maxiter)}\n  }\n  \n  pi = EMfit$pihat     \n  loglik = EMfit$B # actually return log lower bound not log-likelihood! \n  converged = EMfit$converged\n  niter = EMfit$niter\n  loglik.final = EMfit$B[niter]\n  \n  null.loglik = sum(log(matrix_lik[,null.comp]))  \n\n  if(nullcheck==TRUE & VB==FALSE){ #null check doesn't work with VB yet\n      if(null.loglik > loglik.final){ #check whether exceeded \"null\" likelihood where everything is null\n      pi=rep(0,k)\n      pi[null.comp]=1\n      m  = t(pi * t(matrix_lik)) \n      m.rowsum = rowSums(m)\n      loglik[niter] = sum(log(m.rowsum))\n    }\n  }\n  \n  g$pi=pi\n  \n  return(list(loglik=loglik[1:niter],null.loglik=null.loglik,\n            matrix_lik=matrix_lik,converged = converged,g=g))\n}\n\n\n\nnormalize = function(x){return(x/sum(x))}\n\n\n#' @title Estimate mixture proportions of a mixture model by EM algorithm\n#'\n#' @description Return the posterior on beta given a prior (g) that is a mixture of normals (class normalmix) \n#' and observation betahat \\sim N(beta,sebetahat)\n#'\n#' @details This can be used to obt\n#'\n#' @param g: a normalmix with components indicating the prior; works only if g has means 0\n#' @param betahat (n vector of observations) \n#' @param sebetahat (n vector of standard errors/deviations of observations)\n#' \n#' @return A list, (pi1,mu1,sigma1) whose components are each k by n matrices\n#' where k is number of mixture components in g, n is number of observations in betahat\n#' \n#' @export\n#' \n#' \nposterior_dist = function(g,betahat,sebetahat){\n  if(class(g)!=\"normalmix\"){\n    stop(\"Error: posterior_dist implemented only for g of class normalmix\")\n  }\n  pi0 = g$pi\n  mu0 = g$mean\n  sigma0 = g$sd  \n  k= length(pi0)\n  n= length(betahat)\n  \n  if(!all.equal(g$mean,rep(0,k))) stop(\"Error: posterior_dist currently only implemented for zero-centered priors\")\n  \n  pi1 = pi0 * t(matrix_dens(betahat,sebetahat,sigma0))\n  pi1 = apply(pi1, 2, normalize) #pi1 is now an k by n matrix\n\n  #make k by n matrix versions of sigma0^2 and sebetahat^2\n  # and mu0 and betahat\n  s0m2 = matrix(sigma0^2,nrow=k,ncol=n,byrow=FALSE)\n  sebm2 = matrix(sebetahat^2,nrow=k,ncol=n, byrow=TRUE)\n  mu0m = matrix(mu0,nrow=k,ncol=n,byrow=FALSE)\n  bhatm = matrix(betahat,nrow=k,ncol=n,byrow=TRUE)\n\n  sigma1 = (s0m2*sebm2/(s0m2 + sebm2))^(0.5)  \n  w = sebm2/(s0m2 + sebm2)\n  mu1 = w*mu0m + (1-w)*bhatm\n  \n  #WHERE DATA ARE MISSING, SET POSTERIOR = PRIOR\n  ismiss = (is.na(betahat) | is.na(sebetahat)) \n  pi1[,ismiss] = pi0\n  mu1[,ismiss] = mu0\n  sigma1[,ismiss] = sigma0\n  \n  return(list(pi=pi1,mu=mu1,sigma=sigma1))\n}\n\n#return matrix of densities of observations (betahat) \n# assuming betahat_j \\sim N(0, sebetahat_j^2 + sigmaavec_k^2)\n#normalized by maximum of each column\n#INPUT\n#betahat is n vector, \n#sebetahat is n vector, \n#sigmaavec is k vector\n#return is n by k matrix of the normal likelihoods, \n# with (j,k)th element the density of N(betahat_j; mean=0, var = sebetahat_j^2 + sigmaavec_k^2)\n#normalized to have maximum 1 in each column\nmatrix_dens = function(betahat, sebetahat, sigmaavec){\n  k = length(sigmaavec)\n  n = length(betahat)\n  ldens = dnorm(betahat,0,sqrt(outer(sebetahat^2,sigmaavec^2,FUN=\"+\")),log=TRUE)\n  maxldens = apply(ldens, 1, max)\n  ldens = ldens - maxldens\n  return(exp(ldens))\n}\n\n#return the \"effective\" estimate\n#that is the effect size betanew whose z score betanew/se\n#would give the same p value as betahat/se compared to a t with df\neffective.effect=function(betahat,se,df){\n  p = pt(betahat/se,df)\n  qnorm(p,sd=se)\n}\n\n\n#' @title Function to compute q values from local false discovery rates\n#'\n#' @description Computes q values from a vector of local fdr estimates\n#'\n#' @details The q value for a given lfdr is an estimate of the (tail) False Discovery Rate \n#' for all findings with a smaller lfdr, and is found by the average of the lfdr for\n#' all more significant findings. See Storey (2003), Annals of Statistics, for definition of q value.  \n#' \n#' \n#' @param lfdr, a vector of local fdr estimates\n#'\n#' @return vector of q values\n#' \n#' @export\nqval.from.lfdr = function(lfdr){\n  o = order(lfdr)\n  qvalue=rep(NA,length(lfdr))\n  qvalue[o] = (cumsum(sort(lfdr))/(1:sum(!is.na(lfdr))))\n  return(qvalue)\n}\n\n# try to select a default range for the sigmaa values\n# that should be used, based on the values of betahat and sebetahat\n# mult is the multiplier by which the sds differ across the grid\nautoselect.mixsd = function(betahat,sebetahat,mult){\n  sigmaamin = min(sebetahat)/10 #so that the minimum is small compared with measurement precision\n  if(all(betahat^2<sebetahat^2)){\n    sigmaamax = 8*sigmaamin #to deal with the occassional odd case where this could happen; 8 is arbitrary\n  } else {\n    sigmaamax = 2*sqrt(max(betahat^2-sebetahat^2)) #this computes a rough largest value you'd want to use, based on idea that sigmaamax^2 + sebetahat^2 should be at least betahat^2   \n  }\n  if(mult==0){\n    return(c(0,sigmaamax/2))\n  }else{\n    npoint = ceiling(log2(sigmaamax/sigmaamin)/log2(mult))\n    return(mult^((-npoint):0) * sigmaamax)\n  }\n}\n\n\n#' @title Summary method for ash object\n#'\n#' @description Print summary of fitted ash object\n#'\n#' @details See readme for more details\n#' \n#' @export\n#' \nsummary.ash=function(a){\n  print(a$fitted.g)\n  print(tail(a$fit$loglik,1),digits=10)\n  print(a$fit$converged)\n}\n\n#' @title Print method for ash object\n#'\n#' @description Print the fitted distribution of beta values in the EB hierarchical model\n#'\n#' @details None\n#' \n#' @export\n#' \nprint.ash =function(a){\n  print(a$fitted.g)\n}\n\n#' @title Plot method for ash object\n#'\n#' @description Plot the density of the underlying fitted distribution\n#'\n#' @details None\n#' \n#' @export\n#' \nplot.ash = function(a,xmin,xmax,...){\n  x = seq(xmin,xmax,length=1000)\n  y = density(a,x)\n  plot(x,y,type=\"l\",...)\n}\n\n#compute the predictive density of an observation\n#given the fitted ash object a and the vector se of standard errors\n#not implemented yet\npredictive=function(a,se){\n  \n}\n\n\n#' @title Get fitted loglikelihood for ash object\n#'\n#' @description Return the log-likelihood of the data under the fitted distribution\n#'\n#' @param a the fitted ash object\n#'\n#' @details None\n#' \n#' @export\n#' \n#'\nget_loglik = function(a){\n  return(tail(a$fit$loglik,1))\n}\n\n#' @title Get pi0 estimate for ash object\n#'\n#' @description Return estimate of the null proportion, pi0\n#'\n#' @param a the fitted ash object\n#'\n#' @details Extracts the estimate of the null proportion, pi0, from the object a\n#' \n#' @export\n#' \nget_pi0 = function(a){\n  return(ifelse(comp_sd(a$fitted.g)[1]==0,a$fitted.g$pi[1],0))\n}\n\n#' @title Compute loglikelihood for data from ash fit\n#'\n#' @description Return the log-likelihood of the data betahat, with standard errors betahatsd, \n#' under the fitted distribution in the ash object. \n#' \n#'\n#' @param a the fitted ash object\n#' @param betahat the data\n#' @param betahatsd the observed standard errors\n#' @param zscores indicates whether ash object was originally fit to z scores \n#' @details None\n#' \n#' @export\n#' \n#'\nloglik.ash = function(a,betahat,betahatsd,zscores=FALSE){\n  g=a$fitted.g\n  FUN=\"+\"\n  if(zscores==TRUE){\n    g$sd = sqrt(g$sd^2+1) \n    FUN=\"*\"\n  }\n  return(loglik_conv(g,betahat, betahatsd,FUN))\n}\n\n#' @title Density method for ash object\n#'\n#' @description Return the density of the underlying fitted distribution\n#'\n#' @param a the fitted ash object\n#' @param x the vector of locations at which density is to be computed\n#'\n#' @details None\n#' \n#' @export\n#' \n#'\ndensity.ash=function(a,x){list(x=x,y=dens(a$fitted.g,x))}\n\n#' @title cdf method for ash object\n#'\n#' @description Computed the cdf of the underlying fitted distribution\n#'\n#' @param a the fitted ash object\n#' @param x the vector of locations at which cdf is to be computed\n#' @param lower.tail (default=TRUE) whether to compute the lower or upper tail\n#'\n#' @details None\n#' \n#' @export\n#' \n#'\ncdf.ash=function(a,x,lower.tail=TRUE){\n return(list(x=x,y=mixcdf(a$fitted.g,x,lower.tail)))\n}\n\n\n#return the KL-divergence between 2 dirichlet distributions\n#p,q are the vectors of dirichlet parameters of same lengths\ndiriKL = function(p,q){\n  p.sum = sum(p)\n  q.sum = sum(q)\n  k = length(q)\n  KL = lgamma(q.sum)-lgamma(p.sum)+sum((q-p)*(digamma(q)-digamma(rep(q.sum,k))))+sum(lgamma(p)-lgamma(q))\n  return(KL)\n}\n\n#helper function for VBEM\nVB.update = function(matrix_lik, pipost){\n  avgpipost = matrix(exp(rep(digamma(pipost),n)-rep(digamma(sum(pipost)),k*n)),ncol=k,byrow=TRUE)\n  classprob = avgpipost * matrix_lik\n  classprob = classprob/rowSums(classprob) # n by k matrix\n  B = sum(classprob*log(avgpipost*matrix_lik),na.rm=TRUE) - diriKL(prior,pipost) #negative free energy\n  return(list(classprob=classprob,B=B))\n}\n",
    "created" : 1393511387582.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1096041607",
    "id" : "DD00B1CC",
    "lastKnownWriteTime" : 1393467389,
    "path" : "~/Documents/git/ash/package/ashr/R/ash.R",
    "project_path" : null,
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_source"
}